{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14f5e8ee-8e52-4f9f-b278-1afb1e6602cc",
   "metadata": {},
   "source": [
    "# 0. Notebook description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862ba499-d594-4fc3-b8ac-cf8efdaaeaa9",
   "metadata": {},
   "source": [
    "In this notebook, we load the best performing model (using 5 convolutional layers with average pooling, trained on data augmented with flipping and cropping) and use it to make realtime emotion prediction using the webcam of the computer running the notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230698e4-326c-4852-a2bb-ce0d6655cf28",
   "metadata": {},
   "source": [
    "# 1. Import the necessary libraries and load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c86d15-47c3-406c-9007-32a98593250b",
   "metadata": {},
   "source": [
    "We use the CascadeClassifier from the OpenCV library, which is a widely used computer vision tool capable of detecting objects in images and video streams in real-time. Specifically, the Haar Cascade classifier is applied here for face detection. It uses a pre-trained model stored in the 'haarcascade_frontalface_default.xml' file, which is designed to identify frontal faces, such as those captured by a webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d008e905-3eac-49d4-8b7a-327f1e270c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "# Load the Haar Cascade face detector from OpenCV\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b51361-6b12-4132-acc6-5973f5eeaf8e",
   "metadata": {},
   "source": [
    "Below, we load our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "214b0b6e-6052-45fc-900f-86e66c8e1052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout3): Dropout(p=0.25, inplace=False)\n",
       "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool4): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (conv5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4))\n",
       "  (bn5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (fc1): Linear(in_features=10368, out_features=250, bias=True)\n",
       "  (dropout_fc): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=250, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # 1st Conv Layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding='valid')\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "\n",
    "        # 2nd Conv Layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 3rd Conv Layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "\n",
    "        # 4th Conv Layer\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=2)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.pool4 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 5th Conv Layer\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=4)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.pool5 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Calculate the flatten size dynamically\n",
    "        self.flatten_size = self._get_flatten_size()\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(self.flatten_size, 250)\n",
    "        self.dropout_fc = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(250, 7)  # 7 classes for emotion recognition\n",
    "\n",
    "    def _get_flatten_size(self):\n",
    "        # Create a dummy tensor with the same size as input image\n",
    "        dummy_input = torch.zeros(1, 1, 48, 48)  # batch_size, channels, height, width\n",
    "        dummy_output = self._forward_conv_layers(dummy_input)\n",
    "        return dummy_output.numel()\n",
    "\n",
    "    def _forward_conv_layers(self, x):\n",
    "        # Pass through convolutional layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool5(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use the convolutional layers defined in _forward_conv_layers\n",
    "        x = self._forward_conv_layers(x)\n",
    "\n",
    "        # Dynamically flatten the output\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN()\n",
    "model = model.to(device)\n",
    "model_path = 'models/7_Group17_DLProject.pth'\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6abdfe-ba85-4c6d-a2aa-fad2e4243d21",
   "metadata": {},
   "source": [
    "# 2. Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5023b1a-d657-4331-9214-f32710a5db90",
   "metadata": {},
   "source": [
    "To allow our model to use the realtime images, we define the function `preprocess_face` to convert the webcam color images to 48 by 48 pixel grayscale images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eca40ef5-1bbf-465d-9aba-2c77f23158d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing for grayscale 48x48 images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Function to preprocess the face crop\n",
    "def preprocess_face(face_crop):\n",
    "    return transform(face_crop).unsqueeze(0)  # Add batch dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb80ac4-a4d8-42a2-88d0-4bb9596ed459",
   "metadata": {},
   "source": [
    "# 3. Realtime face detection and emotion prediction with webcam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdb895d-9ab1-4b45-8a08-b32dd852cdfe",
   "metadata": {},
   "source": [
    "The code below captures video from the default webcam and performs real-time face detection using the cascade classifier. For each detected face, it extracts the face region and preprocesses it for emotion prediction using a deep learning model. The predicted emotion label is then displayed on the frame, with a rectangle drawn around the face. The loop continues until the user presses the \"q\" key, at which point the webcam is released and the display window is closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "741c2807-73b4-4274-84fc-98ed2692b71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(0)  # 0 for the default camera\n",
    "emotion_labels = {0:'Angry', 1:'Disgust', 2:'Fear', 3:'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale for face detection\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    for (x, y, w, h) in faces:  # Iterate through detected faces\n",
    "        # Extract the face region (square crop)\n",
    "        face_crop = frame[y:y+h, x:x+w]  # Crop the face from the original frame\n",
    "        \n",
    "        try:\n",
    "            # Preprocess the face crop\n",
    "            input_tensor = preprocess_face(face_crop).to(device)\n",
    "            \n",
    "            # Perform prediction\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                predicted_class = torch.argmax(output, dim=1).item()\n",
    "\n",
    "            # Draw a rectangle around the face and add the prediction label\n",
    "            label = f\"Prediction: {emotion_labels[predicted_class]}\"\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)  # Green rectangle around the face\n",
    "            cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing face crop: {e}\")\n",
    "\n",
    "    # Display the frame with predictions\n",
    "    cv2.imshow('Face Detection and Prediction', frame)\n",
    "\n",
    "    # Exit on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
