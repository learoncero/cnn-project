{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook description\n",
    "\n",
    "This notebook demonstrates the customization of the reward function for the `highway-fast-v0 environment`. The custom reward logic is implemented in the `utils/highwayEnvCustomReward.py` file and replaces or enhances elements of the default reward function to better **align with realistic driving scenarios and enhance safety aspects**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the default reward function in `highway-fast-v0`\n",
    "\n",
    "The default reward function in the highway-fast-v0 environment is designed to promote safe and efficient driving behaviors. It calculates a scalar reward for each action based on four key factors:\n",
    "\n",
    "1. **Collision Penalty:**\n",
    "\n",
    "   - The `collision_reward` penalizes the agent if the vehicle crashes. This is a binary penalty, either 0 (no crash) or a negative value (collision occurred).\n",
    "\n",
    "2. **Right Lane Reward:**\n",
    "\n",
    "   - The `right_lane_reward` rewards the agent for staying in the rightmost lane. The reward increases as the vehicle moves closer to the rightmost lane, encouraging proper lane usage.\n",
    "\n",
    "3. **High-Speed Reward:**\n",
    "\n",
    "   - The `high_speed_reward` incentivizes driving at higher speeds. The reward is proportional to the vehicle's forward speed, normalized within a defined speed range.\n",
    "\n",
    "4. **On-Road Reward:**\n",
    "   - The `on_road_reward` encourages the vehicle to stay on the road by multiplying the total reward by 1 if the vehicle is on the road or 0 if it is off-road. This means that if the vehicle is off-road, the total reward is also zero.\n",
    "\n",
    "## Shortcomings of the default reward function in `highway-fast-v0`\n",
    "\n",
    "1. **Overemphasising the rightmost lane:**\n",
    "\n",
    "   - The `right_lane_reward` encourages staying in the rightmost lane, which may not be in line with realistic driving goals. For example, overtaking may require swerving into other lanes, which is not directly encouraged.\n",
    "\n",
    "2. **Speed reward out of context:**\n",
    "\n",
    "   - The `high_speed_reward` rewards speed linearly, but does not take into account traffic density. Speeding in traffic jams should be penalised.\n",
    "\n",
    "3. **Binary Collision Penalty:**\n",
    "\n",
    "   - The `collision_reward` is binary, providing the same penalty regardless of the severity or cause of the crash. This ignores scenarios like near-misses, which could be penalized slightly to encourage caution.\n",
    "\n",
    "4. **Lack of a Safe Distance Mechanism:**\n",
    "\n",
    "   - The current reward function does not provide an incentive to maintain a safe distance from the vehicle in front. However, promoting a safe following distance is a fundamental part of safe driving. It helps avoid collisions and creates smoother traffic flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes made to the reward function\n",
    "\n",
    "### Add safe distance reward\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "1. Identify the Front Vehicle:\n",
    "\n",
    "   - Use the existing `road.neighbour_vehicles(vehicle)` method to locate the vehicle ahead of the agent.\n",
    "\n",
    "2. Compute the Distance:\n",
    "\n",
    "   - Calculate the distance between the agent's vehicle and the identified front vehicle. If no front vehicle exists, assume the distance is infinite.\n",
    "\n",
    "3. Define the Reward:\n",
    "\n",
    "   - Reward the agent if the distance exceeds a safe threshold (e.g., 10 meters). Penalize the agent as the distance decreases below this threshold.\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "- Encourages the agent to maintain a safe distance, reducing the likelihood of rear-end collisions.\n",
    "- Promotes safer and more realistic driving behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Code**\n",
    "\n",
    "````python\n",
    "# safe distance reward\n",
    "front_vehicle, _ = self.road.neighbour_vehicles(self.vehicle) # identify the front vehicle\n",
    "safe_distance = 5\n",
    "if front_vehicle:\n",
    "    distance = max(front_vehicle.position[0] - self.vehicle.position[0], 0)\n",
    "\n",
    "    if distance > safe_distance:\n",
    "        safe_distance_reward = 1  # Full reward if distance is safe\n",
    "    else:\n",
    "        safe_distance_reward = -1 * (safe_distance - distance) / safe_distance\n",
    "else:\n",
    "    safe_distance_reward = 0\n",
    "````\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve existing collision reward\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "1. Collision Penalty:\n",
    "    - Check whether the agent's vehicle has crashed (self.vehicle.crashed).\n",
    "    - Apply a fixed penalty of -1 if a collision has occurred; otherwise, no penalty is applied.\n",
    "\n",
    "2. Near-Miss Penalty:\n",
    "    - Iterate through all vehicles on the road and calculate the Euclidean distance between the agent's vehicle and other vehicles.\n",
    "    - Define a \"near-miss\" threshold (e.g., 2 meters).\n",
    "    - Apply a scaled penalty when the distance falls within the near-miss range (closer near-misses incur higher penalties).\n",
    "\n",
    "3. Combine Penalties:\n",
    "    - The total collision reward is the sum of the collision penalty and the near-miss penalties.\n",
    "\n",
    "**Advantages**\n",
    "- Encourages the agent to avoid collisions entirely by imposing a strict penalty.\n",
    "- Promotes safer behavior by penalizing close interactions (near-misses), even when collisions are avoided.\n",
    "- Improves realism in the simulation, as near-misses are indicative of risky driving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Code**\n",
    "\n",
    "````python\n",
    "# Improved collision reward\n",
    "collision_penalty = 1 if self.vehicle.crashed else 0  # Full penalty for collision\n",
    "near_miss_penalty = 0\n",
    "for vehicle in self.road.vehicles:\n",
    "    if vehicle is not self.vehicle:\n",
    "        distance_to_vehicle = np.linalg.norm(\n",
    "            np.array(self.vehicle.position) - np.array(vehicle.position)\n",
    "        )\n",
    "        if 0 < distance_to_vehicle <= 2:  # Near-miss threshold\n",
    "            near_miss_penalty += -0.5 * (2 - distance_to_vehicle) / 2\n",
    "\n",
    "collision_reward = collision_penalty + near_miss_penalty\n",
    "````\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add high speed reward\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "1. Define Traffic Radius:\n",
    "    - Set a traffic_radius (e.g., 10 meters) within which the agent's vehicle assesses the surrounding traffic density.\n",
    "\n",
    "2. Count Nearby Vehicles:\n",
    "    - Iterate through all vehicles on the road and calculate the Euclidean distance between the agent's vehicle and others.\n",
    "    - Increment the count of nearby vehicles if they are within the defined traffic radius.\n",
    "\n",
    "3. Compute Traffic Density Factor:\n",
    "    - Use the number of nearby vehicles to calculate a traffic density factor, where higher traffic density reduces the reward.\n",
    "    - Define a maximum density (max_density, e.g., 10 vehicles) for scaling. The factor decreases linearly as the density approaches this maximum.\n",
    "\n",
    "4. Scale the High-Speed Reward:\n",
    "    - Adjust the agent's speed-based reward (scaled_speed) using the traffic density factor.\n",
    "    - Clip the reward within a range of 0 to 1 for consistency.\n",
    "\n",
    "**Advantages**\n",
    "- Encourages the agent to drive at higher speeds when traffic density is low, promoting efficient driving.\n",
    "- Discourages risky high-speed driving in dense traffic, reducing the likelihood of collisions or unsafe maneuvers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Code**\n",
    "\n",
    "````python\n",
    "# high speed reward\n",
    "traffic_radius = 10\n",
    "\n",
    "# Count the number of vehicles within the traffic radius\n",
    "nearby_vehicles = 0\n",
    "for other_vehicle in self.road.vehicles:\n",
    "    if other_vehicle is not self.vehicle:\n",
    "        distance = np.linalg.norm(\n",
    "            np.array(other_vehicle.position) - np.array(self.vehicle.position)\n",
    "        )\n",
    "        if distance < traffic_radius:\n",
    "            nearby_vehicles += 1\n",
    "\n",
    "# Traffic density factor: more vehicles -> higher penalty\n",
    "max_density = 10\n",
    "traffic_density_factor = max(0, 1 - nearby_vehicles / max_density)\n",
    "\n",
    "# Adjust high-speed reward based on traffic density\n",
    "high_speed_reward = np.clip(scaled_speed * traffic_density_factor, 0, 1)\n",
    "````\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add rule to discourage overtaking vehicles to the left of the car \n",
    "\n",
    "**Implementation**\n",
    "1. Retrieve the vehicle horizontally closest and to the left to the ego vehicle.\n",
    "2. Record the position of the left vehicle and the position of the ego vehicle.\n",
    "3. In the following step, compare the position of the left vehicle and the ego vehicle to their recorded positions in the previous step.\n",
    "4. If the left vehicle was ahead of the ego vehicle in the previous step, but behind the ego vehicle in the current step, the ego vehicle is considered to have overtaken the left vehicle, and is penalized.\n",
    "\n",
    "\n",
    "**Advantages**\n",
    "- Penalizing overtaking to the left aligns with traffic laws in regions where overtaking is legally allowed only on the right (e.g., in countries with right-hand traffic).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Code**\n",
    "\n",
    "```python\n",
    "ego_vehicle = self.vehicle\n",
    "\n",
    "ego_current_position = ego_vehicle.position[0]\n",
    "left_vehicle = self.get_closest_left_vehicle( ego_vehicle)\n",
    "car_overtook_left_vehicle = False\n",
    "\n",
    "if (left_vehicle):\n",
    "    left_current_position = left_vehicle.position[0]\n",
    "\n",
    "ego_previous_position_exists =hasattr(self, \"ego_vehicle_previous_position\") and (self.ego_vehicle_previous_position is not None)\n",
    "left_previous_position_exists =hasattr(self, \"left_vehicle_previous_position\") and (self.left_vehicle_previous_position is not None)\n",
    "        \n",
    "if left_vehicle and ego_previous_position_exists and left_previous_position_exists:\n",
    "    left_vehicle_was_ahead_of_ego = ~(self.ego_vehicle_previous_position > self.left_vehicle_previous_position)\n",
    "    left_vehicle_now_behind_ego = ego_current_position > left_current_position\n",
    "    car_overtook_left_vehicle = (left_vehicle_was_ahead_of_ego and left_vehicle_now_behind_ego)\n",
    "    \n",
    "self.ego_vehicle_previous_position = ego_current_position\n",
    "# Reset previous position if no left car\n",
    "if left_vehicle is None:\n",
    "    self.left_vehicle_previous_position = None\n",
    "else: \n",
    "    self.left_vehicle_previous_position = left_vehicle.position[0]\n",
    "    \n",
    "if (car_overtook_left_vehicle):\n",
    "    return 1\n",
    "return 0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the custom reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the performance of our custom reward function by comparing it to the default reward function. During the model evaluation process, rewards are logged to a CSV file, capturing the values of various reward components. At the end of the evaluation, we sum up the logged rewards from both the custom and default reward functions and compare the results to assess the effectiveness of the custom function in achieving the desired outcomes (safety and real-world driving rules)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.training'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_model\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate_model\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils.training'"
     ]
    }
   ],
   "source": [
    "import highway_env\n",
    "from gymnasium import register\n",
    "import gymnasium\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from utils.training import train_model\n",
    "from utils.evaluation import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_and_normalize_rewards(file_path):\n",
    "    \"\"\"\n",
    "    Read and aggregate rewards from the CSV file, then normalize by the number of steps.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary of normalized rewards (average reward per step).\n",
    "    \"\"\"\n",
    "    rewards_summary = defaultdict(float)\n",
    "    total_steps = 0  # Count the number of rows (steps)\n",
    "\n",
    "    try:\n",
    "        with open(file_path, mode='r') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                total_steps += 1  # Increment step count for each row\n",
    "                for key, value in row.items():\n",
    "                    rewards_summary[key] += float(value)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {file_path} not found.\")\n",
    "        return None\n",
    "\n",
    "    # Normalize rewards by total steps\n",
    "    if total_steps > 0:\n",
    "        normalized_rewards = {key: value / total_steps for key, value in rewards_summary.items()}\n",
    "    else:\n",
    "        print(\"Error: No steps logged in the file.\")\n",
    "        return None\n",
    "\n",
    "    return normalized_rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default reward function\n",
    "First, we evaluate the performance using the default reward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register(\n",
    "    id='DefaultRewardEnv',\n",
    "    entry_point='HighwayEnvDefaultReward:HighwayEnvDefaultReward',\n",
    ")\n",
    "\n",
    "# Set log_rewards_enabled to True or False as per your requirement\n",
    "log_rewards_enabled = False\n",
    "log_performance_metrics_enabled=False\n",
    "\n",
    "# Create the environment with the custom parameter\n",
    "env = gymnasium.make('DefaultRewardEnv', \n",
    "                     render_mode='rgb_array', \n",
    "                     log_rewards_enabled=log_rewards_enabled, \n",
    "                     log_performance_metrics_enabled=log_performance_metrics_enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model with the default reward function\n",
    "train_model(\n",
    "    env=env,\n",
    "    session_name=\"default_reward_function\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rewards_enabled = True\n",
    "log_performance_metrics_enabled=True\n",
    "\n",
    "\n",
    "env = gymnasium.make('DefaultRewardEnv', render_mode='rgb_array', log_rewards_enabled=log_rewards_enabled, \n",
    "                     log_performance_metrics_enabled=log_performance_metrics_enabled)\n",
    "\n",
    "# evaluate the model with the default reward function\n",
    "evaluate_model(\n",
    "    env=env,\n",
    "    model_path=\"models/default_reward_function\",\n",
    "    algorithm='DQN',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file where rewards are logged\n",
    "reward_log_path = 'default_reward_log.csv'\n",
    "\n",
    "# Aggregate and normalize rewards\n",
    "normalized_rewards = aggregate_and_normalize_rewards(reward_log_path)\n",
    "\n",
    "if normalized_rewards:\n",
    "    print(\"Normalized Rewards (Average Per Step):\")\n",
    "    for reward_type, avg_reward in normalized_rewards.items():\n",
    "        print(f\"{reward_type}: {avg_reward:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file where rewards are logged\n",
    "performance_metrics_file_path =  'default_perfomance_metrics_log.csv'\n",
    "metrics = aggregate_and_normalize_rewards(performance_metrics_file_path)\n",
    "\n",
    "if metrics:\n",
    "    print(\"Normalized Metric Counts (Average Per Step):\")\n",
    "    for metric_name, avg_metric in metrics.items():\n",
    "        print(f\"{metric_name}: {avg_metric:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom reward function\n",
    "\n",
    "Next, we evaluate the performance using our custom reward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the custom environment\n",
    "register(\n",
    "    id='CustomRewardEnv',\n",
    "    entry_point='HighwayEnvCustomReward:HighwayEnvFastCustomReward',\n",
    ")\n",
    "\n",
    "# Set log_rewards_enabled to True or False as per your requirement\n",
    "log_rewards_enabled = False\n",
    "log_performance_metrics_enabled=False\n",
    "# Create the environment with the custom parameter\n",
    "env = gymnasium.make('CustomRewardEnv', render_mode='rgb_array', log_rewards_enabled=log_rewards_enabled, \n",
    "                     log_performance_metrics_enabled=log_performance_metrics_enabled)\n",
    "\n",
    "# Configuration updates for the environment\n",
    "config_updates = {\n",
    "    \"safe_distance_reward\": 0.1,\n",
    "    \"left_vehicle_overtaken_reward\": -0.5,\n",
    "    \"collision_reward\": -4,\n",
    "    \"smooth_driving_reward\" : 0.3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training with the updated environment\n",
    "train_model(\n",
    "    env=env,\n",
    "    config_updates=config_updates,\n",
    "    session_name=\"custom_reward_function\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set log_rewards_enabled to True or False as per your requirement\n",
    "log_rewards_enabled = False\n",
    "log_performance_metrics_enabled=False\n",
    "\n",
    "\n",
    "env = gymnasium.make('CustomRewardEnv', render_mode='rgb_array', log_rewards_enabled=log_rewards_enabled)\n",
    "\n",
    "\n",
    "# Configuration updates for the environment\n",
    "config_updates = {\n",
    "    \"safe_distance_reward\": 0.1,\n",
    "    \"left_vehicle_overtaken_reward\": -0.5,\n",
    "    \"collision_reward\": -4,\n",
    "    \"smooth_driving_reward\": 0.3,\n",
    "}\n",
    "\n",
    "# evaluate the model with the default reward function\n",
    "evaluate_model(\n",
    "    env=env,\n",
    "    config_updates=config_updates,\n",
    "    model_path=\"models/custom_reward_function\",\n",
    "    algorithm='DQN',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file where rewards are logged\n",
    "reward_log_path = 'custom_reward_log.csv'\n",
    "\n",
    "# Aggregate and normalize rewards\n",
    "normalized_rewards = aggregate_and_normalize_rewards(reward_log_path)\n",
    "\n",
    "if normalized_rewards:\n",
    "    print(\"Normalized Rewards (Average Per Step):\")\n",
    "    for reward_type, avg_reward in normalized_rewards.items():\n",
    "        print(f\"{reward_type}: {avg_reward:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file where rewards are logged\n",
    "performance_metrics_file_path =  'custom_perfomance_metrics_log.csv'\n",
    "metrics = aggregate_and_normalize_rewards(performance_metrics_file_path)\n",
    "\n",
    "if metrics:\n",
    "    print(\"Normalized Metric Counts (Average Per Step):\")\n",
    "    for metric_name, avg_metric in metrics.items():\n",
    "        print(f\"{metric_name}: {avg_metric:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Function Comparison\n",
    "\n",
    "Below are the normalized rewards (average per step) for the custom reward function and the default reward function:\n",
    "\n",
    "### Default Reward Function\n",
    "| Reward Type                    | Normalized Reward (Avg. Per Step) |\n",
    "|--------------------------------|----------------------------------|\n",
    "| collision_reward               | 0.1192                           |\n",
    "| right_lane_reward              | 0.4839                           |\n",
    "| high_speed_reward              | 0.9069                           |\n",
    "| on_road_reward                 | 1.0000                           |\n",
    "| safe_distance_reward           | 0.9107                           |\n",
    "| left_vehicle_overtaken_reward  | 0.1150                           |\n",
    "| smooth_driving_reward          | 0.7896                           |\n",
    "\n",
    "### Custom Reward Function\n",
    "| Reward Type                   | Normalized Reward (Avg. Per Step) |\n",
    "|-------------------------------|------------------------------------|\n",
    "| `collision_reward`            | 0.0149                           |\n",
    "| `right_lane_reward`           | 0.4843                           |\n",
    "| `high_speed_reward`           | 0.1630                           |\n",
    "| `on_road_reward`              | 1.0000                           |\n",
    "| `safe_distance_reward`        | 0.9868                           |\n",
    "| `left_vehicle_overtaken_reward` | 0.0262                          |\n",
    "| `smooth_driving_reward` | 0.9559                          |\n",
    "\n",
    "### Reward Behavior\n",
    "- **Collision Reward**: A lower value is better, as it reflects fewer collisions.\n",
    "- **Right Lane Reward**: A higher value is better, indicating more time spent in the correct lane.\n",
    "- **High Speed Reward**: A lower value is better, indicating that attention is paid to traffic density.\n",
    "- **On Road Reward**: A higher value is better, ensuring the vehicle stays on the road.\n",
    "- **Safe Distance Reward**: A higher value is better, showing better maintenance of a safe following distance.\n",
    "- **Left Vehicle Overtaken Reward**: A lower value is better, reflecting less overtaking maneuvers of vehicles driving to the left of the ego.\n",
    "- - **Smooth Driving Reward**: A higher value is better, reflecting more comfortable acceleration or decceleration (within +/- 2m/sˆ2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metric Comparison\n",
    "\n",
    "Below are the normalized predefined metrics (average per step) for the custom reward function and the default reward function:\n",
    "\n",
    "### Default Performance Metrics\n",
    "| Metric                          | Average Per Step |\n",
    "|---------------------------------|------------------|\n",
    "| collision_count                 | 0.1161           |\n",
    "| right_lane_count                | 0.7195           |\n",
    "| on_road_count                   | 0.0000           |\n",
    "| safe_distance_count             | 0.9396           |\n",
    "| left_vehicle_overtaken_count    | 0.1429           |\n",
    "| abrupt_accelerations_count      | 0.2218           |\n",
    "\n",
    "### Custom Performance Metrics\n",
    "| Metric                          | Average Per Step |\n",
    "|---------------------------------|------------------|\n",
    "| collision_count                 | 0.0033           |\n",
    "| right_lane_count                | 0.6944           |\n",
    "| on_road_count                   | 0.0000           |\n",
    "| safe_distance_count             | 0.9974           |\n",
    "| left_vehicle_overtaken_count    | 0.0109           |\n",
    "| abrupt_accelerations_count      | 0.0377           |\n",
    "\n",
    "### Performance metrics\n",
    "- **Collision Count**: A lower value is better, as it reflect the .\n",
    "- **Right Lane Count**: A higher value is better, indicating more time spent in the correct lane.\n",
    "- **High Speed Reward**: A lower value is better, indicating that attention is paid to traffic density.\n",
    "- **On Road Reward**: A higher value is better, ensuring the vehicle stays on the road.\n",
    "- **Safe Distance Reward**: A higher value is better, showing better maintenance of a safe following distance.\n",
    "- **Left Vehicle Overtaken Reward**: A lower value is better, reflecting less overtaking maneuvers of vehicles driving to the left of the ego.\n",
    "- - **Smooth Driving Reward**: A higher value is better, reflecting more comfortable acceleration or decceleration (within +/- 2m/sˆ2).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
