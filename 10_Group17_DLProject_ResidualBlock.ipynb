{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T22:58:01.303304Z",
     "start_time": "2024-12-17T22:58:01.290886Z"
    }
   },
   "cell_type": "code",
   "source": "# TODO: Change documentation after training: Conclusion",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MWKYRenO1H7"
   },
   "source": [
    "## 0. Notebook description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, the standard convolutional layers were replaced with Residual Blocks. Residual Blocks help mitigate the vanishing gradient problem by allowing gradients to flow through shortcut connections directly. This architecture enables the training of deeper networks, improving the model's ability to learn complex features and enhancing overall performance.\n",
    "\n",
    "Residual Blocks introduce shortcut connections that bypass one or more layers. These connections add the input of the block directly to the output. In a typical Residual Block, the input is passed through a series of convolutional layers, and the output of these layers is added to the original input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing libraries and loading the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZH4EnPodO1H8",
    "ExecuteTime": {
     "end_time": "2024-12-17T22:58:06.952304Z",
     "start_time": "2024-12-17T22:58:01.303304Z"
    }
   },
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import classification_report\n",
    "from utils.preprocessing import load_data, ReshapeAndScale, create_dataloaders\n",
    "from utils.fer2013_dataset import Fer2013Dataset"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmi4DkFbO1H9"
   },
   "source": [
    "First, we load the data into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "old1wNLVO1H_",
    "ExecuteTime": {
     "end_time": "2024-12-17T22:58:10.174048Z",
     "start_time": "2024-12-17T22:58:07.196096Z"
    }
   },
   "source": [
    "train_df = load_data(\"data/oversampled_train.csv\")\n",
    "val_df = load_data(\"data/validation.csv\")\n",
    "\n",
    "print(\"Training Data\")\n",
    "print(train_df.head(10))\n",
    "\n",
    "print(\"\\n\\nValidation Data\")\n",
    "print(val_df.head(10))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "                                              pixels  emotion\n",
      "0  208 209 210 209 209 208 206 215 162 47 44 47 5...        3\n",
      "1  164 166 170 168 168 169 171 172 173 172 172 17...        2\n",
      "2  246 248 248 248 244 124 45 37 45 98 141 160 16...        0\n",
      "3  1 0 0 1 1 1 2 3 2 3 3 9 10 7 6 7 9 10 16 16 22...        4\n",
      "4  208 201 211 209 196 192 177 177 187 186 193 19...        0\n",
      "5  4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 ...        2\n",
      "6  255 255 255 255 255 255 255 255 253 252 221 12...        3\n",
      "7  93 100 115 90 93 70 84 66 64 76 76 87 92 98 98...        3\n",
      "8  219 206 211 219 217 222 204 197 198 205 192 13...        6\n",
      "9  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 ...        0\n",
      "\n",
      "\n",
      "Validation Data\n",
      "   emotion                                             pixels\n",
      "0        3  254 253 253 254 254 254 252 252 253 252 253 25...\n",
      "1        4  92 70 64 65 62 64 91 139 167 181 186 187 191 1...\n",
      "2        5  20 19 16 23 24 25 34 24 25 60 113 123 139 149 ...\n",
      "3        4  79 82 83 84 85 89 92 90 93 95 94 97 95 97 93 8...\n",
      "4        2  117 105 95 73 72 65 49 66 92 105 126 154 177 1...\n",
      "5        3  138 138 138 118 129 146 138 120 89 57 50 47 54...\n",
      "6        5  141 132 104 64 37 27 24 23 19 18 19 20 23 22 2...\n",
      "7        3  17 23 19 14 31 27 28 41 52 69 88 96 106 108 10...\n",
      "8        6  117 110 91 49 44 46 47 41 26 39 57 71 83 91 10...\n",
      "9        3  89 141 163 153 136 128 127 125 121 116 30 23 2...\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7JiE5yJDO1IA",
    "outputId": "45dd2cb5-161b-4394-941f-82e2bc4c5699",
    "ExecuteTime": {
     "end_time": "2024-12-17T22:58:10.209269Z",
     "start_time": "2024-12-17T22:58:10.200935Z"
    }
   },
   "source": [
    "print(\"Training data shape:\", train_df.shape)\n",
    "print(\"Validation data shape\", val_df.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (40404, 2)\n",
      "Validation data shape (5742, 2)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHpwVsoYO1IH"
   },
   "source": [
    "## 2. Define a custom dataset\n",
    "We define a custom PyTorch dataset class, `Fer2013Dataset`, for handling the FER2013 data. The dataset is designed to load images (stored as pixel strings) and their corresponding emotion labels. It also supports optional transformations to preprocess the images during training. This setup makes it easy to integrate the dataset with PyTorch DataLoaders.\n",
    "\n",
    "The class is contained in the `utils/fer2013_dataset` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also apply the following preprocessing steps to convert the data into the desired format we can further work with:\n",
    "1. **Reshaping**:\n",
    "   - Convert the pixel string into a `48x48` matrix for visualization and processing.\n",
    "2. **Scaling**:\n",
    "   - Scale pixel values to the range `[0, 1]` by dividing the pixel values by 255.\n",
    "3. **Normalization**:\n",
    "   - Normalize pixel values to the range `[-1, 1]` by subtracting the mean and diving them by the standard deviation.\n",
    "\n",
    "\n",
    "These preprocessing steps are contained in the class `ReshapeAndScale`, which is available under path `utils/preprocessing.py`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T22:58:10.239651Z",
     "start_time": "2024-12-17T22:58:10.233920Z"
    }
   },
   "source": [
    "# Define a default transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    ReshapeAndScale(n_rows=48, n_cols=48),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T22:58:10.267902Z",
     "start_time": "2024-12-17T22:58:10.259828Z"
    }
   },
   "source": [
    "train_dataset = Fer2013Dataset(train_df, train_df['emotion'], transform=transform)\n",
    "val_dataset = Fer2013Dataset(val_df, val_df['emotion'], transform=transform)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders\n",
    "\n",
    "Here, we use the custom dataset to create a full dataset object and split it into training and validation sets (80% and 20%). Then, we create DataLoaders for both subsets to enable batch processing. The training DataLoader shuffles the data for better learning, while the validation DataLoader does not. Finally, we print the shapes of the batches to verify that everything works correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "rAQ30-FQO1IJ",
    "outputId": "aae71765-1a27-4d1a-9fe0-d34cee950122",
    "ExecuteTime": {
     "end_time": "2024-12-17T22:58:10.295943Z",
     "start_time": "2024-12-17T22:58:10.289427Z"
    }
   },
   "source": [
    "batch_size = 32\n",
    "train_loader, val_loader = create_dataloaders(train_dataset, val_dataset, batch_size)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pge_vvuYO1IN"
   },
   "source": [
    "## 3. Define the CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a custom Convolutional Neural Network (CNN) for emotion recognition. The model includes multiple Residual Blocks with batch normalization, dropout for regularization, average pooling for downsampling, and fully connected layers for classification.\n",
    "\n",
    "The network dynamically calculates the flattened size needed for the fully connected layers based on the input size (48x48 grayscale images). Finally, the model is instantiated, moved to the available device (CPU or GPU), and its architecture is printed for verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvFxqkzxO1IO",
    "outputId": "a7824d62-eb91-42c3-8f2f-def089a2ea5c",
    "ExecuteTime": {
     "end_time": "2024-12-17T22:58:10.391552Z",
     "start_time": "2024-12-17T22:58:10.330635Z"
    }
   },
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        return F.relu(x)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # 1st Conv Layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding='valid')\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "\n",
    "        # 2nd Residual Block\n",
    "        self.res_block1 = ResidualBlock(32, 64, stride=1)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 3rd Residual Block\n",
    "        self.res_block2 = ResidualBlock(64, 128, stride=1)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Calculate the flatten size dynamically\n",
    "        self.flatten_size = self._get_flatten_size()\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(self.flatten_size, 250)\n",
    "        self.dropout_fc = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(250, 7)  # 7 classes for emotion recognition\n",
    "\n",
    "    def _get_flatten_size(self):\n",
    "        dummy_input = torch.zeros(1, 1, 48, 48)\n",
    "        dummy_output = self._forward_conv_layers(dummy_input)\n",
    "        return dummy_output.numel()\n",
    "\n",
    "    def _forward_conv_layers(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.res_block1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.res_block2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN()\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model to verify layers\n",
    "print(model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (res_block1): ResidualBlock(\n",
      "    (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (res_block2): ResidualBlock(\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pool2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (fc1): Linear(in_features=15488, out_features=250, bias=True)\n",
      "  (dropout_fc): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=250, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we define the loss function and optimizer for training the model:\n",
    "- **Loss Function**: `CrossEntropyLoss` is used, which is well-suited for multi-class classification tasks like emotion recognition.\n",
    "- **Optimizer**: The Adam optimizer is initialized with a learning rate of `0.0001` to update the model parameters during training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WFyRGyfXO1IR",
    "ExecuteTime": {
     "end_time": "2024-12-17T22:58:10.404514Z",
     "start_time": "2024-12-17T22:58:10.393620Z"
    }
   },
   "source": [
    "criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # call optimizer"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMHZrQsZO1IR"
   },
   "source": [
    "## 5. Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we define the training loop for the CNN:\n",
    "- **Number of Epochs**: The model is trained for 35 epochs.\n",
    "- **Training Process**:\n",
    "  - The model is set to training mode.\n",
    "  - For each batch, we move inputs and labels to the appropriate device, clear the gradients, perform forward and backward passes, and update the model's parameters using the optimizer.\n",
    "  - The running loss is tracked and printed every 100 batches for monitoring."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "iX3PbDPqO1IR",
    "outputId": "09fb2a30-0bb4-47b5-e1d7-2d0d47db40ca",
    "ExecuteTime": {
     "end_time": "2024-12-18T03:36:02.325513Z",
     "start_time": "2024-12-17T22:58:10.436653Z"
    }
   },
   "source": [
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir='tensorboard-runs/10_Group17_DLProject')\n",
    "\n",
    "checkpoint_path = 'model-checkpoints/10_Group17_DLProject'\n",
    "os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "model_save_path = 'models'\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "num_epochs = 35\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 100 == 0:  # Print every 100 batches\n",
    "            avg_loss = running_loss / 100\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {avg_loss:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Log loss to TensorBoard\n",
    "            writer.add_scalar('Loss/train', avg_loss, epoch)\n",
    "\n",
    "    # Save checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': running_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, f\"{checkpoint_path}/checkpoint_epoch_{epoch + 1}.pth\")\n",
    "        print(f\"Checkpoint saved at epoch {epoch + 1}\")\n",
    "\n",
    "# Save the final model after training\n",
    "final_model_path = os.path.join(model_save_path, '10_Group17_DLProject.pth')\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final model saved at {final_model_path}\")\n",
    "\n",
    "# Close the writer after training\n",
    "writer.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/35], Step [100/1263], Loss: 1.9230\n",
      "Epoch [1/35], Step [200/1263], Loss: 1.8306\n",
      "Epoch [1/35], Step [300/1263], Loss: 1.7811\n",
      "Epoch [1/35], Step [400/1263], Loss: 1.7017\n",
      "Epoch [1/35], Step [500/1263], Loss: 1.6253\n",
      "Epoch [1/35], Step [600/1263], Loss: 1.5966\n",
      "Epoch [1/35], Step [700/1263], Loss: 1.5648\n",
      "Epoch [1/35], Step [800/1263], Loss: 1.5390\n",
      "Epoch [1/35], Step [900/1263], Loss: 1.4994\n",
      "Epoch [1/35], Step [1000/1263], Loss: 1.4578\n",
      "Epoch [1/35], Step [1100/1263], Loss: 1.4405\n",
      "Epoch [1/35], Step [1200/1263], Loss: 1.4166\n",
      "Epoch [2/35], Step [100/1263], Loss: 1.3363\n",
      "Epoch [2/35], Step [200/1263], Loss: 1.3444\n",
      "Epoch [2/35], Step [300/1263], Loss: 1.3592\n",
      "Epoch [2/35], Step [400/1263], Loss: 1.2947\n",
      "Epoch [2/35], Step [500/1263], Loss: 1.2827\n",
      "Epoch [2/35], Step [600/1263], Loss: 1.2776\n",
      "Epoch [2/35], Step [700/1263], Loss: 1.2905\n",
      "Epoch [2/35], Step [800/1263], Loss: 1.2753\n",
      "Epoch [2/35], Step [900/1263], Loss: 1.2548\n",
      "Epoch [2/35], Step [1000/1263], Loss: 1.2643\n",
      "Epoch [2/35], Step [1100/1263], Loss: 1.2266\n",
      "Epoch [2/35], Step [1200/1263], Loss: 1.2415\n",
      "Epoch [3/35], Step [100/1263], Loss: 1.1616\n",
      "Epoch [3/35], Step [200/1263], Loss: 1.1965\n",
      "Epoch [3/35], Step [300/1263], Loss: 1.1325\n",
      "Epoch [3/35], Step [400/1263], Loss: 1.1774\n",
      "Epoch [3/35], Step [500/1263], Loss: 1.1536\n",
      "Epoch [3/35], Step [600/1263], Loss: 1.1241\n",
      "Epoch [3/35], Step [700/1263], Loss: 1.1307\n",
      "Epoch [3/35], Step [800/1263], Loss: 1.1388\n",
      "Epoch [3/35], Step [900/1263], Loss: 1.1363\n",
      "Epoch [3/35], Step [1000/1263], Loss: 1.0969\n",
      "Epoch [3/35], Step [1100/1263], Loss: 1.1236\n",
      "Epoch [3/35], Step [1200/1263], Loss: 1.0828\n",
      "Epoch [4/35], Step [100/1263], Loss: 1.0432\n",
      "Epoch [4/35], Step [200/1263], Loss: 1.0649\n",
      "Epoch [4/35], Step [300/1263], Loss: 1.0824\n",
      "Epoch [4/35], Step [400/1263], Loss: 1.0639\n",
      "Epoch [4/35], Step [500/1263], Loss: 1.0280\n",
      "Epoch [4/35], Step [600/1263], Loss: 1.0277\n",
      "Epoch [4/35], Step [700/1263], Loss: 1.0655\n",
      "Epoch [4/35], Step [800/1263], Loss: 1.0481\n",
      "Epoch [4/35], Step [900/1263], Loss: 1.0301\n",
      "Epoch [4/35], Step [1000/1263], Loss: 1.0424\n",
      "Epoch [4/35], Step [1100/1263], Loss: 1.0511\n",
      "Epoch [4/35], Step [1200/1263], Loss: 1.0031\n",
      "Epoch [5/35], Step [100/1263], Loss: 0.9666\n",
      "Epoch [5/35], Step [200/1263], Loss: 0.9608\n",
      "Epoch [5/35], Step [300/1263], Loss: 0.9613\n",
      "Epoch [5/35], Step [400/1263], Loss: 0.9421\n",
      "Epoch [5/35], Step [500/1263], Loss: 0.9560\n",
      "Epoch [5/35], Step [600/1263], Loss: 0.9734\n",
      "Epoch [5/35], Step [700/1263], Loss: 0.9718\n",
      "Epoch [5/35], Step [800/1263], Loss: 0.9664\n",
      "Epoch [5/35], Step [900/1263], Loss: 0.9573\n",
      "Epoch [5/35], Step [1000/1263], Loss: 0.9474\n",
      "Epoch [5/35], Step [1100/1263], Loss: 0.9567\n",
      "Epoch [5/35], Step [1200/1263], Loss: 0.9472\n",
      "Epoch [6/35], Step [100/1263], Loss: 0.9086\n",
      "Epoch [6/35], Step [200/1263], Loss: 0.8962\n",
      "Epoch [6/35], Step [300/1263], Loss: 0.8980\n",
      "Epoch [6/35], Step [400/1263], Loss: 0.8953\n",
      "Epoch [6/35], Step [500/1263], Loss: 0.9120\n",
      "Epoch [6/35], Step [600/1263], Loss: 0.9332\n",
      "Epoch [6/35], Step [700/1263], Loss: 0.9088\n",
      "Epoch [6/35], Step [800/1263], Loss: 0.8807\n",
      "Epoch [6/35], Step [900/1263], Loss: 0.8812\n",
      "Epoch [6/35], Step [1000/1263], Loss: 0.9001\n",
      "Epoch [6/35], Step [1100/1263], Loss: 0.8763\n",
      "Epoch [6/35], Step [1200/1263], Loss: 0.8749\n",
      "Epoch [7/35], Step [100/1263], Loss: 0.8259\n",
      "Epoch [7/35], Step [200/1263], Loss: 0.8354\n",
      "Epoch [7/35], Step [300/1263], Loss: 0.8451\n",
      "Epoch [7/35], Step [400/1263], Loss: 0.7949\n",
      "Epoch [7/35], Step [500/1263], Loss: 0.8170\n",
      "Epoch [7/35], Step [600/1263], Loss: 0.8640\n",
      "Epoch [7/35], Step [700/1263], Loss: 0.8396\n",
      "Epoch [7/35], Step [800/1263], Loss: 0.7983\n",
      "Epoch [7/35], Step [900/1263], Loss: 0.8272\n",
      "Epoch [7/35], Step [1000/1263], Loss: 0.8860\n",
      "Epoch [7/35], Step [1100/1263], Loss: 0.8268\n",
      "Epoch [7/35], Step [1200/1263], Loss: 0.8409\n",
      "Epoch [8/35], Step [100/1263], Loss: 0.7873\n",
      "Epoch [8/35], Step [200/1263], Loss: 0.7608\n",
      "Epoch [8/35], Step [300/1263], Loss: 0.7814\n",
      "Epoch [8/35], Step [400/1263], Loss: 0.7577\n",
      "Epoch [8/35], Step [500/1263], Loss: 0.8139\n",
      "Epoch [8/35], Step [600/1263], Loss: 0.7700\n",
      "Epoch [8/35], Step [700/1263], Loss: 0.8005\n",
      "Epoch [8/35], Step [800/1263], Loss: 0.7754\n",
      "Epoch [8/35], Step [900/1263], Loss: 0.8063\n",
      "Epoch [8/35], Step [1000/1263], Loss: 0.7732\n",
      "Epoch [8/35], Step [1100/1263], Loss: 0.7416\n",
      "Epoch [8/35], Step [1200/1263], Loss: 0.7722\n",
      "Epoch [9/35], Step [100/1263], Loss: 0.7245\n",
      "Epoch [9/35], Step [200/1263], Loss: 0.7216\n",
      "Epoch [9/35], Step [300/1263], Loss: 0.7306\n",
      "Epoch [9/35], Step [400/1263], Loss: 0.7694\n",
      "Epoch [9/35], Step [500/1263], Loss: 0.7508\n",
      "Epoch [9/35], Step [600/1263], Loss: 0.7053\n",
      "Epoch [9/35], Step [700/1263], Loss: 0.7483\n",
      "Epoch [9/35], Step [800/1263], Loss: 0.7227\n",
      "Epoch [9/35], Step [900/1263], Loss: 0.7136\n",
      "Epoch [9/35], Step [1000/1263], Loss: 0.7426\n",
      "Epoch [9/35], Step [1100/1263], Loss: 0.6970\n",
      "Epoch [9/35], Step [1200/1263], Loss: 0.7297\n",
      "Epoch [10/35], Step [100/1263], Loss: 0.6817\n",
      "Epoch [10/35], Step [200/1263], Loss: 0.6609\n",
      "Epoch [10/35], Step [300/1263], Loss: 0.6655\n",
      "Epoch [10/35], Step [400/1263], Loss: 0.6880\n",
      "Epoch [10/35], Step [500/1263], Loss: 0.6680\n",
      "Epoch [10/35], Step [600/1263], Loss: 0.7026\n",
      "Epoch [10/35], Step [700/1263], Loss: 0.6917\n",
      "Epoch [10/35], Step [800/1263], Loss: 0.6776\n",
      "Epoch [10/35], Step [900/1263], Loss: 0.6821\n",
      "Epoch [10/35], Step [1000/1263], Loss: 0.6982\n",
      "Epoch [10/35], Step [1100/1263], Loss: 0.7090\n",
      "Epoch [10/35], Step [1200/1263], Loss: 0.7087\n",
      "Checkpoint saved at epoch 10\n",
      "Epoch [11/35], Step [100/1263], Loss: 0.6303\n",
      "Epoch [11/35], Step [200/1263], Loss: 0.6306\n",
      "Epoch [11/35], Step [300/1263], Loss: 0.6167\n",
      "Epoch [11/35], Step [400/1263], Loss: 0.6510\n",
      "Epoch [11/35], Step [500/1263], Loss: 0.6663\n",
      "Epoch [11/35], Step [600/1263], Loss: 0.6456\n",
      "Epoch [11/35], Step [700/1263], Loss: 0.6065\n",
      "Epoch [11/35], Step [800/1263], Loss: 0.6407\n",
      "Epoch [11/35], Step [900/1263], Loss: 0.6609\n",
      "Epoch [11/35], Step [1000/1263], Loss: 0.6467\n",
      "Epoch [11/35], Step [1100/1263], Loss: 0.6457\n",
      "Epoch [11/35], Step [1200/1263], Loss: 0.6422\n",
      "Epoch [12/35], Step [100/1263], Loss: 0.5926\n",
      "Epoch [12/35], Step [200/1263], Loss: 0.5732\n",
      "Epoch [12/35], Step [300/1263], Loss: 0.6039\n",
      "Epoch [12/35], Step [400/1263], Loss: 0.5875\n",
      "Epoch [12/35], Step [500/1263], Loss: 0.6104\n",
      "Epoch [12/35], Step [600/1263], Loss: 0.5890\n",
      "Epoch [12/35], Step [700/1263], Loss: 0.5805\n",
      "Epoch [12/35], Step [800/1263], Loss: 0.6087\n",
      "Epoch [12/35], Step [900/1263], Loss: 0.5920\n",
      "Epoch [12/35], Step [1000/1263], Loss: 0.6179\n",
      "Epoch [12/35], Step [1100/1263], Loss: 0.6088\n",
      "Epoch [12/35], Step [1200/1263], Loss: 0.6152\n",
      "Epoch [13/35], Step [100/1263], Loss: 0.5331\n",
      "Epoch [13/35], Step [200/1263], Loss: 0.5293\n",
      "Epoch [13/35], Step [300/1263], Loss: 0.5545\n",
      "Epoch [13/35], Step [400/1263], Loss: 0.5148\n",
      "Epoch [13/35], Step [500/1263], Loss: 0.5692\n",
      "Epoch [13/35], Step [600/1263], Loss: 0.5800\n",
      "Epoch [13/35], Step [700/1263], Loss: 0.5697\n",
      "Epoch [13/35], Step [800/1263], Loss: 0.5781\n",
      "Epoch [13/35], Step [900/1263], Loss: 0.5728\n",
      "Epoch [13/35], Step [1000/1263], Loss: 0.5731\n",
      "Epoch [13/35], Step [1100/1263], Loss: 0.5423\n",
      "Epoch [13/35], Step [1200/1263], Loss: 0.5651\n",
      "Epoch [14/35], Step [100/1263], Loss: 0.5065\n",
      "Epoch [14/35], Step [200/1263], Loss: 0.4954\n",
      "Epoch [14/35], Step [300/1263], Loss: 0.5199\n",
      "Epoch [14/35], Step [400/1263], Loss: 0.5159\n",
      "Epoch [14/35], Step [500/1263], Loss: 0.5491\n",
      "Epoch [14/35], Step [600/1263], Loss: 0.5108\n",
      "Epoch [14/35], Step [700/1263], Loss: 0.5400\n",
      "Epoch [14/35], Step [800/1263], Loss: 0.5554\n",
      "Epoch [14/35], Step [900/1263], Loss: 0.5275\n",
      "Epoch [14/35], Step [1000/1263], Loss: 0.5335\n",
      "Epoch [14/35], Step [1100/1263], Loss: 0.5228\n",
      "Epoch [14/35], Step [1200/1263], Loss: 0.5228\n",
      "Epoch [15/35], Step [100/1263], Loss: 0.5056\n",
      "Epoch [15/35], Step [200/1263], Loss: 0.4970\n",
      "Epoch [15/35], Step [300/1263], Loss: 0.4925\n",
      "Epoch [15/35], Step [400/1263], Loss: 0.4783\n",
      "Epoch [15/35], Step [500/1263], Loss: 0.5032\n",
      "Epoch [15/35], Step [600/1263], Loss: 0.5059\n",
      "Epoch [15/35], Step [700/1263], Loss: 0.5157\n",
      "Epoch [15/35], Step [800/1263], Loss: 0.4882\n",
      "Epoch [15/35], Step [900/1263], Loss: 0.4686\n",
      "Epoch [15/35], Step [1000/1263], Loss: 0.5208\n",
      "Epoch [15/35], Step [1100/1263], Loss: 0.4754\n",
      "Epoch [15/35], Step [1200/1263], Loss: 0.5049\n",
      "Epoch [16/35], Step [100/1263], Loss: 0.4716\n",
      "Epoch [16/35], Step [200/1263], Loss: 0.4374\n",
      "Epoch [16/35], Step [300/1263], Loss: 0.4579\n",
      "Epoch [16/35], Step [400/1263], Loss: 0.4458\n",
      "Epoch [16/35], Step [500/1263], Loss: 0.4530\n",
      "Epoch [16/35], Step [600/1263], Loss: 0.4566\n",
      "Epoch [16/35], Step [700/1263], Loss: 0.4590\n",
      "Epoch [16/35], Step [800/1263], Loss: 0.4646\n",
      "Epoch [16/35], Step [900/1263], Loss: 0.4679\n",
      "Epoch [16/35], Step [1000/1263], Loss: 0.4692\n",
      "Epoch [16/35], Step [1100/1263], Loss: 0.4749\n",
      "Epoch [16/35], Step [1200/1263], Loss: 0.4659\n",
      "Epoch [17/35], Step [100/1263], Loss: 0.4206\n",
      "Epoch [17/35], Step [200/1263], Loss: 0.4603\n",
      "Epoch [17/35], Step [300/1263], Loss: 0.4292\n",
      "Epoch [17/35], Step [400/1263], Loss: 0.4219\n",
      "Epoch [17/35], Step [500/1263], Loss: 0.4420\n",
      "Epoch [17/35], Step [600/1263], Loss: 0.4364\n",
      "Epoch [17/35], Step [700/1263], Loss: 0.4361\n",
      "Epoch [17/35], Step [800/1263], Loss: 0.4493\n",
      "Epoch [17/35], Step [900/1263], Loss: 0.4529\n",
      "Epoch [17/35], Step [1000/1263], Loss: 0.4493\n",
      "Epoch [17/35], Step [1100/1263], Loss: 0.4409\n",
      "Epoch [17/35], Step [1200/1263], Loss: 0.4564\n",
      "Epoch [18/35], Step [100/1263], Loss: 0.4010\n",
      "Epoch [18/35], Step [200/1263], Loss: 0.3884\n",
      "Epoch [18/35], Step [300/1263], Loss: 0.4111\n",
      "Epoch [18/35], Step [400/1263], Loss: 0.4002\n",
      "Epoch [18/35], Step [500/1263], Loss: 0.4160\n",
      "Epoch [18/35], Step [600/1263], Loss: 0.4097\n",
      "Epoch [18/35], Step [700/1263], Loss: 0.4241\n",
      "Epoch [18/35], Step [800/1263], Loss: 0.4161\n",
      "Epoch [18/35], Step [900/1263], Loss: 0.4239\n",
      "Epoch [18/35], Step [1000/1263], Loss: 0.4148\n",
      "Epoch [18/35], Step [1100/1263], Loss: 0.4236\n",
      "Epoch [18/35], Step [1200/1263], Loss: 0.4383\n",
      "Epoch [19/35], Step [100/1263], Loss: 0.3599\n",
      "Epoch [19/35], Step [200/1263], Loss: 0.3878\n",
      "Epoch [19/35], Step [300/1263], Loss: 0.4178\n",
      "Epoch [19/35], Step [400/1263], Loss: 0.3811\n",
      "Epoch [19/35], Step [500/1263], Loss: 0.3782\n",
      "Epoch [19/35], Step [600/1263], Loss: 0.3938\n",
      "Epoch [19/35], Step [700/1263], Loss: 0.3891\n",
      "Epoch [19/35], Step [800/1263], Loss: 0.3915\n",
      "Epoch [19/35], Step [900/1263], Loss: 0.3639\n",
      "Epoch [19/35], Step [1000/1263], Loss: 0.4036\n",
      "Epoch [19/35], Step [1100/1263], Loss: 0.3981\n",
      "Epoch [19/35], Step [1200/1263], Loss: 0.3957\n",
      "Epoch [20/35], Step [100/1263], Loss: 0.3446\n",
      "Epoch [20/35], Step [200/1263], Loss: 0.3332\n",
      "Epoch [20/35], Step [300/1263], Loss: 0.3533\n",
      "Epoch [20/35], Step [400/1263], Loss: 0.3359\n",
      "Epoch [20/35], Step [500/1263], Loss: 0.3526\n",
      "Epoch [20/35], Step [600/1263], Loss: 0.3632\n",
      "Epoch [20/35], Step [700/1263], Loss: 0.3750\n",
      "Epoch [20/35], Step [800/1263], Loss: 0.3527\n",
      "Epoch [20/35], Step [900/1263], Loss: 0.3660\n",
      "Epoch [20/35], Step [1000/1263], Loss: 0.3766\n",
      "Epoch [20/35], Step [1100/1263], Loss: 0.3865\n",
      "Epoch [20/35], Step [1200/1263], Loss: 0.3700\n",
      "Checkpoint saved at epoch 20\n",
      "Epoch [21/35], Step [100/1263], Loss: 0.3402\n",
      "Epoch [21/35], Step [200/1263], Loss: 0.3214\n",
      "Epoch [21/35], Step [300/1263], Loss: 0.3402\n",
      "Epoch [21/35], Step [400/1263], Loss: 0.3463\n",
      "Epoch [21/35], Step [500/1263], Loss: 0.3370\n",
      "Epoch [21/35], Step [600/1263], Loss: 0.3641\n",
      "Epoch [21/35], Step [700/1263], Loss: 0.3521\n",
      "Epoch [21/35], Step [800/1263], Loss: 0.3500\n",
      "Epoch [21/35], Step [900/1263], Loss: 0.3722\n",
      "Epoch [21/35], Step [1000/1263], Loss: 0.3662\n",
      "Epoch [21/35], Step [1100/1263], Loss: 0.3614\n",
      "Epoch [21/35], Step [1200/1263], Loss: 0.3599\n",
      "Epoch [22/35], Step [100/1263], Loss: 0.3327\n",
      "Epoch [22/35], Step [200/1263], Loss: 0.3269\n",
      "Epoch [22/35], Step [300/1263], Loss: 0.3342\n",
      "Epoch [22/35], Step [400/1263], Loss: 0.3193\n",
      "Epoch [22/35], Step [500/1263], Loss: 0.3406\n",
      "Epoch [22/35], Step [600/1263], Loss: 0.3468\n",
      "Epoch [22/35], Step [700/1263], Loss: 0.3350\n",
      "Epoch [22/35], Step [800/1263], Loss: 0.3325\n",
      "Epoch [22/35], Step [900/1263], Loss: 0.3248\n",
      "Epoch [22/35], Step [1000/1263], Loss: 0.3233\n",
      "Epoch [22/35], Step [1100/1263], Loss: 0.3436\n",
      "Epoch [22/35], Step [1200/1263], Loss: 0.3278\n",
      "Epoch [23/35], Step [100/1263], Loss: 0.2921\n",
      "Epoch [23/35], Step [200/1263], Loss: 0.2926\n",
      "Epoch [23/35], Step [300/1263], Loss: 0.3064\n",
      "Epoch [23/35], Step [400/1263], Loss: 0.3204\n",
      "Epoch [23/35], Step [500/1263], Loss: 0.3197\n",
      "Epoch [23/35], Step [600/1263], Loss: 0.3273\n",
      "Epoch [23/35], Step [700/1263], Loss: 0.3180\n",
      "Epoch [23/35], Step [800/1263], Loss: 0.3058\n",
      "Epoch [23/35], Step [900/1263], Loss: 0.2934\n",
      "Epoch [23/35], Step [1000/1263], Loss: 0.3283\n",
      "Epoch [23/35], Step [1100/1263], Loss: 0.3191\n",
      "Epoch [23/35], Step [1200/1263], Loss: 0.3341\n",
      "Epoch [24/35], Step [100/1263], Loss: 0.2702\n",
      "Epoch [24/35], Step [200/1263], Loss: 0.2988\n",
      "Epoch [24/35], Step [300/1263], Loss: 0.2856\n",
      "Epoch [24/35], Step [400/1263], Loss: 0.2880\n",
      "Epoch [24/35], Step [500/1263], Loss: 0.2931\n",
      "Epoch [24/35], Step [600/1263], Loss: 0.2891\n",
      "Epoch [24/35], Step [700/1263], Loss: 0.3077\n",
      "Epoch [24/35], Step [800/1263], Loss: 0.3018\n",
      "Epoch [24/35], Step [900/1263], Loss: 0.3162\n",
      "Epoch [24/35], Step [1000/1263], Loss: 0.3110\n",
      "Epoch [24/35], Step [1100/1263], Loss: 0.3190\n",
      "Epoch [24/35], Step [1200/1263], Loss: 0.3031\n",
      "Epoch [25/35], Step [100/1263], Loss: 0.2831\n",
      "Epoch [25/35], Step [200/1263], Loss: 0.2797\n",
      "Epoch [25/35], Step [300/1263], Loss: 0.2757\n",
      "Epoch [25/35], Step [400/1263], Loss: 0.2878\n",
      "Epoch [25/35], Step [500/1263], Loss: 0.2868\n",
      "Epoch [25/35], Step [600/1263], Loss: 0.2911\n",
      "Epoch [25/35], Step [700/1263], Loss: 0.2766\n",
      "Epoch [25/35], Step [800/1263], Loss: 0.3012\n",
      "Epoch [25/35], Step [900/1263], Loss: 0.2905\n",
      "Epoch [25/35], Step [1000/1263], Loss: 0.2782\n",
      "Epoch [25/35], Step [1100/1263], Loss: 0.2900\n",
      "Epoch [25/35], Step [1200/1263], Loss: 0.2965\n",
      "Epoch [26/35], Step [100/1263], Loss: 0.2478\n",
      "Epoch [26/35], Step [200/1263], Loss: 0.2609\n",
      "Epoch [26/35], Step [300/1263], Loss: 0.2697\n",
      "Epoch [26/35], Step [400/1263], Loss: 0.2624\n",
      "Epoch [26/35], Step [500/1263], Loss: 0.2696\n",
      "Epoch [26/35], Step [600/1263], Loss: 0.2817\n",
      "Epoch [26/35], Step [700/1263], Loss: 0.2669\n",
      "Epoch [26/35], Step [800/1263], Loss: 0.2488\n",
      "Epoch [26/35], Step [900/1263], Loss: 0.2940\n",
      "Epoch [26/35], Step [1000/1263], Loss: 0.2707\n",
      "Epoch [26/35], Step [1100/1263], Loss: 0.2911\n",
      "Epoch [26/35], Step [1200/1263], Loss: 0.2833\n",
      "Epoch [27/35], Step [100/1263], Loss: 0.2687\n",
      "Epoch [27/35], Step [200/1263], Loss: 0.2425\n",
      "Epoch [27/35], Step [300/1263], Loss: 0.2579\n",
      "Epoch [27/35], Step [400/1263], Loss: 0.2554\n",
      "Epoch [27/35], Step [500/1263], Loss: 0.2492\n",
      "Epoch [27/35], Step [600/1263], Loss: 0.2649\n",
      "Epoch [27/35], Step [700/1263], Loss: 0.2737\n",
      "Epoch [27/35], Step [800/1263], Loss: 0.2654\n",
      "Epoch [27/35], Step [900/1263], Loss: 0.2733\n",
      "Epoch [27/35], Step [1000/1263], Loss: 0.2687\n",
      "Epoch [27/35], Step [1100/1263], Loss: 0.2776\n",
      "Epoch [27/35], Step [1200/1263], Loss: 0.2605\n",
      "Epoch [28/35], Step [100/1263], Loss: 0.2358\n",
      "Epoch [28/35], Step [200/1263], Loss: 0.2433\n",
      "Epoch [28/35], Step [300/1263], Loss: 0.2399\n",
      "Epoch [28/35], Step [400/1263], Loss: 0.2317\n",
      "Epoch [28/35], Step [500/1263], Loss: 0.2330\n",
      "Epoch [28/35], Step [600/1263], Loss: 0.2325\n",
      "Epoch [28/35], Step [700/1263], Loss: 0.2560\n",
      "Epoch [28/35], Step [800/1263], Loss: 0.2538\n",
      "Epoch [28/35], Step [900/1263], Loss: 0.2518\n",
      "Epoch [28/35], Step [1000/1263], Loss: 0.2693\n",
      "Epoch [28/35], Step [1100/1263], Loss: 0.2377\n",
      "Epoch [28/35], Step [1200/1263], Loss: 0.2922\n",
      "Epoch [29/35], Step [100/1263], Loss: 0.2439\n",
      "Epoch [29/35], Step [200/1263], Loss: 0.2282\n",
      "Epoch [29/35], Step [300/1263], Loss: 0.2253\n",
      "Epoch [29/35], Step [400/1263], Loss: 0.2583\n",
      "Epoch [29/35], Step [500/1263], Loss: 0.2593\n",
      "Epoch [29/35], Step [600/1263], Loss: 0.2478\n",
      "Epoch [29/35], Step [700/1263], Loss: 0.2489\n",
      "Epoch [29/35], Step [800/1263], Loss: 0.2429\n",
      "Epoch [29/35], Step [900/1263], Loss: 0.2688\n",
      "Epoch [29/35], Step [1000/1263], Loss: 0.2322\n",
      "Epoch [29/35], Step [1100/1263], Loss: 0.2296\n",
      "Epoch [29/35], Step [1200/1263], Loss: 0.2471\n",
      "Epoch [30/35], Step [100/1263], Loss: 0.2353\n",
      "Epoch [30/35], Step [200/1263], Loss: 0.2266\n",
      "Epoch [30/35], Step [300/1263], Loss: 0.2183\n",
      "Epoch [30/35], Step [400/1263], Loss: 0.2181\n",
      "Epoch [30/35], Step [500/1263], Loss: 0.2435\n",
      "Epoch [30/35], Step [600/1263], Loss: 0.2264\n",
      "Epoch [30/35], Step [700/1263], Loss: 0.2287\n",
      "Epoch [30/35], Step [800/1263], Loss: 0.2539\n",
      "Epoch [30/35], Step [900/1263], Loss: 0.2433\n",
      "Epoch [30/35], Step [1000/1263], Loss: 0.2370\n",
      "Epoch [30/35], Step [1100/1263], Loss: 0.2424\n",
      "Epoch [30/35], Step [1200/1263], Loss: 0.2427\n",
      "Checkpoint saved at epoch 30\n",
      "Epoch [31/35], Step [100/1263], Loss: 0.2033\n",
      "Epoch [31/35], Step [200/1263], Loss: 0.1986\n",
      "Epoch [31/35], Step [300/1263], Loss: 0.2175\n",
      "Epoch [31/35], Step [400/1263], Loss: 0.2094\n",
      "Epoch [31/35], Step [500/1263], Loss: 0.2309\n",
      "Epoch [31/35], Step [600/1263], Loss: 0.2431\n",
      "Epoch [31/35], Step [700/1263], Loss: 0.2334\n",
      "Epoch [31/35], Step [800/1263], Loss: 0.2304\n",
      "Epoch [31/35], Step [900/1263], Loss: 0.2331\n",
      "Epoch [31/35], Step [1000/1263], Loss: 0.2265\n",
      "Epoch [31/35], Step [1100/1263], Loss: 0.2328\n",
      "Epoch [31/35], Step [1200/1263], Loss: 0.2312\n",
      "Epoch [32/35], Step [100/1263], Loss: 0.2021\n",
      "Epoch [32/35], Step [200/1263], Loss: 0.2233\n",
      "Epoch [32/35], Step [300/1263], Loss: 0.2083\n",
      "Epoch [32/35], Step [400/1263], Loss: 0.2191\n",
      "Epoch [32/35], Step [500/1263], Loss: 0.2268\n",
      "Epoch [32/35], Step [600/1263], Loss: 0.2167\n",
      "Epoch [32/35], Step [700/1263], Loss: 0.2273\n",
      "Epoch [32/35], Step [800/1263], Loss: 0.2277\n",
      "Epoch [32/35], Step [900/1263], Loss: 0.2111\n",
      "Epoch [32/35], Step [1000/1263], Loss: 0.2029\n",
      "Epoch [32/35], Step [1100/1263], Loss: 0.2127\n",
      "Epoch [32/35], Step [1200/1263], Loss: 0.2254\n",
      "Epoch [33/35], Step [100/1263], Loss: 0.2129\n",
      "Epoch [33/35], Step [200/1263], Loss: 0.1846\n",
      "Epoch [33/35], Step [300/1263], Loss: 0.2083\n",
      "Epoch [33/35], Step [400/1263], Loss: 0.2209\n",
      "Epoch [33/35], Step [500/1263], Loss: 0.2037\n",
      "Epoch [33/35], Step [600/1263], Loss: 0.2104\n",
      "Epoch [33/35], Step [700/1263], Loss: 0.2280\n",
      "Epoch [33/35], Step [800/1263], Loss: 0.2040\n",
      "Epoch [33/35], Step [900/1263], Loss: 0.2185\n",
      "Epoch [33/35], Step [1000/1263], Loss: 0.2026\n",
      "Epoch [33/35], Step [1100/1263], Loss: 0.2181\n",
      "Epoch [33/35], Step [1200/1263], Loss: 0.2402\n",
      "Epoch [34/35], Step [100/1263], Loss: 0.2070\n",
      "Epoch [34/35], Step [200/1263], Loss: 0.1981\n",
      "Epoch [34/35], Step [300/1263], Loss: 0.2095\n",
      "Epoch [34/35], Step [400/1263], Loss: 0.2029\n",
      "Epoch [34/35], Step [500/1263], Loss: 0.2086\n",
      "Epoch [34/35], Step [600/1263], Loss: 0.1912\n",
      "Epoch [34/35], Step [700/1263], Loss: 0.1979\n",
      "Epoch [34/35], Step [800/1263], Loss: 0.1987\n",
      "Epoch [34/35], Step [900/1263], Loss: 0.1997\n",
      "Epoch [34/35], Step [1000/1263], Loss: 0.1900\n",
      "Epoch [34/35], Step [1100/1263], Loss: 0.2122\n",
      "Epoch [34/35], Step [1200/1263], Loss: 0.2044\n",
      "Epoch [35/35], Step [100/1263], Loss: 0.2075\n",
      "Epoch [35/35], Step [200/1263], Loss: 0.1882\n",
      "Epoch [35/35], Step [300/1263], Loss: 0.1908\n",
      "Epoch [35/35], Step [400/1263], Loss: 0.2049\n",
      "Epoch [35/35], Step [500/1263], Loss: 0.1851\n",
      "Epoch [35/35], Step [600/1263], Loss: 0.2004\n",
      "Epoch [35/35], Step [700/1263], Loss: 0.1883\n",
      "Epoch [35/35], Step [800/1263], Loss: 0.2108\n",
      "Epoch [35/35], Step [900/1263], Loss: 0.2010\n",
      "Epoch [35/35], Step [1000/1263], Loss: 0.2001\n",
      "Epoch [35/35], Step [1100/1263], Loss: 0.1868\n",
      "Epoch [35/35], Step [1200/1263], Loss: 0.2033\n",
      "Final model saved at models\\10_Group17_DLProject.pth\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we evaluate the trained model using the validation dataset:\n",
    "- The model is set to evaluation mode, and gradient computation is disabled.\n",
    "- For each batch, we perform a forward pass and predict the class labels.\n",
    "- Ground truth labels and predictions are stored and used to generate a classification report using `sklearn`. This report provides precision, recall, and F1-scores for each emotion class."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dboWdi-7O1IS",
    "ExecuteTime": {
     "end_time": "2024-12-18T03:36:28.592400Z",
     "start_time": "2024-12-18T03:36:02.486771Z"
    }
   },
   "source": [
    "model = CNN()\n",
    "model = model.to(device)\n",
    "model_path = 'models/10_Group17_DLProject.pth'\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "# Initialize lists to store ground truth and predictions\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "# Disable gradient computation\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        # Move inputs and labels to the same device as the model\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get the predicted class\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Append ground truth and predictions to respective lists\n",
    "        y_true.extend(labels.cpu().numpy())  # Convert tensors to numpy\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Generate the classification report\n",
    "emotion_labels = {0:'Angry', 1:'Disgust', 2:'Fear', 3:'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n",
    "print(classification_report(y_true, y_pred, target_names=list(emotion_labels.values())))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.55      0.45      0.49       799\n",
      "     Disgust       0.71      0.53      0.61        87\n",
      "        Fear       0.40      0.50      0.44       820\n",
      "       Happy       0.80      0.81      0.80      1443\n",
      "         Sad       0.45      0.50      0.47       966\n",
      "    Surprise       0.79      0.72      0.75       634\n",
      "     Neutral       0.54      0.50      0.52       993\n",
      "\n",
      "    accuracy                           0.59      5742\n",
      "   macro avg       0.61      0.57      0.58      5742\n",
      "weighted avg       0.60      0.59      0.60      5742\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Using Residual Blocks in the convolutional neural network resulted in a lower accuracy of 0.59 compared to the previous architecture. This suggests that, for this specific emotion recognition task, the introduction of Residual Blocks did not improve the model's performance and may have even hindered it."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
