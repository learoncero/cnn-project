{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MWKYRenO1H7"
   },
   "source": [
    "## 0. Notebook description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the findings of the previous notebooks, this notebook uses 6 convolutional layers. It also uses **average pooling** in all layers except the first. No changes were made to any other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing libraries and loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T16:07:15.043785Z",
     "start_time": "2024-12-05T16:07:08.550838Z"
    },
    "id": "ZH4EnPodO1H8"
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import classification_report\n",
    "from utils.preprocessing import oversample_data, load_data, ReshapeAndScale, create_dataloaders\n",
    "from utils.fer2013_dataset import Fer2013Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmi4DkFbO1H9"
   },
   "source": [
    "First, we load the data into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T16:07:17.127186Z",
     "start_time": "2024-12-05T16:07:15.062323Z"
    },
    "id": "old1wNLVO1H_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "                                              pixels  emotion\n",
      "0  252 253 252 252 251 243 237 209 209 212 226 19...        6\n",
      "1  94 91 77 66 56 53 51 45 47 49 40 42 43 46 42 4...        0\n",
      "2  91 82 65 66 75 89 108 119 124 134 139 139 134 ...        3\n",
      "3  139 101 115 141 191 245 240 235 243 239 234 23...        1\n",
      "4  238 237 238 239 238 238 237 248 183 189 136 53...        1\n",
      "5  140 127 126 152 149 142 143 129 101 107 146 14...        5\n",
      "6  181 246 193 129 133 221 220 213 139 84 67 30 2...        6\n",
      "7  7 8 8 8 16 28 38 41 40 38 37 37 37 39 40 42 43...        2\n",
      "8  196 198 177 156 103 44 37 37 57 80 89 85 82 81...        0\n",
      "9  0 0 0 0 0 0 0 0 2 0 7 46 54 61 56 25 23 29 30 ...        4\n",
      "\n",
      "\n",
      "Validation Data\n",
      "   emotion                                             pixels\n",
      "0        3  254 253 253 254 254 254 252 252 253 252 253 25...\n",
      "1        4  92 70 64 65 62 64 91 139 167 181 186 187 191 1...\n",
      "2        5  20 19 16 23 24 25 34 24 25 60 113 123 139 149 ...\n",
      "3        4  79 82 83 84 85 89 92 90 93 95 94 97 95 97 93 8...\n",
      "4        2  117 105 95 73 72 65 49 66 92 105 126 154 177 1...\n",
      "5        3  138 138 138 118 129 146 138 120 89 57 50 47 54...\n",
      "6        5  141 132 104 64 37 27 24 23 19 18 19 20 23 22 2...\n",
      "7        3  17 23 19 14 31 27 28 41 52 69 88 96 106 108 10...\n",
      "8        6  117 110 91 49 44 46 47 41 26 39 57 71 83 91 10...\n",
      "9        3  89 141 163 153 136 128 127 125 121 116 30 23 2...\n"
     ]
    }
   ],
   "source": [
    "train_df = load_data(\"data/oversampled_train.csv\")\n",
    "val_df = load_data(\"data/validation.csv\")\n",
    "\n",
    "print(\"Training Data\")\n",
    "print(train_df.head(10))\n",
    "\n",
    "print(\"\\n\\nValidation Data\")\n",
    "print(val_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T16:07:17.397147Z",
     "start_time": "2024-12-05T16:07:17.387174Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7JiE5yJDO1IA",
    "outputId": "45dd2cb5-161b-4394-941f-82e2bc4c5699"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (40404, 2)\n",
      "Validation data shape (5742, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data shape:\", train_df.shape)\n",
    "print(\"Validation data shape\", val_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHpwVsoYO1IH"
   },
   "source": [
    "## 2. Define a custom dataset\n",
    "We define a custom PyTorch dataset class, `Fer2013Dataset`, for handling the FER2013 data. The dataset is designed to load images (stored as pixel strings) and their corresponding emotion labels. It also supports optional transformations to preprocess the images during training. This setup makes it easy to integrate the dataset with PyTorch DataLoaders.\n",
    "\n",
    "The class is contained in the `utils/fer2013_dataset` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also apply the following preprocessing steps to convert the data into the desired format we can further work with:\n",
    "1. **Reshaping**:\n",
    "   - Convert the pixel string into a `48x48` matrix for visualization and processing.\n",
    "2. **Scaling**:\n",
    "   - Scale pixel values to the range `[0, 1]` by dividing the pixel values by 255.\n",
    "3. **Normalization**:\n",
    "   - Normalize pixel values to the range `[-1, 1]` by subtracting the mean and diving them by the standard deviation.\n",
    "\n",
    "\n",
    "These preprocessing steps are contained in the class `ReshapeAndScale`, which is available under path `utils/preprocessing.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a default transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    ReshapeAndScale(n_rows=48, n_cols=48),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T16:07:19.877304Z",
     "start_time": "2024-12-05T16:07:19.870241Z"
    },
    "id": "8cuhkZCOO1II"
   },
   "outputs": [],
   "source": [
    "train_dataset = Fer2013Dataset(train_df, train_df['emotion'], transform=transform)\n",
    "val_dataset = Fer2013Dataset(val_df, val_df['emotion'], transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders\n",
    "\n",
    "We create DataLoaders for both subsets to enable batch processing. The training DataLoader shuffles the data for better learning, while the validation DataLoader does not. Finally, we print the shapes of the batches to verify that everything works correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T16:07:58.887965Z",
     "start_time": "2024-12-05T16:07:19.894776Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rAQ30-FQO1IJ",
    "outputId": "aae71765-1a27-4d1a-9fe0-d34cee950122"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader, val_loader = create_dataloaders(train_dataset, val_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pge_vvuYO1IN"
   },
   "source": [
    "## 3. Define the CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a custom Convolutional Neural Network (CNN) for emotion recognition. The model includes multiple convolutional layers with batch normalization, dropout for regularization, max pooling for downsampling, and fully connected layers for classification. \n",
    "\n",
    "The network dynamically calculates the flattened size needed for the fully connected layers based on the input size (48x48 grayscale images). Finally, we instantiate the model, move it to the available device (CPU or GPU), and print its architecture for verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T16:07:59.032722Z",
     "start_time": "2024-12-05T16:07:58.948090Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvFxqkzxO1IO",
    "outputId": "a7824d62-eb91-42c3-8f2f-def089a2ea5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool6): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (dropout3): Dropout(p=0.25, inplace=False)\n",
      "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (conv5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "  (bn5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (fc1): Linear(in_features=2048, out_features=250, bias=True)\n",
      "  (dropout_fc): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=250, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # 1st Conv Layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding='valid')\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "\n",
    "        # 2nd Conv Layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 3rd Conv Layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding='valid')\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool6 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "\n",
    "        # 4th Conv Layer\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding='same')\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.pool4 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 5th Conv Layer\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding='valid')\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.pool5 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "\n",
    "        # Calculate the flatten size dynamically\n",
    "        self.flatten_size = self._get_flatten_size()\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(self.flatten_size, 250)\n",
    "        self.dropout_fc = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(250, 7)  # 7 classes for emotion recognition\n",
    "\n",
    "    def _get_flatten_size(self):\n",
    "        # Create a dummy tensor with the same size as input image\n",
    "        dummy_input = torch.zeros(1, 1, 48, 48)  # batch_size, channels, height, width\n",
    "        dummy_output = self._forward_conv_layers(dummy_input)\n",
    "        return dummy_output.numel()\n",
    "\n",
    "    def _forward_conv_layers(self, x):\n",
    "        # Pass through convolutional layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool5(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use the convolutional layers defined in _forward_conv_layers\n",
    "        x = self._forward_conv_layers(x)\n",
    "\n",
    "        # Dynamically flatten the output\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = CNN()\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Print model to verify layers\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we define the loss function and optimizer for training the model:\n",
    "- **Loss Function**: `CrossEntropyLoss` is used, which is well-suited for multi-class classification tasks like emotion recognition.\n",
    "- **Optimizer**: The Adam optimizer is initialized with a learning rate of `0.0001` to update the model parameters during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T16:07:59.063835Z",
     "start_time": "2024-12-05T16:07:59.059962Z"
    },
    "id": "WFyRGyfXO1IR"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # call optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMHZrQsZO1IR"
   },
   "source": [
    "## 5. Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we define the training loop for the CNN:\n",
    "- **Number of Epochs**: The model is trained for 35 epochs.\n",
    "- **Training Process**:\n",
    "  - The model is set to training mode.\n",
    "  - For each batch, we move inputs and labels to the appropriate device, clear the gradients, perform forward and backward passes, and update the model's parameters using the optimizer.\n",
    "  - The running loss is tracked and printed every 100 batches for monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T19:33:10.523679Z",
     "start_time": "2024-12-05T16:07:59.073844Z"
    },
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "iX3PbDPqO1IR",
    "outputId": "09fb2a30-0bb4-47b5-e1d7-2d0d47db40ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/35], Step [100/1263], Loss: 1.9062\n",
      "Epoch [1/35], Step [200/1263], Loss: 1.8444\n",
      "Epoch [1/35], Step [300/1263], Loss: 1.7723\n",
      "Epoch [1/35], Step [400/1263], Loss: 1.7347\n",
      "Epoch [1/35], Step [500/1263], Loss: 1.6892\n",
      "Epoch [1/35], Step [600/1263], Loss: 1.6181\n",
      "Epoch [1/35], Step [700/1263], Loss: 1.5900\n",
      "Epoch [1/35], Step [800/1263], Loss: 1.5261\n",
      "Epoch [1/35], Step [900/1263], Loss: 1.4903\n",
      "Epoch [1/35], Step [1000/1263], Loss: 1.4680\n",
      "Epoch [1/35], Step [1100/1263], Loss: 1.4385\n",
      "Epoch [1/35], Step [1200/1263], Loss: 1.4113\n",
      "Epoch [2/35], Step [100/1263], Loss: 1.3561\n",
      "Epoch [2/35], Step [200/1263], Loss: 1.3339\n",
      "Epoch [2/35], Step [300/1263], Loss: 1.3122\n",
      "Epoch [2/35], Step [400/1263], Loss: 1.3247\n",
      "Epoch [2/35], Step [500/1263], Loss: 1.2949\n",
      "Epoch [2/35], Step [600/1263], Loss: 1.2798\n",
      "Epoch [2/35], Step [700/1263], Loss: 1.2629\n",
      "Epoch [2/35], Step [800/1263], Loss: 1.2618\n",
      "Epoch [2/35], Step [900/1263], Loss: 1.2566\n",
      "Epoch [2/35], Step [1000/1263], Loss: 1.2259\n",
      "Epoch [2/35], Step [1100/1263], Loss: 1.2258\n",
      "Epoch [2/35], Step [1200/1263], Loss: 1.1963\n",
      "Epoch [3/35], Step [100/1263], Loss: 1.1896\n",
      "Epoch [3/35], Step [200/1263], Loss: 1.1558\n",
      "Epoch [3/35], Step [300/1263], Loss: 1.1326\n",
      "Epoch [3/35], Step [400/1263], Loss: 1.1096\n",
      "Epoch [3/35], Step [500/1263], Loss: 1.1395\n",
      "Epoch [3/35], Step [600/1263], Loss: 1.1179\n",
      "Epoch [3/35], Step [700/1263], Loss: 1.1177\n",
      "Epoch [3/35], Step [800/1263], Loss: 1.1180\n",
      "Epoch [3/35], Step [900/1263], Loss: 1.0993\n",
      "Epoch [3/35], Step [1000/1263], Loss: 1.0622\n",
      "Epoch [3/35], Step [1100/1263], Loss: 1.0656\n",
      "Epoch [3/35], Step [1200/1263], Loss: 1.0979\n",
      "Epoch [4/35], Step [100/1263], Loss: 1.0205\n",
      "Epoch [4/35], Step [200/1263], Loss: 1.0297\n",
      "Epoch [4/35], Step [300/1263], Loss: 1.0478\n",
      "Epoch [4/35], Step [400/1263], Loss: 1.0142\n",
      "Epoch [4/35], Step [500/1263], Loss: 0.9985\n",
      "Epoch [4/35], Step [600/1263], Loss: 1.0122\n",
      "Epoch [4/35], Step [700/1263], Loss: 1.0377\n",
      "Epoch [4/35], Step [800/1263], Loss: 1.0041\n",
      "Epoch [4/35], Step [900/1263], Loss: 0.9938\n",
      "Epoch [4/35], Step [1000/1263], Loss: 1.0025\n",
      "Epoch [4/35], Step [1100/1263], Loss: 0.9882\n",
      "Epoch [4/35], Step [1200/1263], Loss: 1.0111\n",
      "Epoch [5/35], Step [100/1263], Loss: 0.9574\n",
      "Epoch [5/35], Step [200/1263], Loss: 0.9608\n",
      "Epoch [5/35], Step [300/1263], Loss: 0.9811\n",
      "Epoch [5/35], Step [400/1263], Loss: 0.9476\n",
      "Epoch [5/35], Step [500/1263], Loss: 0.9354\n",
      "Epoch [5/35], Step [600/1263], Loss: 0.9292\n",
      "Epoch [5/35], Step [700/1263], Loss: 0.9615\n",
      "Epoch [5/35], Step [800/1263], Loss: 0.9442\n",
      "Epoch [5/35], Step [900/1263], Loss: 0.9496\n",
      "Epoch [5/35], Step [1000/1263], Loss: 0.9249\n",
      "Epoch [5/35], Step [1100/1263], Loss: 0.9233\n",
      "Epoch [5/35], Step [1200/1263], Loss: 0.9244\n",
      "Epoch [6/35], Step [100/1263], Loss: 0.8869\n",
      "Epoch [6/35], Step [200/1263], Loss: 0.9028\n",
      "Epoch [6/35], Step [300/1263], Loss: 0.8901\n",
      "Epoch [6/35], Step [400/1263], Loss: 0.8998\n",
      "Epoch [6/35], Step [500/1263], Loss: 0.8971\n",
      "Epoch [6/35], Step [600/1263], Loss: 0.8947\n",
      "Epoch [6/35], Step [700/1263], Loss: 0.9042\n",
      "Epoch [6/35], Step [800/1263], Loss: 0.8809\n",
      "Epoch [6/35], Step [900/1263], Loss: 0.8758\n",
      "Epoch [6/35], Step [1000/1263], Loss: 0.8806\n",
      "Epoch [6/35], Step [1100/1263], Loss: 0.8655\n",
      "Epoch [6/35], Step [1200/1263], Loss: 0.8746\n",
      "Epoch [7/35], Step [100/1263], Loss: 0.8587\n",
      "Epoch [7/35], Step [200/1263], Loss: 0.8316\n",
      "Epoch [7/35], Step [300/1263], Loss: 0.8210\n",
      "Epoch [7/35], Step [400/1263], Loss: 0.8125\n",
      "Epoch [7/35], Step [500/1263], Loss: 0.8376\n",
      "Epoch [7/35], Step [600/1263], Loss: 0.8449\n",
      "Epoch [7/35], Step [700/1263], Loss: 0.8651\n",
      "Epoch [7/35], Step [800/1263], Loss: 0.8520\n",
      "Epoch [7/35], Step [900/1263], Loss: 0.8441\n",
      "Epoch [7/35], Step [1000/1263], Loss: 0.8086\n",
      "Epoch [7/35], Step [1100/1263], Loss: 0.8043\n",
      "Epoch [7/35], Step [1200/1263], Loss: 0.8444\n",
      "Epoch [8/35], Step [100/1263], Loss: 0.7598\n",
      "Epoch [8/35], Step [200/1263], Loss: 0.8048\n",
      "Epoch [8/35], Step [300/1263], Loss: 0.7901\n",
      "Epoch [8/35], Step [400/1263], Loss: 0.8074\n",
      "Epoch [8/35], Step [500/1263], Loss: 0.8064\n",
      "Epoch [8/35], Step [600/1263], Loss: 0.8121\n",
      "Epoch [8/35], Step [700/1263], Loss: 0.8127\n",
      "Epoch [8/35], Step [800/1263], Loss: 0.7657\n",
      "Epoch [8/35], Step [900/1263], Loss: 0.7968\n",
      "Epoch [8/35], Step [1000/1263], Loss: 0.8087\n",
      "Epoch [8/35], Step [1100/1263], Loss: 0.7908\n",
      "Epoch [8/35], Step [1200/1263], Loss: 0.7847\n",
      "Epoch [9/35], Step [100/1263], Loss: 0.7611\n",
      "Epoch [9/35], Step [200/1263], Loss: 0.7605\n",
      "Epoch [9/35], Step [300/1263], Loss: 0.7572\n",
      "Epoch [9/35], Step [400/1263], Loss: 0.7278\n",
      "Epoch [9/35], Step [500/1263], Loss: 0.7419\n",
      "Epoch [9/35], Step [600/1263], Loss: 0.7505\n",
      "Epoch [9/35], Step [700/1263], Loss: 0.7723\n",
      "Epoch [9/35], Step [800/1263], Loss: 0.7521\n",
      "Epoch [9/35], Step [900/1263], Loss: 0.7282\n",
      "Epoch [9/35], Step [1000/1263], Loss: 0.7397\n",
      "Epoch [9/35], Step [1100/1263], Loss: 0.7693\n",
      "Epoch [9/35], Step [1200/1263], Loss: 0.7422\n",
      "Epoch [10/35], Step [100/1263], Loss: 0.7189\n",
      "Epoch [10/35], Step [200/1263], Loss: 0.7102\n",
      "Epoch [10/35], Step [300/1263], Loss: 0.7052\n",
      "Epoch [10/35], Step [400/1263], Loss: 0.6930\n",
      "Epoch [10/35], Step [500/1263], Loss: 0.7196\n",
      "Epoch [10/35], Step [600/1263], Loss: 0.7229\n",
      "Epoch [10/35], Step [700/1263], Loss: 0.7207\n",
      "Epoch [10/35], Step [800/1263], Loss: 0.6970\n",
      "Epoch [10/35], Step [900/1263], Loss: 0.7047\n",
      "Epoch [10/35], Step [1000/1263], Loss: 0.6832\n",
      "Epoch [10/35], Step [1100/1263], Loss: 0.7422\n",
      "Epoch [10/35], Step [1200/1263], Loss: 0.6932\n",
      "Checkpoint saved at epoch 10\n",
      "Epoch [11/35], Step [100/1263], Loss: 0.6498\n",
      "Epoch [11/35], Step [200/1263], Loss: 0.6868\n",
      "Epoch [11/35], Step [300/1263], Loss: 0.6593\n",
      "Epoch [11/35], Step [400/1263], Loss: 0.6456\n",
      "Epoch [11/35], Step [500/1263], Loss: 0.6745\n",
      "Epoch [11/35], Step [600/1263], Loss: 0.6507\n",
      "Epoch [11/35], Step [700/1263], Loss: 0.6637\n",
      "Epoch [11/35], Step [800/1263], Loss: 0.6701\n",
      "Epoch [11/35], Step [900/1263], Loss: 0.6746\n",
      "Epoch [11/35], Step [1000/1263], Loss: 0.6995\n",
      "Epoch [11/35], Step [1100/1263], Loss: 0.6787\n",
      "Epoch [11/35], Step [1200/1263], Loss: 0.6963\n",
      "Epoch [12/35], Step [100/1263], Loss: 0.6304\n",
      "Epoch [12/35], Step [200/1263], Loss: 0.6374\n",
      "Epoch [12/35], Step [300/1263], Loss: 0.6360\n",
      "Epoch [12/35], Step [400/1263], Loss: 0.6240\n",
      "Epoch [12/35], Step [500/1263], Loss: 0.6229\n",
      "Epoch [12/35], Step [600/1263], Loss: 0.6429\n",
      "Epoch [12/35], Step [700/1263], Loss: 0.6328\n",
      "Epoch [12/35], Step [800/1263], Loss: 0.6246\n",
      "Epoch [12/35], Step [900/1263], Loss: 0.6265\n",
      "Epoch [12/35], Step [1000/1263], Loss: 0.6496\n",
      "Epoch [12/35], Step [1100/1263], Loss: 0.6566\n",
      "Epoch [12/35], Step [1200/1263], Loss: 0.6449\n",
      "Epoch [13/35], Step [100/1263], Loss: 0.5805\n",
      "Epoch [13/35], Step [200/1263], Loss: 0.5899\n",
      "Epoch [13/35], Step [300/1263], Loss: 0.5919\n",
      "Epoch [13/35], Step [400/1263], Loss: 0.6064\n",
      "Epoch [13/35], Step [500/1263], Loss: 0.6107\n",
      "Epoch [13/35], Step [600/1263], Loss: 0.6037\n",
      "Epoch [13/35], Step [700/1263], Loss: 0.5956\n",
      "Epoch [13/35], Step [800/1263], Loss: 0.5805\n",
      "Epoch [13/35], Step [900/1263], Loss: 0.6096\n",
      "Epoch [13/35], Step [1000/1263], Loss: 0.5799\n",
      "Epoch [13/35], Step [1100/1263], Loss: 0.6161\n",
      "Epoch [13/35], Step [1200/1263], Loss: 0.6203\n",
      "Epoch [14/35], Step [100/1263], Loss: 0.5606\n",
      "Epoch [14/35], Step [200/1263], Loss: 0.5370\n",
      "Epoch [14/35], Step [300/1263], Loss: 0.5726\n",
      "Epoch [14/35], Step [400/1263], Loss: 0.5562\n",
      "Epoch [14/35], Step [500/1263], Loss: 0.5386\n",
      "Epoch [14/35], Step [600/1263], Loss: 0.5629\n",
      "Epoch [14/35], Step [700/1263], Loss: 0.5963\n",
      "Epoch [14/35], Step [800/1263], Loss: 0.5651\n",
      "Epoch [14/35], Step [900/1263], Loss: 0.5542\n",
      "Epoch [14/35], Step [1000/1263], Loss: 0.5721\n",
      "Epoch [14/35], Step [1100/1263], Loss: 0.5691\n",
      "Epoch [14/35], Step [1200/1263], Loss: 0.5722\n",
      "Epoch [15/35], Step [100/1263], Loss: 0.5196\n",
      "Epoch [15/35], Step [200/1263], Loss: 0.5084\n",
      "Epoch [15/35], Step [300/1263], Loss: 0.5417\n",
      "Epoch [15/35], Step [400/1263], Loss: 0.5351\n",
      "Epoch [15/35], Step [500/1263], Loss: 0.5477\n",
      "Epoch [15/35], Step [600/1263], Loss: 0.5260\n",
      "Epoch [15/35], Step [700/1263], Loss: 0.5301\n",
      "Epoch [15/35], Step [800/1263], Loss: 0.5400\n",
      "Epoch [15/35], Step [900/1263], Loss: 0.5253\n",
      "Epoch [15/35], Step [1000/1263], Loss: 0.5259\n",
      "Epoch [15/35], Step [1100/1263], Loss: 0.5241\n",
      "Epoch [15/35], Step [1200/1263], Loss: 0.5427\n",
      "Epoch [16/35], Step [100/1263], Loss: 0.5184\n",
      "Epoch [16/35], Step [200/1263], Loss: 0.5036\n",
      "Epoch [16/35], Step [300/1263], Loss: 0.4846\n",
      "Epoch [16/35], Step [400/1263], Loss: 0.5000\n",
      "Epoch [16/35], Step [500/1263], Loss: 0.4986\n",
      "Epoch [16/35], Step [600/1263], Loss: 0.4873\n",
      "Epoch [16/35], Step [700/1263], Loss: 0.5135\n",
      "Epoch [16/35], Step [800/1263], Loss: 0.5098\n",
      "Epoch [16/35], Step [900/1263], Loss: 0.4878\n",
      "Epoch [16/35], Step [1000/1263], Loss: 0.4972\n",
      "Epoch [16/35], Step [1100/1263], Loss: 0.5190\n",
      "Epoch [16/35], Step [1200/1263], Loss: 0.5141\n",
      "Epoch [17/35], Step [100/1263], Loss: 0.4533\n",
      "Epoch [17/35], Step [200/1263], Loss: 0.4710\n",
      "Epoch [17/35], Step [300/1263], Loss: 0.4654\n",
      "Epoch [17/35], Step [400/1263], Loss: 0.4533\n",
      "Epoch [17/35], Step [500/1263], Loss: 0.4740\n",
      "Epoch [17/35], Step [600/1263], Loss: 0.4512\n",
      "Epoch [17/35], Step [700/1263], Loss: 0.5052\n",
      "Epoch [17/35], Step [800/1263], Loss: 0.4810\n",
      "Epoch [17/35], Step [900/1263], Loss: 0.4957\n",
      "Epoch [17/35], Step [1000/1263], Loss: 0.4665\n",
      "Epoch [17/35], Step [1100/1263], Loss: 0.4866\n",
      "Epoch [17/35], Step [1200/1263], Loss: 0.4675\n",
      "Epoch [18/35], Step [100/1263], Loss: 0.4137\n",
      "Epoch [18/35], Step [200/1263], Loss: 0.4396\n",
      "Epoch [18/35], Step [300/1263], Loss: 0.4350\n",
      "Epoch [18/35], Step [400/1263], Loss: 0.4338\n",
      "Epoch [18/35], Step [500/1263], Loss: 0.4304\n",
      "Epoch [18/35], Step [600/1263], Loss: 0.4489\n",
      "Epoch [18/35], Step [700/1263], Loss: 0.4502\n",
      "Epoch [18/35], Step [800/1263], Loss: 0.4565\n",
      "Epoch [18/35], Step [900/1263], Loss: 0.4689\n",
      "Epoch [18/35], Step [1000/1263], Loss: 0.4617\n",
      "Epoch [18/35], Step [1100/1263], Loss: 0.4634\n",
      "Epoch [18/35], Step [1200/1263], Loss: 0.4519\n",
      "Epoch [19/35], Step [100/1263], Loss: 0.3948\n",
      "Epoch [19/35], Step [200/1263], Loss: 0.4229\n",
      "Epoch [19/35], Step [300/1263], Loss: 0.4041\n",
      "Epoch [19/35], Step [400/1263], Loss: 0.4308\n",
      "Epoch [19/35], Step [500/1263], Loss: 0.4336\n",
      "Epoch [19/35], Step [600/1263], Loss: 0.4120\n",
      "Epoch [19/35], Step [700/1263], Loss: 0.4407\n",
      "Epoch [19/35], Step [800/1263], Loss: 0.4184\n",
      "Epoch [19/35], Step [900/1263], Loss: 0.4434\n",
      "Epoch [19/35], Step [1000/1263], Loss: 0.4183\n",
      "Epoch [19/35], Step [1100/1263], Loss: 0.4231\n",
      "Epoch [19/35], Step [1200/1263], Loss: 0.4357\n",
      "Epoch [20/35], Step [100/1263], Loss: 0.3647\n",
      "Epoch [20/35], Step [200/1263], Loss: 0.3748\n",
      "Epoch [20/35], Step [300/1263], Loss: 0.4057\n",
      "Epoch [20/35], Step [400/1263], Loss: 0.4072\n",
      "Epoch [20/35], Step [500/1263], Loss: 0.3855\n",
      "Epoch [20/35], Step [600/1263], Loss: 0.4239\n",
      "Epoch [20/35], Step [700/1263], Loss: 0.4005\n",
      "Epoch [20/35], Step [800/1263], Loss: 0.4107\n",
      "Epoch [20/35], Step [900/1263], Loss: 0.4023\n",
      "Epoch [20/35], Step [1000/1263], Loss: 0.3969\n",
      "Epoch [20/35], Step [1100/1263], Loss: 0.4244\n",
      "Epoch [20/35], Step [1200/1263], Loss: 0.4030\n",
      "Checkpoint saved at epoch 20\n",
      "Epoch [21/35], Step [100/1263], Loss: 0.3508\n",
      "Epoch [21/35], Step [200/1263], Loss: 0.3768\n",
      "Epoch [21/35], Step [300/1263], Loss: 0.3770\n",
      "Epoch [21/35], Step [400/1263], Loss: 0.3516\n",
      "Epoch [21/35], Step [500/1263], Loss: 0.3779\n",
      "Epoch [21/35], Step [600/1263], Loss: 0.3954\n",
      "Epoch [21/35], Step [700/1263], Loss: 0.3920\n",
      "Epoch [21/35], Step [800/1263], Loss: 0.3933\n",
      "Epoch [21/35], Step [900/1263], Loss: 0.3934\n",
      "Epoch [21/35], Step [1000/1263], Loss: 0.4054\n",
      "Epoch [21/35], Step [1100/1263], Loss: 0.3570\n",
      "Epoch [21/35], Step [1200/1263], Loss: 0.3848\n",
      "Epoch [22/35], Step [100/1263], Loss: 0.3509\n",
      "Epoch [22/35], Step [200/1263], Loss: 0.3453\n",
      "Epoch [22/35], Step [300/1263], Loss: 0.3326\n",
      "Epoch [22/35], Step [400/1263], Loss: 0.3414\n",
      "Epoch [22/35], Step [500/1263], Loss: 0.3737\n",
      "Epoch [22/35], Step [600/1263], Loss: 0.3535\n",
      "Epoch [22/35], Step [700/1263], Loss: 0.3537\n",
      "Epoch [22/35], Step [800/1263], Loss: 0.3609\n",
      "Epoch [22/35], Step [900/1263], Loss: 0.3762\n",
      "Epoch [22/35], Step [1000/1263], Loss: 0.3591\n",
      "Epoch [22/35], Step [1100/1263], Loss: 0.3682\n",
      "Epoch [22/35], Step [1200/1263], Loss: 0.3822\n",
      "Epoch [23/35], Step [100/1263], Loss: 0.3196\n",
      "Epoch [23/35], Step [200/1263], Loss: 0.3025\n",
      "Epoch [23/35], Step [300/1263], Loss: 0.3421\n",
      "Epoch [23/35], Step [400/1263], Loss: 0.3561\n",
      "Epoch [23/35], Step [500/1263], Loss: 0.3526\n",
      "Epoch [23/35], Step [600/1263], Loss: 0.3575\n",
      "Epoch [23/35], Step [700/1263], Loss: 0.3496\n",
      "Epoch [23/35], Step [800/1263], Loss: 0.3622\n",
      "Epoch [23/35], Step [900/1263], Loss: 0.3257\n",
      "Epoch [23/35], Step [1000/1263], Loss: 0.3254\n",
      "Epoch [23/35], Step [1100/1263], Loss: 0.3412\n",
      "Epoch [23/35], Step [1200/1263], Loss: 0.3538\n",
      "Epoch [24/35], Step [100/1263], Loss: 0.3073\n",
      "Epoch [24/35], Step [200/1263], Loss: 0.3010\n",
      "Epoch [24/35], Step [300/1263], Loss: 0.3076\n",
      "Epoch [24/35], Step [400/1263], Loss: 0.3209\n",
      "Epoch [24/35], Step [500/1263], Loss: 0.3134\n",
      "Epoch [24/35], Step [600/1263], Loss: 0.2994\n",
      "Epoch [24/35], Step [700/1263], Loss: 0.3282\n",
      "Epoch [24/35], Step [800/1263], Loss: 0.3396\n",
      "Epoch [24/35], Step [900/1263], Loss: 0.3270\n",
      "Epoch [24/35], Step [1000/1263], Loss: 0.3146\n",
      "Epoch [24/35], Step [1100/1263], Loss: 0.3337\n",
      "Epoch [24/35], Step [1200/1263], Loss: 0.3165\n",
      "Epoch [25/35], Step [100/1263], Loss: 0.2959\n",
      "Epoch [25/35], Step [200/1263], Loss: 0.2967\n",
      "Epoch [25/35], Step [300/1263], Loss: 0.2908\n",
      "Epoch [25/35], Step [400/1263], Loss: 0.2866\n",
      "Epoch [25/35], Step [500/1263], Loss: 0.3044\n",
      "Epoch [25/35], Step [600/1263], Loss: 0.3209\n",
      "Epoch [25/35], Step [700/1263], Loss: 0.3027\n",
      "Epoch [25/35], Step [800/1263], Loss: 0.3115\n",
      "Epoch [25/35], Step [900/1263], Loss: 0.3253\n",
      "Epoch [25/35], Step [1000/1263], Loss: 0.3246\n",
      "Epoch [25/35], Step [1100/1263], Loss: 0.3149\n",
      "Epoch [25/35], Step [1200/1263], Loss: 0.3131\n",
      "Epoch [26/35], Step [100/1263], Loss: 0.2941\n",
      "Epoch [26/35], Step [200/1263], Loss: 0.2764\n",
      "Epoch [26/35], Step [300/1263], Loss: 0.2832\n",
      "Epoch [26/35], Step [400/1263], Loss: 0.2808\n",
      "Epoch [26/35], Step [500/1263], Loss: 0.2899\n",
      "Epoch [26/35], Step [600/1263], Loss: 0.3059\n",
      "Epoch [26/35], Step [700/1263], Loss: 0.2900\n",
      "Epoch [26/35], Step [800/1263], Loss: 0.2869\n",
      "Epoch [26/35], Step [900/1263], Loss: 0.3057\n",
      "Epoch [26/35], Step [1000/1263], Loss: 0.3142\n",
      "Epoch [26/35], Step [1100/1263], Loss: 0.3052\n",
      "Epoch [26/35], Step [1200/1263], Loss: 0.2839\n",
      "Epoch [27/35], Step [100/1263], Loss: 0.2589\n",
      "Epoch [27/35], Step [200/1263], Loss: 0.2546\n",
      "Epoch [27/35], Step [300/1263], Loss: 0.2643\n",
      "Epoch [27/35], Step [400/1263], Loss: 0.2898\n",
      "Epoch [27/35], Step [500/1263], Loss: 0.2835\n",
      "Epoch [27/35], Step [600/1263], Loss: 0.2842\n",
      "Epoch [27/35], Step [700/1263], Loss: 0.3067\n",
      "Epoch [27/35], Step [800/1263], Loss: 0.2821\n",
      "Epoch [27/35], Step [900/1263], Loss: 0.2810\n",
      "Epoch [27/35], Step [1000/1263], Loss: 0.2717\n",
      "Epoch [27/35], Step [1100/1263], Loss: 0.2911\n",
      "Epoch [27/35], Step [1200/1263], Loss: 0.2690\n",
      "Epoch [28/35], Step [100/1263], Loss: 0.2488\n",
      "Epoch [28/35], Step [200/1263], Loss: 0.2490\n",
      "Epoch [28/35], Step [300/1263], Loss: 0.2550\n",
      "Epoch [28/35], Step [400/1263], Loss: 0.2603\n",
      "Epoch [28/35], Step [500/1263], Loss: 0.2659\n",
      "Epoch [28/35], Step [600/1263], Loss: 0.2586\n",
      "Epoch [28/35], Step [700/1263], Loss: 0.2569\n",
      "Epoch [28/35], Step [800/1263], Loss: 0.2445\n",
      "Epoch [28/35], Step [900/1263], Loss: 0.2482\n",
      "Epoch [28/35], Step [1000/1263], Loss: 0.2723\n",
      "Epoch [28/35], Step [1100/1263], Loss: 0.2978\n",
      "Epoch [28/35], Step [1200/1263], Loss: 0.2619\n",
      "Epoch [29/35], Step [100/1263], Loss: 0.2270\n",
      "Epoch [29/35], Step [200/1263], Loss: 0.2235\n",
      "Epoch [29/35], Step [300/1263], Loss: 0.2511\n",
      "Epoch [29/35], Step [400/1263], Loss: 0.2191\n",
      "Epoch [29/35], Step [500/1263], Loss: 0.2525\n",
      "Epoch [29/35], Step [600/1263], Loss: 0.2398\n",
      "Epoch [29/35], Step [700/1263], Loss: 0.2631\n",
      "Epoch [29/35], Step [800/1263], Loss: 0.2492\n",
      "Epoch [29/35], Step [900/1263], Loss: 0.2763\n",
      "Epoch [29/35], Step [1000/1263], Loss: 0.2605\n",
      "Epoch [29/35], Step [1100/1263], Loss: 0.2496\n",
      "Epoch [29/35], Step [1200/1263], Loss: 0.2518\n",
      "Epoch [30/35], Step [100/1263], Loss: 0.2208\n",
      "Epoch [30/35], Step [200/1263], Loss: 0.2195\n",
      "Epoch [30/35], Step [300/1263], Loss: 0.2472\n",
      "Epoch [30/35], Step [400/1263], Loss: 0.2219\n",
      "Epoch [30/35], Step [500/1263], Loss: 0.2312\n",
      "Epoch [30/35], Step [600/1263], Loss: 0.2179\n",
      "Epoch [30/35], Step [700/1263], Loss: 0.2510\n",
      "Epoch [30/35], Step [800/1263], Loss: 0.2529\n",
      "Epoch [30/35], Step [900/1263], Loss: 0.2356\n",
      "Epoch [30/35], Step [1000/1263], Loss: 0.2481\n",
      "Epoch [30/35], Step [1100/1263], Loss: 0.2400\n",
      "Epoch [30/35], Step [1200/1263], Loss: 0.2445\n",
      "Checkpoint saved at epoch 30\n",
      "Epoch [31/35], Step [100/1263], Loss: 0.2118\n",
      "Epoch [31/35], Step [200/1263], Loss: 0.2100\n",
      "Epoch [31/35], Step [300/1263], Loss: 0.2366\n",
      "Epoch [31/35], Step [400/1263], Loss: 0.2199\n",
      "Epoch [31/35], Step [500/1263], Loss: 0.2206\n",
      "Epoch [31/35], Step [600/1263], Loss: 0.2314\n",
      "Epoch [31/35], Step [700/1263], Loss: 0.2371\n",
      "Epoch [31/35], Step [800/1263], Loss: 0.2391\n",
      "Epoch [31/35], Step [900/1263], Loss: 0.2519\n",
      "Epoch [31/35], Step [1000/1263], Loss: 0.2347\n",
      "Epoch [31/35], Step [1100/1263], Loss: 0.2462\n",
      "Epoch [31/35], Step [1200/1263], Loss: 0.2366\n",
      "Epoch [32/35], Step [100/1263], Loss: 0.2057\n",
      "Epoch [32/35], Step [200/1263], Loss: 0.2086\n",
      "Epoch [32/35], Step [300/1263], Loss: 0.2091\n",
      "Epoch [32/35], Step [400/1263], Loss: 0.2040\n",
      "Epoch [32/35], Step [500/1263], Loss: 0.2169\n",
      "Epoch [32/35], Step [600/1263], Loss: 0.2198\n",
      "Epoch [32/35], Step [700/1263], Loss: 0.2252\n",
      "Epoch [32/35], Step [800/1263], Loss: 0.2019\n",
      "Epoch [32/35], Step [900/1263], Loss: 0.2108\n",
      "Epoch [32/35], Step [1000/1263], Loss: 0.2375\n",
      "Epoch [32/35], Step [1100/1263], Loss: 0.2299\n",
      "Epoch [32/35], Step [1200/1263], Loss: 0.2392\n",
      "Epoch [33/35], Step [100/1263], Loss: 0.2067\n",
      "Epoch [33/35], Step [200/1263], Loss: 0.2020\n",
      "Epoch [33/35], Step [300/1263], Loss: 0.2052\n",
      "Epoch [33/35], Step [400/1263], Loss: 0.1843\n",
      "Epoch [33/35], Step [500/1263], Loss: 0.2207\n",
      "Epoch [33/35], Step [600/1263], Loss: 0.2130\n",
      "Epoch [33/35], Step [700/1263], Loss: 0.1985\n",
      "Epoch [33/35], Step [800/1263], Loss: 0.2323\n",
      "Epoch [33/35], Step [900/1263], Loss: 0.2159\n",
      "Epoch [33/35], Step [1000/1263], Loss: 0.2160\n",
      "Epoch [33/35], Step [1100/1263], Loss: 0.2224\n",
      "Epoch [33/35], Step [1200/1263], Loss: 0.2193\n",
      "Epoch [34/35], Step [100/1263], Loss: 0.1726\n",
      "Epoch [34/35], Step [200/1263], Loss: 0.2032\n",
      "Epoch [34/35], Step [300/1263], Loss: 0.1867\n",
      "Epoch [34/35], Step [400/1263], Loss: 0.1940\n",
      "Epoch [34/35], Step [500/1263], Loss: 0.1937\n",
      "Epoch [34/35], Step [600/1263], Loss: 0.1925\n",
      "Epoch [34/35], Step [700/1263], Loss: 0.2067\n",
      "Epoch [34/35], Step [800/1263], Loss: 0.1939\n",
      "Epoch [34/35], Step [900/1263], Loss: 0.2178\n",
      "Epoch [34/35], Step [1000/1263], Loss: 0.2104\n",
      "Epoch [34/35], Step [1100/1263], Loss: 0.2152\n",
      "Epoch [34/35], Step [1200/1263], Loss: 0.2087\n",
      "Epoch [35/35], Step [100/1263], Loss: 0.1676\n",
      "Epoch [35/35], Step [200/1263], Loss: 0.1814\n",
      "Epoch [35/35], Step [300/1263], Loss: 0.1935\n",
      "Epoch [35/35], Step [400/1263], Loss: 0.2116\n",
      "Epoch [35/35], Step [500/1263], Loss: 0.2070\n",
      "Epoch [35/35], Step [600/1263], Loss: 0.1834\n",
      "Epoch [35/35], Step [700/1263], Loss: 0.1997\n",
      "Epoch [35/35], Step [800/1263], Loss: 0.1866\n",
      "Epoch [35/35], Step [900/1263], Loss: 0.1948\n",
      "Epoch [35/35], Step [1000/1263], Loss: 0.2084\n",
      "Epoch [35/35], Step [1100/1263], Loss: 0.2147\n",
      "Epoch [35/35], Step [1200/1263], Loss: 0.2006\n"
     ]
    }
   ],
   "source": [
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir='facial-expression-detection/5_Group17_DLProject')\n",
    "\n",
    "checkpoint_path = 'model-checkpoints/5_Group17_DLProject'\n",
    "os.makedirs(checkpoint_path, exist_ok=True)\n",
    "model_save_path = 'models'\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "num_epochs = 35\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 100 == 0:  # Print every 100 batches\n",
    "            avg_loss = running_loss / 100\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {avg_loss:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Log loss to TensorBoard\n",
    "            writer.add_scalar('Loss/train', avg_loss, epoch)\n",
    "\n",
    "    # Save checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': running_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, f\"{checkpoint_path}/checkpoint_epoch_{epoch + 1}.pth\")\n",
    "        print(f\"Checkpoint saved at epoch {epoch + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved at models/5_Group17_DLProject.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the final model after training\n",
    "final_model_path = os.path.join(model_save_path, '5_Group17_DLProject.pth')\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final model saved at {final_model_path}\")\n",
    "# Close the writer after training\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we evaluate the trained model using the validation dataset:\n",
    "- The model is set to evaluation mode, and gradient computation is disabled.\n",
    "- For each batch, we perform a forward pass and predict the class labels.\n",
    "- Ground truth labels and predictions are stored and used to generate a classification report using `sklearn`. This report provides precision, recall, and F1-scores for each emotion class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T19:33:52.598661Z",
     "start_time": "2024-12-05T19:33:10.840394Z"
    },
    "id": "dboWdi-7O1IS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.49      0.51      0.50       799\n",
      "     Disgust       0.63      0.56      0.59        87\n",
      "        Fear       0.47      0.39      0.43       820\n",
      "       Happy       0.78      0.82      0.80      1443\n",
      "         Sad       0.49      0.43      0.46       966\n",
      "    Surprise       0.72      0.75      0.73       634\n",
      "     Neutral       0.51      0.58      0.54       993\n",
      "\n",
      "    accuracy                           0.60      5742\n",
      "   macro avg       0.58      0.58      0.58      5742\n",
      "weighted avg       0.59      0.60      0.59      5742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "model = model.to(device)\n",
    "model_path = 'models/5_Group17_DLProject.pth'\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "\n",
    "# Initialize lists to store ground truth and predictions\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "# Disable gradient computation\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        # Move inputs and labels to the same device as the model\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get the predicted class\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Append ground truth and predictions to respective lists\n",
    "        y_true.extend(labels.cpu().numpy())  # Convert tensors to numpy\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Generate the classification report\n",
    "emotion_labels = {0:'Angry', 1:'Disgust', 2:'Fear', 3:'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n",
    "print(classification_report(y_true, y_pred, target_names=list(emotion_labels.values())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates that using five convolutional layers combined with average pooling in all but the first layer leads to no improvement in performance over the baseline, achieving an accuracy of  0.60. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
