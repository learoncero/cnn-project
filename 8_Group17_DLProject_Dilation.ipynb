{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MWKYRenO1H7"
   },
   "source": [
    "## 0. Notebook description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the findings of the previous notebooks, this notebook uses 6 convolutional layers. It also uses **average pooling** in all layers except the first. Dilated Convolutions were applied in layers conv3, conv4, conv5, and conv6 with values of dilation=2 or dilation=4 to expand the receptive field. The padding was also proportionally increased to preserve the output dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dilated convolutions, also known as atrous convolutions, are a type of convolution where the kernel is applied over an area larger than its length by skipping input values with a certain step. The dilation rate determines the spacing between the kernel elements.\n",
    "\n",
    "**Dilation**:\n",
    "- **Dilation Rate**: The number of pixels to skip between each kernel element.\n",
    "- **Effect**: Expands the receptive field of the convolutional layer without increasing the number of parameters or the amount of computation. This allows the network to capture more context and larger patterns in the input data.\n",
    "\n",
    "In summary, dilated convolutions help in capturing more global information by increasing the receptive field while keeping the computational cost manageable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing libraries and loading the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZH4EnPodO1H8",
    "ExecuteTime": {
     "end_time": "2024-12-17T10:36:17.096783Z",
     "start_time": "2024-12-17T10:36:03.674933Z"
    }
   },
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import classification_report\n",
    "from utils.preprocessing import load_data, ReshapeAndScale, create_dataloaders\n",
    "from utils.fer2013_dataset import Fer2013Dataset"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmi4DkFbO1H9"
   },
   "source": [
    "First, we load the data into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "old1wNLVO1H_",
    "ExecuteTime": {
     "end_time": "2024-12-17T10:36:20.684608Z",
     "start_time": "2024-12-17T10:36:17.110912Z"
    }
   },
   "source": [
    "train_df = load_data(\"data/oversampled_train.csv\")\n",
    "val_df = load_data(\"data/validation.csv\")\n",
    "\n",
    "print(\"Training Data\")\n",
    "print(train_df.head(10))\n",
    "\n",
    "print(\"\\n\\nValidation Data\")\n",
    "print(val_df.head(10))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "                                              pixels  emotion\n",
      "0  208 209 210 209 209 208 206 215 162 47 44 47 5...        3\n",
      "1  164 166 170 168 168 169 171 172 173 172 172 17...        2\n",
      "2  246 248 248 248 244 124 45 37 45 98 141 160 16...        0\n",
      "3  1 0 0 1 1 1 2 3 2 3 3 9 10 7 6 7 9 10 16 16 22...        4\n",
      "4  208 201 211 209 196 192 177 177 187 186 193 19...        0\n",
      "5  4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 ...        2\n",
      "6  255 255 255 255 255 255 255 255 253 252 221 12...        3\n",
      "7  93 100 115 90 93 70 84 66 64 76 76 87 92 98 98...        3\n",
      "8  219 206 211 219 217 222 204 197 198 205 192 13...        6\n",
      "9  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 ...        0\n",
      "\n",
      "\n",
      "Validation Data\n",
      "   emotion                                             pixels\n",
      "0        3  254 253 253 254 254 254 252 252 253 252 253 25...\n",
      "1        4  92 70 64 65 62 64 91 139 167 181 186 187 191 1...\n",
      "2        5  20 19 16 23 24 25 34 24 25 60 113 123 139 149 ...\n",
      "3        4  79 82 83 84 85 89 92 90 93 95 94 97 95 97 93 8...\n",
      "4        2  117 105 95 73 72 65 49 66 92 105 126 154 177 1...\n",
      "5        3  138 138 138 118 129 146 138 120 89 57 50 47 54...\n",
      "6        5  141 132 104 64 37 27 24 23 19 18 19 20 23 22 2...\n",
      "7        3  17 23 19 14 31 27 28 41 52 69 88 96 106 108 10...\n",
      "8        6  117 110 91 49 44 46 47 41 26 39 57 71 83 91 10...\n",
      "9        3  89 141 163 153 136 128 127 125 121 116 30 23 2...\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7JiE5yJDO1IA",
    "outputId": "45dd2cb5-161b-4394-941f-82e2bc4c5699",
    "ExecuteTime": {
     "end_time": "2024-12-17T10:36:21.024038Z",
     "start_time": "2024-12-17T10:36:21.019138Z"
    }
   },
   "source": [
    "print(\"Training data shape:\", train_df.shape)\n",
    "print(\"Validation data shape\", val_df.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (40404, 2)\n",
      "Validation data shape (5742, 2)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Define a custom dataset"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We define a custom PyTorch dataset class, `Fer2013Dataset`, for handling the FER2013 data. The dataset is designed to load images (stored as pixel strings) and their corresponding emotion labels. It also supports optional transformations to preprocess the images during training. This setup makes it easy to integrate the dataset with PyTorch DataLoaders.\n",
    "\n",
    "The class is contained in the `utils/fer2013_dataset` file"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We also apply the following preprocessing steps to convert the data into the desired format we can further work with:\n",
    "1. **Reshaping**:\n",
    "   - Convert the pixel string into a `48x48` matrix for visualization and processing.\n",
    "2. **Scaling**:\n",
    "   - Scale pixel values to the range `[0, 1]` by dividing the pixel values by 255.\n",
    "3. **Normalization**:\n",
    "   - Normalize pixel values to the range `[-1, 1]` by subtracting the mean and diving them by the standard deviation.\n",
    "\n",
    "\n",
    "These preprocessing steps are contained in the class `ReshapeAndScale`, which is available under path `utils/preprocessing.py`."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T10:36:21.077418Z",
     "start_time": "2024-12-17T10:36:21.072597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a default transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    ReshapeAndScale(n_rows=48, n_cols=48),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "id": "8cuhkZCOO1II",
    "ExecuteTime": {
     "end_time": "2024-12-17T10:36:21.092982Z",
     "start_time": "2024-12-17T10:36:21.089820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = Fer2013Dataset(train_df, train_df['emotion'], transform=transform)\n",
    "val_dataset = Fer2013Dataset(val_df, val_df['emotion'], transform=transform)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create DataLoaders\n",
    "\n",
    "We create DataLoaders for both subsets to enable batch processing. The training DataLoader shuffles the data for better learning, while the validation DataLoader does not. Finally, we print the shapes of the batches to verify that everything works correctly."
   ]
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "rAQ30-FQO1IJ",
    "outputId": "aae71765-1a27-4d1a-9fe0-d34cee950122",
    "ExecuteTime": {
     "end_time": "2024-12-17T10:36:21.110911Z",
     "start_time": "2024-12-17T10:36:21.106212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "train_loader, val_loader = create_dataloaders(train_dataset, val_dataset, batch_size)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pge_vvuYO1IN"
   },
   "source": "## 3. Define the CNN model"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a custom Convolutional Neural Network (CNN) for emotion recognition. The model includes multiple convolutional layers with batch normalization, dropout for regularization, max pooling for downsampling, dilation for determining the kernel spacing and fully connected layers for classification.\n",
    "\n",
    "The network dynamically calculates the flattened size needed for the fully connected layers based on the input size (48x48 grayscale images). Finally, we instantiate the model, move it to the available device (CPU or GPU), and print its architecture for verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvFxqkzxO1IO",
    "outputId": "a7824d62-eb91-42c3-8f2f-def089a2ea5c",
    "ExecuteTime": {
     "end_time": "2024-12-17T10:36:21.250286Z",
     "start_time": "2024-12-17T10:36:21.128106Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # 1st Conv Layer (No dilation)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding='valid')\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "\n",
    "        # 2nd Conv Layer (No dilation)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 3rd Conv Layer (dilation=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=2, dilation=2)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "\n",
    "        # 4th Conv Layer (dilation=2)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=2, dilation=2)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.pool4 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 5th Conv Layer (dilation=4)\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=4, dilation=4)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.pool5 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Calculate the flatten size dynamically\n",
    "        self.flatten_size = self._get_flatten_size()\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(self.flatten_size, 250)\n",
    "        self.dropout_fc = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(250, 7)  # 7 classes for emotion recognition\n",
    "\n",
    "    def _get_flatten_size(self):\n",
    "        # Create a dummy tensor with the same size as input image\n",
    "        dummy_input = torch.zeros(1, 1, 48, 48)  # batch_size, channels, height, width\n",
    "        dummy_output = self._forward_conv_layers(dummy_input)\n",
    "        return dummy_output.numel()\n",
    "\n",
    "    def _forward_conv_layers(self, x):\n",
    "        # Pass through convolutional layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool5(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use the convolutional layers defined in _forward_conv_layers\n",
    "        x = self._forward_conv_layers(x)\n",
    "\n",
    "        # Dynamically flatten the output\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN()\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model to verify layers\n",
    "print(model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
      "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout3): Dropout(p=0.25, inplace=False)\n",
      "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
      "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (conv5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))\n",
      "  (bn5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (fc1): Linear(in_features=3200, out_features=250, bias=True)\n",
      "  (dropout_fc): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=250, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Define Loss Function and Optimizer"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we define the loss function and optimizer for training the model:\n",
    "- **Loss Function**: `CrossEntropyLoss` is used, which is well-suited for multi-class classification tasks like emotion recognition.\n",
    "- **Optimizer**: The Adam optimizer is initialized with a learning rate of `0.0001` to update the model parameters during training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WFyRGyfXO1IR",
    "ExecuteTime": {
     "end_time": "2024-12-17T10:36:21.271257Z",
     "start_time": "2024-12-17T10:36:21.260959Z"
    }
   },
   "source": [
    "criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # call optimizer"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMHZrQsZO1IR"
   },
   "source": "## 5. Train the Model"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we define the training loop for the CNN:\n",
    "- **Number of Epochs**: The model is trained for 35 epochs.\n",
    "- **Training Process**:\n",
    "  - The model is set to training mode.\n",
    "  - For each batch, we move inputs and labels to the appropriate device, clear the gradients, perform forward and backward passes, and update the model's parameters using the optimizer.\n",
    "  - The running loss is tracked and printed every 100 batches for monitoring."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "iX3PbDPqO1IR",
    "outputId": "09fb2a30-0bb4-47b5-e1d7-2d0d47db40ca",
    "ExecuteTime": {
     "end_time": "2024-12-17T13:45:33.331240Z",
     "start_time": "2024-12-17T10:36:21.300811Z"
    }
   },
   "source": [
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir='tensorboard-runs/8_Group17_DLProject')\n",
    "\n",
    "checkpoint_path = 'model-checkpoints/8_Group17_DLProject'\n",
    "os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "model_save_path = 'models'\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "num_epochs = 35\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 100 == 0:  # Print every 100 batches\n",
    "            avg_loss = running_loss / 100\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {avg_loss:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Log loss to TensorBoard\n",
    "            writer.add_scalar('Loss/train', avg_loss, epoch)\n",
    "\n",
    "    # Save checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': running_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, f\"{checkpoint_path}/checkpoint_epoch_{epoch + 1}.pth\")\n",
    "        print(f\"Checkpoint saved at epoch {epoch + 1}\")\n",
    "\n",
    "# Save the final model after training\n",
    "final_model_path = os.path.join(model_save_path, '8_Group17_DLProject.pth')\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final model saved at {final_model_path}\")\n",
    "\n",
    "# Close the writer after training\n",
    "writer.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/35], Step [100/1263], Loss: 1.9223\n",
      "Epoch [1/35], Step [200/1263], Loss: 1.8731\n",
      "Epoch [1/35], Step [300/1263], Loss: 1.8105\n",
      "Epoch [1/35], Step [400/1263], Loss: 1.7618\n",
      "Epoch [1/35], Step [500/1263], Loss: 1.6718\n",
      "Epoch [1/35], Step [600/1263], Loss: 1.6105\n",
      "Epoch [1/35], Step [700/1263], Loss: 1.5988\n",
      "Epoch [1/35], Step [800/1263], Loss: 1.5482\n",
      "Epoch [1/35], Step [900/1263], Loss: 1.5088\n",
      "Epoch [1/35], Step [1000/1263], Loss: 1.4707\n",
      "Epoch [1/35], Step [1100/1263], Loss: 1.4634\n",
      "Epoch [1/35], Step [1200/1263], Loss: 1.4352\n",
      "Epoch [2/35], Step [100/1263], Loss: 1.3660\n",
      "Epoch [2/35], Step [200/1263], Loss: 1.3662\n",
      "Epoch [2/35], Step [300/1263], Loss: 1.3455\n",
      "Epoch [2/35], Step [400/1263], Loss: 1.3278\n",
      "Epoch [2/35], Step [500/1263], Loss: 1.2847\n",
      "Epoch [2/35], Step [600/1263], Loss: 1.3358\n",
      "Epoch [2/35], Step [700/1263], Loss: 1.2664\n",
      "Epoch [2/35], Step [800/1263], Loss: 1.2815\n",
      "Epoch [2/35], Step [900/1263], Loss: 1.2454\n",
      "Epoch [2/35], Step [1000/1263], Loss: 1.2112\n",
      "Epoch [2/35], Step [1100/1263], Loss: 1.2001\n",
      "Epoch [2/35], Step [1200/1263], Loss: 1.2051\n",
      "Epoch [3/35], Step [100/1263], Loss: 1.1835\n",
      "Epoch [3/35], Step [200/1263], Loss: 1.1675\n",
      "Epoch [3/35], Step [300/1263], Loss: 1.1396\n",
      "Epoch [3/35], Step [400/1263], Loss: 1.1156\n",
      "Epoch [3/35], Step [500/1263], Loss: 1.1337\n",
      "Epoch [3/35], Step [600/1263], Loss: 1.1187\n",
      "Epoch [3/35], Step [700/1263], Loss: 1.1423\n",
      "Epoch [3/35], Step [800/1263], Loss: 1.0844\n",
      "Epoch [3/35], Step [900/1263], Loss: 1.0878\n",
      "Epoch [3/35], Step [1000/1263], Loss: 1.0750\n",
      "Epoch [3/35], Step [1100/1263], Loss: 1.1026\n",
      "Epoch [3/35], Step [1200/1263], Loss: 1.0924\n",
      "Epoch [4/35], Step [100/1263], Loss: 1.0317\n",
      "Epoch [4/35], Step [200/1263], Loss: 1.0511\n",
      "Epoch [4/35], Step [300/1263], Loss: 1.0042\n",
      "Epoch [4/35], Step [400/1263], Loss: 1.0356\n",
      "Epoch [4/35], Step [500/1263], Loss: 1.0284\n",
      "Epoch [4/35], Step [600/1263], Loss: 1.0141\n",
      "Epoch [4/35], Step [700/1263], Loss: 1.0204\n",
      "Epoch [4/35], Step [800/1263], Loss: 1.0308\n",
      "Epoch [4/35], Step [900/1263], Loss: 1.0336\n",
      "Epoch [4/35], Step [1000/1263], Loss: 1.0068\n",
      "Epoch [4/35], Step [1100/1263], Loss: 1.0258\n",
      "Epoch [4/35], Step [1200/1263], Loss: 1.0104\n",
      "Epoch [5/35], Step [100/1263], Loss: 0.9533\n",
      "Epoch [5/35], Step [200/1263], Loss: 0.9268\n",
      "Epoch [5/35], Step [300/1263], Loss: 0.9322\n",
      "Epoch [5/35], Step [400/1263], Loss: 0.9402\n",
      "Epoch [5/35], Step [500/1263], Loss: 0.9786\n",
      "Epoch [5/35], Step [600/1263], Loss: 0.9681\n",
      "Epoch [5/35], Step [700/1263], Loss: 0.9756\n",
      "Epoch [5/35], Step [800/1263], Loss: 0.9450\n",
      "Epoch [5/35], Step [900/1263], Loss: 0.9432\n",
      "Epoch [5/35], Step [1000/1263], Loss: 0.9352\n",
      "Epoch [5/35], Step [1100/1263], Loss: 0.9196\n",
      "Epoch [5/35], Step [1200/1263], Loss: 0.9433\n",
      "Epoch [6/35], Step [100/1263], Loss: 0.9031\n",
      "Epoch [6/35], Step [200/1263], Loss: 0.9103\n",
      "Epoch [6/35], Step [300/1263], Loss: 0.8787\n",
      "Epoch [6/35], Step [400/1263], Loss: 0.8845\n",
      "Epoch [6/35], Step [500/1263], Loss: 0.9014\n",
      "Epoch [6/35], Step [600/1263], Loss: 0.8975\n",
      "Epoch [6/35], Step [700/1263], Loss: 0.8763\n",
      "Epoch [6/35], Step [800/1263], Loss: 0.8861\n",
      "Epoch [6/35], Step [900/1263], Loss: 0.8868\n",
      "Epoch [6/35], Step [1000/1263], Loss: 0.8925\n",
      "Epoch [6/35], Step [1100/1263], Loss: 0.8860\n",
      "Epoch [6/35], Step [1200/1263], Loss: 0.8668\n",
      "Epoch [7/35], Step [100/1263], Loss: 0.8463\n",
      "Epoch [7/35], Step [200/1263], Loss: 0.8405\n",
      "Epoch [7/35], Step [300/1263], Loss: 0.8336\n",
      "Epoch [7/35], Step [400/1263], Loss: 0.8407\n",
      "Epoch [7/35], Step [500/1263], Loss: 0.8503\n",
      "Epoch [7/35], Step [600/1263], Loss: 0.8102\n",
      "Epoch [7/35], Step [700/1263], Loss: 0.8056\n",
      "Epoch [7/35], Step [800/1263], Loss: 0.8226\n",
      "Epoch [7/35], Step [900/1263], Loss: 0.8070\n",
      "Epoch [7/35], Step [1000/1263], Loss: 0.8394\n",
      "Epoch [7/35], Step [1100/1263], Loss: 0.8337\n",
      "Epoch [7/35], Step [1200/1263], Loss: 0.7852\n",
      "Epoch [8/35], Step [100/1263], Loss: 0.7977\n",
      "Epoch [8/35], Step [200/1263], Loss: 0.7788\n",
      "Epoch [8/35], Step [300/1263], Loss: 0.8005\n",
      "Epoch [8/35], Step [400/1263], Loss: 0.7631\n",
      "Epoch [8/35], Step [500/1263], Loss: 0.7957\n",
      "Epoch [8/35], Step [600/1263], Loss: 0.7441\n",
      "Epoch [8/35], Step [700/1263], Loss: 0.7901\n",
      "Epoch [8/35], Step [800/1263], Loss: 0.7594\n",
      "Epoch [8/35], Step [900/1263], Loss: 0.7520\n",
      "Epoch [8/35], Step [1000/1263], Loss: 0.7855\n",
      "Epoch [8/35], Step [1100/1263], Loss: 0.7728\n",
      "Epoch [8/35], Step [1200/1263], Loss: 0.7896\n",
      "Epoch [9/35], Step [100/1263], Loss: 0.7325\n",
      "Epoch [9/35], Step [200/1263], Loss: 0.7169\n",
      "Epoch [9/35], Step [300/1263], Loss: 0.7023\n",
      "Epoch [9/35], Step [400/1263], Loss: 0.7281\n",
      "Epoch [9/35], Step [500/1263], Loss: 0.7435\n",
      "Epoch [9/35], Step [600/1263], Loss: 0.7441\n",
      "Epoch [9/35], Step [700/1263], Loss: 0.7358\n",
      "Epoch [9/35], Step [800/1263], Loss: 0.7038\n",
      "Epoch [9/35], Step [900/1263], Loss: 0.7178\n",
      "Epoch [9/35], Step [1000/1263], Loss: 0.7036\n",
      "Epoch [9/35], Step [1100/1263], Loss: 0.7301\n",
      "Epoch [9/35], Step [1200/1263], Loss: 0.7044\n",
      "Epoch [10/35], Step [100/1263], Loss: 0.6592\n",
      "Epoch [10/35], Step [200/1263], Loss: 0.6678\n",
      "Epoch [10/35], Step [300/1263], Loss: 0.6714\n",
      "Epoch [10/35], Step [400/1263], Loss: 0.6894\n",
      "Epoch [10/35], Step [500/1263], Loss: 0.6993\n",
      "Epoch [10/35], Step [600/1263], Loss: 0.6612\n",
      "Epoch [10/35], Step [700/1263], Loss: 0.6808\n",
      "Epoch [10/35], Step [800/1263], Loss: 0.6715\n",
      "Epoch [10/35], Step [900/1263], Loss: 0.6980\n",
      "Epoch [10/35], Step [1000/1263], Loss: 0.6997\n",
      "Epoch [10/35], Step [1100/1263], Loss: 0.6625\n",
      "Epoch [10/35], Step [1200/1263], Loss: 0.6382\n",
      "Checkpoint saved at epoch 10\n",
      "Epoch [11/35], Step [100/1263], Loss: 0.6127\n",
      "Epoch [11/35], Step [200/1263], Loss: 0.6213\n",
      "Epoch [11/35], Step [300/1263], Loss: 0.6152\n",
      "Epoch [11/35], Step [400/1263], Loss: 0.6071\n",
      "Epoch [11/35], Step [500/1263], Loss: 0.6296\n",
      "Epoch [11/35], Step [600/1263], Loss: 0.6361\n",
      "Epoch [11/35], Step [700/1263], Loss: 0.6146\n",
      "Epoch [11/35], Step [800/1263], Loss: 0.6557\n",
      "Epoch [11/35], Step [900/1263], Loss: 0.6322\n",
      "Epoch [11/35], Step [1000/1263], Loss: 0.6511\n",
      "Epoch [11/35], Step [1100/1263], Loss: 0.6283\n",
      "Epoch [11/35], Step [1200/1263], Loss: 0.6204\n",
      "Epoch [12/35], Step [100/1263], Loss: 0.5878\n",
      "Epoch [12/35], Step [200/1263], Loss: 0.5782\n",
      "Epoch [12/35], Step [300/1263], Loss: 0.6197\n",
      "Epoch [12/35], Step [400/1263], Loss: 0.5881\n",
      "Epoch [12/35], Step [500/1263], Loss: 0.5678\n",
      "Epoch [12/35], Step [600/1263], Loss: 0.5967\n",
      "Epoch [12/35], Step [700/1263], Loss: 0.5955\n",
      "Epoch [12/35], Step [800/1263], Loss: 0.5691\n",
      "Epoch [12/35], Step [900/1263], Loss: 0.5730\n",
      "Epoch [12/35], Step [1000/1263], Loss: 0.5826\n",
      "Epoch [12/35], Step [1100/1263], Loss: 0.5762\n",
      "Epoch [12/35], Step [1200/1263], Loss: 0.5899\n",
      "Epoch [13/35], Step [100/1263], Loss: 0.5167\n",
      "Epoch [13/35], Step [200/1263], Loss: 0.5161\n",
      "Epoch [13/35], Step [300/1263], Loss: 0.5591\n",
      "Epoch [13/35], Step [400/1263], Loss: 0.5565\n",
      "Epoch [13/35], Step [500/1263], Loss: 0.5555\n",
      "Epoch [13/35], Step [600/1263], Loss: 0.5449\n",
      "Epoch [13/35], Step [700/1263], Loss: 0.5714\n",
      "Epoch [13/35], Step [800/1263], Loss: 0.5723\n",
      "Epoch [13/35], Step [900/1263], Loss: 0.5189\n",
      "Epoch [13/35], Step [1000/1263], Loss: 0.5151\n",
      "Epoch [13/35], Step [1100/1263], Loss: 0.5468\n",
      "Epoch [13/35], Step [1200/1263], Loss: 0.5421\n",
      "Epoch [14/35], Step [100/1263], Loss: 0.4930\n",
      "Epoch [14/35], Step [200/1263], Loss: 0.4853\n",
      "Epoch [14/35], Step [300/1263], Loss: 0.5294\n",
      "Epoch [14/35], Step [400/1263], Loss: 0.4809\n",
      "Epoch [14/35], Step [500/1263], Loss: 0.4653\n",
      "Epoch [14/35], Step [600/1263], Loss: 0.5028\n",
      "Epoch [14/35], Step [700/1263], Loss: 0.4850\n",
      "Epoch [14/35], Step [800/1263], Loss: 0.5015\n",
      "Epoch [14/35], Step [900/1263], Loss: 0.5214\n",
      "Epoch [14/35], Step [1000/1263], Loss: 0.4874\n",
      "Epoch [14/35], Step [1100/1263], Loss: 0.5039\n",
      "Epoch [14/35], Step [1200/1263], Loss: 0.5192\n",
      "Epoch [15/35], Step [100/1263], Loss: 0.4571\n",
      "Epoch [15/35], Step [200/1263], Loss: 0.4324\n",
      "Epoch [15/35], Step [300/1263], Loss: 0.4458\n",
      "Epoch [15/35], Step [400/1263], Loss: 0.4368\n",
      "Epoch [15/35], Step [500/1263], Loss: 0.4628\n",
      "Epoch [15/35], Step [600/1263], Loss: 0.4723\n",
      "Epoch [15/35], Step [700/1263], Loss: 0.4639\n",
      "Epoch [15/35], Step [800/1263], Loss: 0.4450\n",
      "Epoch [15/35], Step [900/1263], Loss: 0.4891\n",
      "Epoch [15/35], Step [1000/1263], Loss: 0.4634\n",
      "Epoch [15/35], Step [1100/1263], Loss: 0.4916\n",
      "Epoch [15/35], Step [1200/1263], Loss: 0.4662\n",
      "Epoch [16/35], Step [100/1263], Loss: 0.3928\n",
      "Epoch [16/35], Step [200/1263], Loss: 0.4214\n",
      "Epoch [16/35], Step [300/1263], Loss: 0.4368\n",
      "Epoch [16/35], Step [400/1263], Loss: 0.4391\n",
      "Epoch [16/35], Step [500/1263], Loss: 0.4178\n",
      "Epoch [16/35], Step [600/1263], Loss: 0.4363\n",
      "Epoch [16/35], Step [700/1263], Loss: 0.4276\n",
      "Epoch [16/35], Step [800/1263], Loss: 0.4101\n",
      "Epoch [16/35], Step [900/1263], Loss: 0.4155\n",
      "Epoch [16/35], Step [1000/1263], Loss: 0.4610\n",
      "Epoch [16/35], Step [1100/1263], Loss: 0.4580\n",
      "Epoch [16/35], Step [1200/1263], Loss: 0.4620\n",
      "Epoch [17/35], Step [100/1263], Loss: 0.3919\n",
      "Epoch [17/35], Step [200/1263], Loss: 0.3739\n",
      "Epoch [17/35], Step [300/1263], Loss: 0.3918\n",
      "Epoch [17/35], Step [400/1263], Loss: 0.3965\n",
      "Epoch [17/35], Step [500/1263], Loss: 0.3901\n",
      "Epoch [17/35], Step [600/1263], Loss: 0.3825\n",
      "Epoch [17/35], Step [700/1263], Loss: 0.3645\n",
      "Epoch [17/35], Step [800/1263], Loss: 0.3889\n",
      "Epoch [17/35], Step [900/1263], Loss: 0.4021\n",
      "Epoch [17/35], Step [1000/1263], Loss: 0.3964\n",
      "Epoch [17/35], Step [1100/1263], Loss: 0.3964\n",
      "Epoch [17/35], Step [1200/1263], Loss: 0.3974\n",
      "Epoch [18/35], Step [100/1263], Loss: 0.3428\n",
      "Epoch [18/35], Step [200/1263], Loss: 0.3747\n",
      "Epoch [18/35], Step [300/1263], Loss: 0.3533\n",
      "Epoch [18/35], Step [400/1263], Loss: 0.3565\n",
      "Epoch [18/35], Step [500/1263], Loss: 0.3880\n",
      "Epoch [18/35], Step [600/1263], Loss: 0.3683\n",
      "Epoch [18/35], Step [700/1263], Loss: 0.3603\n",
      "Epoch [18/35], Step [800/1263], Loss: 0.3867\n",
      "Epoch [18/35], Step [900/1263], Loss: 0.3984\n",
      "Epoch [18/35], Step [1000/1263], Loss: 0.3472\n",
      "Epoch [18/35], Step [1100/1263], Loss: 0.3785\n",
      "Epoch [18/35], Step [1200/1263], Loss: 0.3824\n",
      "Epoch [19/35], Step [100/1263], Loss: 0.2947\n",
      "Epoch [19/35], Step [200/1263], Loss: 0.3038\n",
      "Epoch [19/35], Step [300/1263], Loss: 0.3371\n",
      "Epoch [19/35], Step [400/1263], Loss: 0.3425\n",
      "Epoch [19/35], Step [500/1263], Loss: 0.3331\n",
      "Epoch [19/35], Step [600/1263], Loss: 0.3514\n",
      "Epoch [19/35], Step [700/1263], Loss: 0.3400\n",
      "Epoch [19/35], Step [800/1263], Loss: 0.3497\n",
      "Epoch [19/35], Step [900/1263], Loss: 0.3615\n",
      "Epoch [19/35], Step [1000/1263], Loss: 0.3783\n",
      "Epoch [19/35], Step [1100/1263], Loss: 0.3720\n",
      "Epoch [19/35], Step [1200/1263], Loss: 0.3328\n",
      "Epoch [20/35], Step [100/1263], Loss: 0.2880\n",
      "Epoch [20/35], Step [200/1263], Loss: 0.3300\n",
      "Epoch [20/35], Step [300/1263], Loss: 0.3068\n",
      "Epoch [20/35], Step [400/1263], Loss: 0.3089\n",
      "Epoch [20/35], Step [500/1263], Loss: 0.3290\n",
      "Epoch [20/35], Step [600/1263], Loss: 0.3404\n",
      "Epoch [20/35], Step [700/1263], Loss: 0.2927\n",
      "Epoch [20/35], Step [800/1263], Loss: 0.3114\n",
      "Epoch [20/35], Step [900/1263], Loss: 0.3320\n",
      "Epoch [20/35], Step [1000/1263], Loss: 0.3263\n",
      "Epoch [20/35], Step [1100/1263], Loss: 0.3088\n",
      "Epoch [20/35], Step [1200/1263], Loss: 0.3305\n",
      "Checkpoint saved at epoch 20\n",
      "Epoch [21/35], Step [100/1263], Loss: 0.2648\n",
      "Epoch [21/35], Step [200/1263], Loss: 0.2747\n",
      "Epoch [21/35], Step [300/1263], Loss: 0.3216\n",
      "Epoch [21/35], Step [400/1263], Loss: 0.3057\n",
      "Epoch [21/35], Step [500/1263], Loss: 0.3008\n",
      "Epoch [21/35], Step [600/1263], Loss: 0.2986\n",
      "Epoch [21/35], Step [700/1263], Loss: 0.2959\n",
      "Epoch [21/35], Step [800/1263], Loss: 0.2910\n",
      "Epoch [21/35], Step [900/1263], Loss: 0.3137\n",
      "Epoch [21/35], Step [1000/1263], Loss: 0.2995\n",
      "Epoch [21/35], Step [1100/1263], Loss: 0.2932\n",
      "Epoch [21/35], Step [1200/1263], Loss: 0.3017\n",
      "Epoch [22/35], Step [100/1263], Loss: 0.2461\n",
      "Epoch [22/35], Step [200/1263], Loss: 0.2797\n",
      "Epoch [22/35], Step [300/1263], Loss: 0.2638\n",
      "Epoch [22/35], Step [400/1263], Loss: 0.2846\n",
      "Epoch [22/35], Step [500/1263], Loss: 0.2439\n",
      "Epoch [22/35], Step [600/1263], Loss: 0.2542\n",
      "Epoch [22/35], Step [700/1263], Loss: 0.2910\n",
      "Epoch [22/35], Step [800/1263], Loss: 0.2849\n",
      "Epoch [22/35], Step [900/1263], Loss: 0.2946\n",
      "Epoch [22/35], Step [1000/1263], Loss: 0.3063\n",
      "Epoch [22/35], Step [1100/1263], Loss: 0.2810\n",
      "Epoch [22/35], Step [1200/1263], Loss: 0.2931\n",
      "Epoch [23/35], Step [100/1263], Loss: 0.2369\n",
      "Epoch [23/35], Step [200/1263], Loss: 0.2325\n",
      "Epoch [23/35], Step [300/1263], Loss: 0.2515\n",
      "Epoch [23/35], Step [400/1263], Loss: 0.2471\n",
      "Epoch [23/35], Step [500/1263], Loss: 0.2725\n",
      "Epoch [23/35], Step [600/1263], Loss: 0.2523\n",
      "Epoch [23/35], Step [700/1263], Loss: 0.2407\n",
      "Epoch [23/35], Step [800/1263], Loss: 0.2659\n",
      "Epoch [23/35], Step [900/1263], Loss: 0.2604\n",
      "Epoch [23/35], Step [1000/1263], Loss: 0.2678\n",
      "Epoch [23/35], Step [1100/1263], Loss: 0.2744\n",
      "Epoch [23/35], Step [1200/1263], Loss: 0.2687\n",
      "Epoch [24/35], Step [100/1263], Loss: 0.2267\n",
      "Epoch [24/35], Step [200/1263], Loss: 0.2286\n",
      "Epoch [24/35], Step [300/1263], Loss: 0.2435\n",
      "Epoch [24/35], Step [400/1263], Loss: 0.2437\n",
      "Epoch [24/35], Step [500/1263], Loss: 0.2443\n",
      "Epoch [24/35], Step [600/1263], Loss: 0.2421\n",
      "Epoch [24/35], Step [700/1263], Loss: 0.2567\n",
      "Epoch [24/35], Step [800/1263], Loss: 0.2346\n",
      "Epoch [24/35], Step [900/1263], Loss: 0.2484\n",
      "Epoch [24/35], Step [1000/1263], Loss: 0.2528\n",
      "Epoch [24/35], Step [1100/1263], Loss: 0.2398\n",
      "Epoch [24/35], Step [1200/1263], Loss: 0.2354\n",
      "Epoch [25/35], Step [100/1263], Loss: 0.2194\n",
      "Epoch [25/35], Step [200/1263], Loss: 0.2122\n",
      "Epoch [25/35], Step [300/1263], Loss: 0.2221\n",
      "Epoch [25/35], Step [400/1263], Loss: 0.2340\n",
      "Epoch [25/35], Step [500/1263], Loss: 0.2170\n",
      "Epoch [25/35], Step [600/1263], Loss: 0.2324\n",
      "Epoch [25/35], Step [700/1263], Loss: 0.2252\n",
      "Epoch [25/35], Step [800/1263], Loss: 0.2463\n",
      "Epoch [25/35], Step [900/1263], Loss: 0.2275\n",
      "Epoch [25/35], Step [1000/1263], Loss: 0.2171\n",
      "Epoch [25/35], Step [1100/1263], Loss: 0.2501\n",
      "Epoch [25/35], Step [1200/1263], Loss: 0.2452\n",
      "Epoch [26/35], Step [100/1263], Loss: 0.1990\n",
      "Epoch [26/35], Step [200/1263], Loss: 0.1977\n",
      "Epoch [26/35], Step [300/1263], Loss: 0.1942\n",
      "Epoch [26/35], Step [400/1263], Loss: 0.2101\n",
      "Epoch [26/35], Step [500/1263], Loss: 0.2121\n",
      "Epoch [26/35], Step [600/1263], Loss: 0.1995\n",
      "Epoch [26/35], Step [700/1263], Loss: 0.2123\n",
      "Epoch [26/35], Step [800/1263], Loss: 0.2173\n",
      "Epoch [26/35], Step [900/1263], Loss: 0.2210\n",
      "Epoch [26/35], Step [1000/1263], Loss: 0.2288\n",
      "Epoch [26/35], Step [1100/1263], Loss: 0.2435\n",
      "Epoch [26/35], Step [1200/1263], Loss: 0.2067\n",
      "Epoch [27/35], Step [100/1263], Loss: 0.1950\n",
      "Epoch [27/35], Step [200/1263], Loss: 0.1889\n",
      "Epoch [27/35], Step [300/1263], Loss: 0.2040\n",
      "Epoch [27/35], Step [400/1263], Loss: 0.1942\n",
      "Epoch [27/35], Step [500/1263], Loss: 0.1928\n",
      "Epoch [27/35], Step [600/1263], Loss: 0.2009\n",
      "Epoch [27/35], Step [700/1263], Loss: 0.2050\n",
      "Epoch [27/35], Step [800/1263], Loss: 0.2137\n",
      "Epoch [27/35], Step [900/1263], Loss: 0.2078\n",
      "Epoch [27/35], Step [1000/1263], Loss: 0.2092\n",
      "Epoch [27/35], Step [1100/1263], Loss: 0.1994\n",
      "Epoch [27/35], Step [1200/1263], Loss: 0.2074\n",
      "Epoch [28/35], Step [100/1263], Loss: 0.1919\n",
      "Epoch [28/35], Step [200/1263], Loss: 0.1687\n",
      "Epoch [28/35], Step [300/1263], Loss: 0.1776\n",
      "Epoch [28/35], Step [400/1263], Loss: 0.1802\n",
      "Epoch [28/35], Step [500/1263], Loss: 0.1985\n",
      "Epoch [28/35], Step [600/1263], Loss: 0.1883\n",
      "Epoch [28/35], Step [700/1263], Loss: 0.1965\n",
      "Epoch [28/35], Step [800/1263], Loss: 0.1921\n",
      "Epoch [28/35], Step [900/1263], Loss: 0.2041\n",
      "Epoch [28/35], Step [1000/1263], Loss: 0.2031\n",
      "Epoch [28/35], Step [1100/1263], Loss: 0.2031\n",
      "Epoch [28/35], Step [1200/1263], Loss: 0.2196\n",
      "Epoch [29/35], Step [100/1263], Loss: 0.1697\n",
      "Epoch [29/35], Step [200/1263], Loss: 0.1564\n",
      "Epoch [29/35], Step [300/1263], Loss: 0.1933\n",
      "Epoch [29/35], Step [400/1263], Loss: 0.1783\n",
      "Epoch [29/35], Step [500/1263], Loss: 0.1857\n",
      "Epoch [29/35], Step [600/1263], Loss: 0.1871\n",
      "Epoch [29/35], Step [700/1263], Loss: 0.1878\n",
      "Epoch [29/35], Step [800/1263], Loss: 0.1885\n",
      "Epoch [29/35], Step [900/1263], Loss: 0.1931\n",
      "Epoch [29/35], Step [1000/1263], Loss: 0.1865\n",
      "Epoch [29/35], Step [1100/1263], Loss: 0.1901\n",
      "Epoch [29/35], Step [1200/1263], Loss: 0.1882\n",
      "Epoch [30/35], Step [100/1263], Loss: 0.1677\n",
      "Epoch [30/35], Step [200/1263], Loss: 0.1577\n",
      "Epoch [30/35], Step [300/1263], Loss: 0.1590\n",
      "Epoch [30/35], Step [400/1263], Loss: 0.1763\n",
      "Epoch [30/35], Step [500/1263], Loss: 0.1844\n",
      "Epoch [30/35], Step [600/1263], Loss: 0.1694\n",
      "Epoch [30/35], Step [700/1263], Loss: 0.1632\n",
      "Epoch [30/35], Step [800/1263], Loss: 0.1741\n",
      "Epoch [30/35], Step [900/1263], Loss: 0.1841\n",
      "Epoch [30/35], Step [1000/1263], Loss: 0.1758\n",
      "Epoch [30/35], Step [1100/1263], Loss: 0.1654\n",
      "Epoch [30/35], Step [1200/1263], Loss: 0.1819\n",
      "Checkpoint saved at epoch 30\n",
      "Epoch [31/35], Step [100/1263], Loss: 0.1518\n",
      "Epoch [31/35], Step [200/1263], Loss: 0.1422\n",
      "Epoch [31/35], Step [300/1263], Loss: 0.1374\n",
      "Epoch [31/35], Step [400/1263], Loss: 0.1684\n",
      "Epoch [31/35], Step [500/1263], Loss: 0.1572\n",
      "Epoch [31/35], Step [600/1263], Loss: 0.1807\n",
      "Epoch [31/35], Step [700/1263], Loss: 0.1800\n",
      "Epoch [31/35], Step [800/1263], Loss: 0.1785\n",
      "Epoch [31/35], Step [900/1263], Loss: 0.1745\n",
      "Epoch [31/35], Step [1000/1263], Loss: 0.1681\n",
      "Epoch [31/35], Step [1100/1263], Loss: 0.1711\n",
      "Epoch [31/35], Step [1200/1263], Loss: 0.1724\n",
      "Epoch [32/35], Step [100/1263], Loss: 0.1355\n",
      "Epoch [32/35], Step [200/1263], Loss: 0.1575\n",
      "Epoch [32/35], Step [300/1263], Loss: 0.1516\n",
      "Epoch [32/35], Step [400/1263], Loss: 0.1556\n",
      "Epoch [32/35], Step [500/1263], Loss: 0.1627\n",
      "Epoch [32/35], Step [600/1263], Loss: 0.1615\n",
      "Epoch [32/35], Step [700/1263], Loss: 0.1627\n",
      "Epoch [32/35], Step [800/1263], Loss: 0.1665\n",
      "Epoch [32/35], Step [900/1263], Loss: 0.1470\n",
      "Epoch [32/35], Step [1000/1263], Loss: 0.1660\n",
      "Epoch [32/35], Step [1100/1263], Loss: 0.1694\n",
      "Epoch [32/35], Step [1200/1263], Loss: 0.1672\n",
      "Epoch [33/35], Step [100/1263], Loss: 0.1537\n",
      "Epoch [33/35], Step [200/1263], Loss: 0.1343\n",
      "Epoch [33/35], Step [300/1263], Loss: 0.1346\n",
      "Epoch [33/35], Step [400/1263], Loss: 0.1236\n",
      "Epoch [33/35], Step [500/1263], Loss: 0.1388\n",
      "Epoch [33/35], Step [600/1263], Loss: 0.1583\n",
      "Epoch [33/35], Step [700/1263], Loss: 0.1496\n",
      "Epoch [33/35], Step [800/1263], Loss: 0.1519\n",
      "Epoch [33/35], Step [900/1263], Loss: 0.1482\n",
      "Epoch [33/35], Step [1000/1263], Loss: 0.1422\n",
      "Epoch [33/35], Step [1100/1263], Loss: 0.1625\n",
      "Epoch [33/35], Step [1200/1263], Loss: 0.1577\n",
      "Epoch [34/35], Step [100/1263], Loss: 0.1233\n",
      "Epoch [34/35], Step [200/1263], Loss: 0.1276\n",
      "Epoch [34/35], Step [300/1263], Loss: 0.1290\n",
      "Epoch [34/35], Step [400/1263], Loss: 0.1301\n",
      "Epoch [34/35], Step [500/1263], Loss: 0.1402\n",
      "Epoch [34/35], Step [600/1263], Loss: 0.1598\n",
      "Epoch [34/35], Step [700/1263], Loss: 0.1536\n",
      "Epoch [34/35], Step [800/1263], Loss: 0.1496\n",
      "Epoch [34/35], Step [900/1263], Loss: 0.1546\n",
      "Epoch [34/35], Step [1000/1263], Loss: 0.1563\n",
      "Epoch [34/35], Step [1100/1263], Loss: 0.1790\n",
      "Epoch [34/35], Step [1200/1263], Loss: 0.1411\n",
      "Epoch [35/35], Step [100/1263], Loss: 0.1188\n",
      "Epoch [35/35], Step [200/1263], Loss: 0.1280\n",
      "Epoch [35/35], Step [300/1263], Loss: 0.1392\n",
      "Epoch [35/35], Step [400/1263], Loss: 0.1345\n",
      "Epoch [35/35], Step [500/1263], Loss: 0.1148\n",
      "Epoch [35/35], Step [600/1263], Loss: 0.1532\n",
      "Epoch [35/35], Step [700/1263], Loss: 0.1377\n",
      "Epoch [35/35], Step [800/1263], Loss: 0.1404\n",
      "Epoch [35/35], Step [900/1263], Loss: 0.1355\n",
      "Epoch [35/35], Step [1000/1263], Loss: 0.1477\n",
      "Epoch [35/35], Step [1100/1263], Loss: 0.1406\n",
      "Epoch [35/35], Step [1200/1263], Loss: 0.1490\n",
      "Final model saved at models\\8_Group17_DLProject.pth\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Evaluate the Model"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we evaluate the trained model using the validation dataset:\n",
    "- The model is set to evaluation mode, and gradient computation is disabled.\n",
    "- For each batch, we perform a forward pass and predict the class labels.\n",
    "- Ground truth labels and predictions are stored and used to generate a classification report using `sklearn`. This report provides precision, recall, and F1-scores for each emotion class."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dboWdi-7O1IS",
    "ExecuteTime": {
     "end_time": "2024-12-17T13:45:48.371630Z",
     "start_time": "2024-12-17T13:45:33.563054Z"
    }
   },
   "source": [
    "model = CNN()\n",
    "model = model.to(device)\n",
    "model_path = 'models/8_Group17_DLProject.pth'\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "# Initialize lists to store ground truth and predictions\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "# Disable gradient computation\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        # Move inputs and labels to the same device as the model\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get the predicted class\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Append ground truth and predictions to respective lists\n",
    "        y_true.extend(labels.cpu().numpy())  # Convert tensors to numpy\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Generate the classification report\n",
    "emotion_labels = {0:'Angry', 1:'Disgust', 2:'Fear', 3:'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n",
    "print(classification_report(y_true, y_pred, target_names=list(emotion_labels.values())))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.52      0.52      0.52       799\n",
      "     Disgust       0.66      0.54      0.59        87\n",
      "        Fear       0.51      0.45      0.47       820\n",
      "       Happy       0.78      0.82      0.80      1443\n",
      "         Sad       0.43      0.60      0.50       966\n",
      "    Surprise       0.79      0.72      0.76       634\n",
      "     Neutral       0.60      0.45      0.52       993\n",
      "\n",
      "    accuracy                           0.61      5742\n",
      "   macro avg       0.61      0.58      0.59      5742\n",
      "weighted avg       0.62      0.61      0.61      5742\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The results of this experiment show that using six convolutional layers with dilated convolutions and average pooling achieved an accuracy of 0.61. This performance is comparable to previous models with fewer layers and without dilated convolutions, suggesting that the additional complexity may not significantly improve accuracy for this specific emotion recognition task."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
