{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Notebook description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to our baseline, this notebook uses only **four convolutional layers**. No other changes were made to any other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing libraries and loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import classification_report\n",
    "from utils.preprocessing import load_data, ReshapeAndScale, create_dataloaders\n",
    "from utils.fer2013_dataset import Fer2013Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the data into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "                                              pixels  emotion\n",
      "0  1 30 98 117 115 86 44 43 65 120 128 99 85 82 7...        0\n",
      "1  43 40 31 28 26 28 27 24 23 28 33 33 37 39 39 3...        3\n",
      "2  122 95 76 73 77 86 70 114 92 55 66 58 65 58 77...        6\n",
      "3  255 255 254 255 178 185 202 195 194 191 190 19...        1\n",
      "4  130 133 132 135 60 17 34 35 40 45 49 57 62 66 ...        6\n",
      "5  38 44 40 44 39 37 41 40 42 45 44 39 44 50 57 6...        4\n",
      "6  59 60 59 90 84 87 116 135 144 158 170 176 176 ...        4\n",
      "7  155 134 154 129 139 160 153 140 127 116 108 11...        0\n",
      "8  36 17 8 13 12 10 12 14 14 14 13 14 34 80 139 1...        1\n",
      "9  44 50 65 83 92 100 109 124 134 140 141 145 145...        1\n",
      "\n",
      "\n",
      "Validation Data\n",
      "   emotion                                             pixels\n",
      "0        3  254 253 253 254 254 254 252 252 253 252 253 25...\n",
      "1        4  92 70 64 65 62 64 91 139 167 181 186 187 191 1...\n",
      "2        5  20 19 16 23 24 25 34 24 25 60 113 123 139 149 ...\n",
      "3        4  79 82 83 84 85 89 92 90 93 95 94 97 95 97 93 8...\n",
      "4        2  117 105 95 73 72 65 49 66 92 105 126 154 177 1...\n",
      "5        3  138 138 138 118 129 146 138 120 89 57 50 47 54...\n",
      "6        5  141 132 104 64 37 27 24 23 19 18 19 20 23 22 2...\n",
      "7        3  17 23 19 14 31 27 28 41 52 69 88 96 106 108 10...\n",
      "8        6  117 110 91 49 44 46 47 41 26 39 57 71 83 91 10...\n",
      "9        3  89 141 163 153 136 128 127 125 121 116 30 23 2...\n"
     ]
    }
   ],
   "source": [
    "train_df = load_data(\"data/oversampled_train.csv\")\n",
    "val_df = load_data(\"data/validation.csv\")\n",
    "\n",
    "print(\"Training Data\")\n",
    "print(train_df.head(10))\n",
    "\n",
    "print(\"\\n\\nValidation Data\")\n",
    "print(val_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (40404, 2)\n",
      "Validation data shape (5742, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data shape:\", train_df.shape)\n",
    "print(\"Validation data shape\", val_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHpwVsoYO1IH"
   },
   "source": [
    "## 2. Define a custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a custom PyTorch dataset class, `Fer2013Dataset`, for handling the FER2013 data. The dataset is designed to load images (stored as pixel strings) and their corresponding emotion labels. It also supports optional transformations to preprocess the images during training. This setup makes it easy to integrate the dataset with PyTorch DataLoaders.\n",
    "\n",
    "The class is contained in the `utils/fer2013_dataset` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also apply the following preprocessing steps to convert the data into the desired format we can further work with:\n",
    "1. **Reshaping**:\n",
    "   - Convert the pixel string into a `48x48` matrix for visualization and processing.\n",
    "2. **Scaling**:\n",
    "   - Scale pixel values to the range `[0, 1]` by dividing the pixel values by 255.\n",
    "3. **Normalization**:\n",
    "   - Normalize pixel values to the range `[-1, 1]` by subtracting the mean and diving them by the standard deviation.\n",
    "\n",
    "\n",
    "These preprocessing steps are contained in the class `ReshapeAndScale`, which is available under path `utils/preprocessing.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a default transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    ReshapeAndScale(n_rows=48, n_cols=48),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8cuhkZCOO1II"
   },
   "outputs": [],
   "source": [
    "train_dataset = Fer2013Dataset(train_df, train_df['emotion'], transform=transform)\n",
    "val_dataset = Fer2013Dataset(val_df, val_df['emotion'], transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders\n",
    "\n",
    "We create DataLoaders for both subsets to enable batch processing. The training DataLoader shuffles the data for better learning, while the validation DataLoader does not. Finally, we print the shapes of the batches to verify that everything works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "rAQ30-FQO1IJ",
    "outputId": "aae71765-1a27-4d1a-9fe0-d34cee950122"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader, val_loader = create_dataloaders(train_dataset, val_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pge_vvuYO1IN"
   },
   "source": [
    "## 3. Define the CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a custom Convolutional Neural Network (CNN) for emotion recognition. The model includes multiple convolutional layers with batch normalization, dropout for regularization, max pooling for downsampling, and fully connected layers for classification. \n",
    "\n",
    "The network dynamically calculates the flattened size needed for the fully connected layers based on the input size (48x48 grayscale images). Finally, we instantiate the model, move it to the available device (CPU or GPU), and print its architecture for verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvFxqkzxO1IO",
    "outputId": "a7824d62-eb91-42c3-8f2f-def089a2ea5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout3): Dropout(p=0.25, inplace=False)\n",
      "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=12800, out_features=250, bias=True)\n",
      "  (dropout_fc): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=250, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # 1st Conv Layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding='valid')\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "\n",
    "        # 2nd Conv Layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 3rd Conv Layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding='valid')\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "\n",
    "        # 4th Conv Layer\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding='same')\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Calculate the flatten size dynamically\n",
    "        self.flatten_size = self._get_flatten_size()\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(self.flatten_size, 250)\n",
    "        self.dropout_fc = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(250, 7)  # 7 classes for emotion recognition\n",
    "\n",
    "    def _get_flatten_size(self):\n",
    "        # Create a dummy tensor with the same size as input image\n",
    "        dummy_input = torch.zeros(1, 1, 48, 48)  # batch_size, channels, height, width\n",
    "        dummy_output = self._forward_conv_layers(dummy_input)\n",
    "        return dummy_output.numel()\n",
    "\n",
    "    def _forward_conv_layers(self, x):\n",
    "        # Pass through convolutional layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use the convolutional layers defined in _forward_conv_layers\n",
    "        x = self._forward_conv_layers(x)\n",
    "\n",
    "        # Dynamically flatten the output\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN()\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model to verify layers\n",
    "print(model)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAlA86MtO1IQ"
   },
   "source": [
    "## 4. Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we define the loss function and optimizer for training the model:\n",
    "- **Loss Function**: `CrossEntropyLoss` is used, which is well-suited for multi-class classification tasks like emotion recognition.\n",
    "- **Optimizer**: The Adam optimizer is initialized with a learning rate of `0.0001` to update the model parameters during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WFyRGyfXO1IR"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # call optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMHZrQsZO1IR"
   },
   "source": [
    "## 5. Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we define the training loop for the CNN:\n",
    "- **Number of Epochs**: The model is trained for 35 epochs.\n",
    "- **Training Process**:\n",
    "  - The model is set to training mode.\n",
    "  - For each batch, we move inputs and labels to the appropriate device, clear the gradients, perform forward and backward passes, and update the model's parameters using the optimizer.\n",
    "  - The running loss is tracked and printed every 100 batches for monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/35], Step [100/1263], Loss: 1.6817\n",
      "Epoch [1/35], Step [200/1263], Loss: 1.6687\n",
      "Epoch [1/35], Step [300/1263], Loss: 1.6244\n",
      "Epoch [1/35], Step [400/1263], Loss: 1.6482\n",
      "Epoch [1/35], Step [500/1263], Loss: 1.6324\n",
      "Epoch [1/35], Step [600/1263], Loss: 1.6196\n",
      "Epoch [1/35], Step [700/1263], Loss: 1.6092\n",
      "Epoch [1/35], Step [800/1263], Loss: 1.5840\n",
      "Epoch [1/35], Step [900/1263], Loss: 1.5944\n",
      "Epoch [1/35], Step [1000/1263], Loss: 1.5720\n",
      "Epoch [1/35], Step [1100/1263], Loss: 1.5881\n",
      "Epoch [1/35], Step [1200/1263], Loss: 1.5589\n",
      "Epoch [2/35], Step [100/1263], Loss: 1.5371\n",
      "Epoch [2/35], Step [200/1263], Loss: 1.5356\n",
      "Epoch [2/35], Step [300/1263], Loss: 1.5208\n",
      "Epoch [2/35], Step [400/1263], Loss: 1.5076\n",
      "Epoch [2/35], Step [500/1263], Loss: 1.4989\n",
      "Epoch [2/35], Step [600/1263], Loss: 1.4992\n",
      "Epoch [2/35], Step [700/1263], Loss: 1.4962\n",
      "Epoch [2/35], Step [800/1263], Loss: 1.5074\n",
      "Epoch [2/35], Step [900/1263], Loss: 1.4779\n",
      "Epoch [2/35], Step [1000/1263], Loss: 1.4767\n",
      "Epoch [2/35], Step [1100/1263], Loss: 1.4709\n",
      "Epoch [2/35], Step [1200/1263], Loss: 1.4469\n",
      "Epoch [3/35], Step [100/1263], Loss: 1.4474\n",
      "Epoch [3/35], Step [200/1263], Loss: 1.4493\n",
      "Epoch [3/35], Step [300/1263], Loss: 1.4473\n",
      "Epoch [3/35], Step [400/1263], Loss: 1.4394\n",
      "Epoch [3/35], Step [500/1263], Loss: 1.4532\n",
      "Epoch [3/35], Step [600/1263], Loss: 1.4393\n",
      "Epoch [3/35], Step [700/1263], Loss: 1.4004\n",
      "Epoch [3/35], Step [800/1263], Loss: 1.4057\n",
      "Epoch [3/35], Step [900/1263], Loss: 1.3866\n",
      "Epoch [3/35], Step [1000/1263], Loss: 1.3844\n",
      "Epoch [3/35], Step [1100/1263], Loss: 1.4020\n",
      "Epoch [3/35], Step [1200/1263], Loss: 1.3692\n",
      "Epoch [4/35], Step [100/1263], Loss: 1.3863\n",
      "Epoch [4/35], Step [200/1263], Loss: 1.3809\n",
      "Epoch [4/35], Step [300/1263], Loss: 1.3747\n",
      "Epoch [4/35], Step [400/1263], Loss: 1.3621\n",
      "Epoch [4/35], Step [500/1263], Loss: 1.3696\n",
      "Epoch [4/35], Step [600/1263], Loss: 1.3638\n",
      "Epoch [4/35], Step [700/1263], Loss: 1.3666\n",
      "Epoch [4/35], Step [800/1263], Loss: 1.3728\n",
      "Epoch [4/35], Step [900/1263], Loss: 1.3527\n",
      "Epoch [4/35], Step [1000/1263], Loss: 1.3507\n",
      "Epoch [4/35], Step [1100/1263], Loss: 1.3277\n",
      "Epoch [4/35], Step [1200/1263], Loss: 1.3447\n",
      "Epoch [5/35], Step [100/1263], Loss: 1.3262\n",
      "Epoch [5/35], Step [200/1263], Loss: 1.3156\n",
      "Epoch [5/35], Step [300/1263], Loss: 1.3132\n",
      "Epoch [5/35], Step [400/1263], Loss: 1.3544\n",
      "Epoch [5/35], Step [500/1263], Loss: 1.3229\n",
      "Epoch [5/35], Step [600/1263], Loss: 1.3101\n",
      "Epoch [5/35], Step [700/1263], Loss: 1.2838\n",
      "Epoch [5/35], Step [800/1263], Loss: 1.3120\n",
      "Epoch [5/35], Step [900/1263], Loss: 1.3087\n",
      "Epoch [5/35], Step [1000/1263], Loss: 1.3133\n",
      "Epoch [5/35], Step [1100/1263], Loss: 1.3264\n",
      "Epoch [5/35], Step [1200/1263], Loss: 1.3051\n",
      "Epoch [6/35], Step [100/1263], Loss: 1.2659\n",
      "Epoch [6/35], Step [200/1263], Loss: 1.2937\n",
      "Epoch [6/35], Step [300/1263], Loss: 1.2729\n",
      "Epoch [6/35], Step [400/1263], Loss: 1.3051\n",
      "Epoch [6/35], Step [500/1263], Loss: 1.2891\n",
      "Epoch [6/35], Step [600/1263], Loss: 1.3003\n",
      "Epoch [6/35], Step [700/1263], Loss: 1.2512\n",
      "Epoch [6/35], Step [800/1263], Loss: 1.2385\n",
      "Epoch [6/35], Step [900/1263], Loss: 1.2596\n",
      "Epoch [6/35], Step [1000/1263], Loss: 1.2801\n",
      "Epoch [6/35], Step [1100/1263], Loss: 1.2697\n",
      "Epoch [6/35], Step [1200/1263], Loss: 1.2685\n",
      "Epoch [7/35], Step [100/1263], Loss: 1.2408\n",
      "Epoch [7/35], Step [200/1263], Loss: 1.2319\n",
      "Epoch [7/35], Step [300/1263], Loss: 1.2559\n",
      "Epoch [7/35], Step [400/1263], Loss: 1.2422\n",
      "Epoch [7/35], Step [500/1263], Loss: 1.2296\n",
      "Epoch [7/35], Step [600/1263], Loss: 1.2440\n",
      "Epoch [7/35], Step [700/1263], Loss: 1.2581\n",
      "Epoch [7/35], Step [800/1263], Loss: 1.2395\n",
      "Epoch [7/35], Step [900/1263], Loss: 1.2292\n",
      "Epoch [7/35], Step [1000/1263], Loss: 1.2391\n",
      "Epoch [7/35], Step [1100/1263], Loss: 1.2581\n",
      "Epoch [7/35], Step [1200/1263], Loss: 1.2369\n",
      "Epoch [8/35], Step [100/1263], Loss: 1.2056\n",
      "Epoch [8/35], Step [200/1263], Loss: 1.2031\n",
      "Epoch [8/35], Step [300/1263], Loss: 1.2368\n",
      "Epoch [8/35], Step [400/1263], Loss: 1.1863\n",
      "Epoch [8/35], Step [500/1263], Loss: 1.2241\n",
      "Epoch [8/35], Step [600/1263], Loss: 1.2027\n",
      "Epoch [8/35], Step [700/1263], Loss: 1.2244\n",
      "Epoch [8/35], Step [800/1263], Loss: 1.2072\n",
      "Epoch [8/35], Step [900/1263], Loss: 1.2036\n",
      "Epoch [8/35], Step [1000/1263], Loss: 1.2164\n",
      "Epoch [8/35], Step [1100/1263], Loss: 1.2326\n",
      "Epoch [8/35], Step [1200/1263], Loss: 1.2081\n",
      "Epoch [9/35], Step [100/1263], Loss: 1.1936\n",
      "Epoch [9/35], Step [200/1263], Loss: 1.2001\n",
      "Epoch [9/35], Step [300/1263], Loss: 1.1974\n",
      "Epoch [9/35], Step [400/1263], Loss: 1.1546\n",
      "Epoch [9/35], Step [500/1263], Loss: 1.1926\n",
      "Epoch [9/35], Step [600/1263], Loss: 1.1980\n",
      "Epoch [9/35], Step [700/1263], Loss: 1.1901\n",
      "Epoch [9/35], Step [800/1263], Loss: 1.1977\n",
      "Epoch [9/35], Step [900/1263], Loss: 1.1973\n",
      "Epoch [9/35], Step [1000/1263], Loss: 1.1862\n",
      "Epoch [9/35], Step [1100/1263], Loss: 1.1753\n",
      "Epoch [9/35], Step [1200/1263], Loss: 1.2008\n",
      "Epoch [10/35], Step [100/1263], Loss: 1.1662\n",
      "Epoch [10/35], Step [200/1263], Loss: 1.1457\n",
      "Epoch [10/35], Step [300/1263], Loss: 1.1602\n",
      "Epoch [10/35], Step [400/1263], Loss: 1.1473\n",
      "Epoch [10/35], Step [500/1263], Loss: 1.1944\n",
      "Epoch [10/35], Step [600/1263], Loss: 1.1678\n",
      "Epoch [10/35], Step [700/1263], Loss: 1.1752\n",
      "Epoch [10/35], Step [800/1263], Loss: 1.1986\n",
      "Epoch [10/35], Step [900/1263], Loss: 1.2060\n",
      "Epoch [10/35], Step [1000/1263], Loss: 1.1532\n",
      "Epoch [10/35], Step [1100/1263], Loss: 1.1603\n",
      "Epoch [10/35], Step [1200/1263], Loss: 1.1668\n",
      "Checkpoint saved at epoch 10\n",
      "Epoch [11/35], Step [100/1263], Loss: 1.1189\n",
      "Epoch [11/35], Step [200/1263], Loss: 1.1222\n",
      "Epoch [11/35], Step [300/1263], Loss: 1.1808\n",
      "Epoch [11/35], Step [400/1263], Loss: 1.1465\n",
      "Epoch [11/35], Step [500/1263], Loss: 1.1559\n",
      "Epoch [11/35], Step [600/1263], Loss: 1.1303\n",
      "Epoch [11/35], Step [700/1263], Loss: 1.1491\n",
      "Epoch [11/35], Step [800/1263], Loss: 1.1636\n",
      "Epoch [11/35], Step [900/1263], Loss: 1.1250\n",
      "Epoch [11/35], Step [1000/1263], Loss: 1.1438\n",
      "Epoch [11/35], Step [1100/1263], Loss: 1.1343\n",
      "Epoch [11/35], Step [1200/1263], Loss: 1.1351\n",
      "Epoch [12/35], Step [100/1263], Loss: 1.1189\n",
      "Epoch [12/35], Step [200/1263], Loss: 1.1231\n",
      "Epoch [12/35], Step [300/1263], Loss: 1.1496\n",
      "Epoch [12/35], Step [400/1263], Loss: 1.1370\n",
      "Epoch [12/35], Step [500/1263], Loss: 1.1219\n",
      "Epoch [12/35], Step [600/1263], Loss: 1.1172\n",
      "Epoch [12/35], Step [700/1263], Loss: 1.1578\n",
      "Epoch [12/35], Step [800/1263], Loss: 1.1352\n",
      "Epoch [12/35], Step [900/1263], Loss: 1.1172\n",
      "Epoch [12/35], Step [1000/1263], Loss: 1.0918\n",
      "Epoch [12/35], Step [1100/1263], Loss: 1.1291\n",
      "Epoch [12/35], Step [1200/1263], Loss: 1.1132\n",
      "Epoch [13/35], Step [100/1263], Loss: 1.1030\n",
      "Epoch [13/35], Step [200/1263], Loss: 1.0813\n",
      "Epoch [13/35], Step [300/1263], Loss: 1.1032\n",
      "Epoch [13/35], Step [400/1263], Loss: 1.0927\n",
      "Epoch [13/35], Step [500/1263], Loss: 1.1055\n",
      "Epoch [13/35], Step [600/1263], Loss: 1.1530\n",
      "Epoch [13/35], Step [700/1263], Loss: 1.0870\n",
      "Epoch [13/35], Step [800/1263], Loss: 1.0931\n",
      "Epoch [13/35], Step [900/1263], Loss: 1.1107\n",
      "Epoch [13/35], Step [1000/1263], Loss: 1.0956\n",
      "Epoch [13/35], Step [1100/1263], Loss: 1.1196\n",
      "Epoch [13/35], Step [1200/1263], Loss: 1.1320\n",
      "Epoch [14/35], Step [100/1263], Loss: 1.0876\n",
      "Epoch [14/35], Step [200/1263], Loss: 1.0657\n",
      "Epoch [14/35], Step [300/1263], Loss: 1.0909\n",
      "Epoch [14/35], Step [400/1263], Loss: 1.0874\n",
      "Epoch [14/35], Step [500/1263], Loss: 1.0795\n",
      "Epoch [14/35], Step [600/1263], Loss: 1.1210\n",
      "Epoch [14/35], Step [700/1263], Loss: 1.0645\n",
      "Epoch [14/35], Step [800/1263], Loss: 1.0638\n",
      "Epoch [14/35], Step [900/1263], Loss: 1.0955\n",
      "Epoch [14/35], Step [1000/1263], Loss: 1.0837\n",
      "Epoch [14/35], Step [1100/1263], Loss: 1.1084\n",
      "Epoch [14/35], Step [1200/1263], Loss: 1.1164\n",
      "Epoch [15/35], Step [100/1263], Loss: 1.0813\n",
      "Epoch [15/35], Step [200/1263], Loss: 1.0777\n",
      "Epoch [15/35], Step [300/1263], Loss: 1.0906\n",
      "Epoch [15/35], Step [400/1263], Loss: 1.0681\n",
      "Epoch [15/35], Step [500/1263], Loss: 1.0852\n",
      "Epoch [15/35], Step [600/1263], Loss: 1.0595\n",
      "Epoch [15/35], Step [700/1263], Loss: 1.0629\n",
      "Epoch [15/35], Step [800/1263], Loss: 1.0527\n",
      "Epoch [15/35], Step [900/1263], Loss: 1.0664\n",
      "Epoch [15/35], Step [1000/1263], Loss: 1.0737\n",
      "Epoch [15/35], Step [1100/1263], Loss: 1.0615\n",
      "Epoch [15/35], Step [1200/1263], Loss: 1.0641\n",
      "Epoch [16/35], Step [100/1263], Loss: 1.0387\n",
      "Epoch [16/35], Step [200/1263], Loss: 1.0634\n",
      "Epoch [16/35], Step [300/1263], Loss: 1.0601\n",
      "Epoch [16/35], Step [400/1263], Loss: 1.0546\n",
      "Epoch [16/35], Step [500/1263], Loss: 1.0591\n",
      "Epoch [16/35], Step [600/1263], Loss: 1.0780\n",
      "Epoch [16/35], Step [700/1263], Loss: 1.0613\n",
      "Epoch [16/35], Step [800/1263], Loss: 1.0560\n",
      "Epoch [16/35], Step [900/1263], Loss: 1.0545\n",
      "Epoch [16/35], Step [1000/1263], Loss: 1.0391\n",
      "Epoch [16/35], Step [1100/1263], Loss: 1.0499\n",
      "Epoch [16/35], Step [1200/1263], Loss: 1.0462\n",
      "Epoch [17/35], Step [100/1263], Loss: 1.0147\n",
      "Epoch [17/35], Step [200/1263], Loss: 1.0374\n",
      "Epoch [17/35], Step [300/1263], Loss: 1.0389\n",
      "Epoch [17/35], Step [400/1263], Loss: 1.0351\n",
      "Epoch [17/35], Step [500/1263], Loss: 1.0328\n",
      "Epoch [17/35], Step [600/1263], Loss: 1.0326\n",
      "Epoch [17/35], Step [700/1263], Loss: 1.0182\n",
      "Epoch [17/35], Step [800/1263], Loss: 1.0461\n",
      "Epoch [17/35], Step [900/1263], Loss: 1.0485\n",
      "Epoch [17/35], Step [1000/1263], Loss: 1.0488\n",
      "Epoch [17/35], Step [1100/1263], Loss: 1.0495\n",
      "Epoch [17/35], Step [1200/1263], Loss: 1.0683\n",
      "Epoch [18/35], Step [100/1263], Loss: 1.0121\n",
      "Epoch [18/35], Step [200/1263], Loss: 1.0305\n",
      "Epoch [18/35], Step [300/1263], Loss: 1.0271\n",
      "Epoch [18/35], Step [400/1263], Loss: 1.0641\n",
      "Epoch [18/35], Step [500/1263], Loss: 1.0061\n",
      "Epoch [18/35], Step [600/1263], Loss: 1.0047\n",
      "Epoch [18/35], Step [700/1263], Loss: 1.0307\n",
      "Epoch [18/35], Step [800/1263], Loss: 1.0361\n",
      "Epoch [18/35], Step [900/1263], Loss: 1.0367\n",
      "Epoch [18/35], Step [1000/1263], Loss: 1.0383\n",
      "Epoch [18/35], Step [1100/1263], Loss: 1.0089\n",
      "Epoch [18/35], Step [1200/1263], Loss: 1.0447\n",
      "Epoch [19/35], Step [100/1263], Loss: 0.9945\n",
      "Epoch [19/35], Step [200/1263], Loss: 1.0220\n",
      "Epoch [19/35], Step [300/1263], Loss: 1.0215\n",
      "Epoch [19/35], Step [400/1263], Loss: 0.9686\n",
      "Epoch [19/35], Step [500/1263], Loss: 1.0166\n",
      "Epoch [19/35], Step [600/1263], Loss: 1.0037\n",
      "Epoch [19/35], Step [700/1263], Loss: 1.0272\n",
      "Epoch [19/35], Step [800/1263], Loss: 1.0325\n",
      "Epoch [19/35], Step [900/1263], Loss: 1.0189\n",
      "Epoch [19/35], Step [1000/1263], Loss: 1.0202\n",
      "Epoch [19/35], Step [1100/1263], Loss: 1.0096\n",
      "Epoch [19/35], Step [1200/1263], Loss: 1.0207\n",
      "Epoch [20/35], Step [100/1263], Loss: 0.9957\n",
      "Epoch [20/35], Step [200/1263], Loss: 1.0162\n",
      "Epoch [20/35], Step [300/1263], Loss: 1.0192\n",
      "Epoch [20/35], Step [400/1263], Loss: 1.0103\n",
      "Epoch [20/35], Step [500/1263], Loss: 0.9733\n",
      "Epoch [20/35], Step [600/1263], Loss: 1.0165\n",
      "Epoch [20/35], Step [700/1263], Loss: 1.0197\n",
      "Epoch [20/35], Step [800/1263], Loss: 1.0061\n",
      "Epoch [20/35], Step [900/1263], Loss: 0.9849\n",
      "Epoch [20/35], Step [1000/1263], Loss: 1.0147\n",
      "Epoch [20/35], Step [1100/1263], Loss: 0.9644\n",
      "Epoch [20/35], Step [1200/1263], Loss: 1.0202\n",
      "Checkpoint saved at epoch 20\n",
      "Epoch [21/35], Step [100/1263], Loss: 0.9857\n",
      "Epoch [21/35], Step [200/1263], Loss: 0.9687\n",
      "Epoch [21/35], Step [300/1263], Loss: 0.9875\n",
      "Epoch [21/35], Step [400/1263], Loss: 1.0040\n",
      "Epoch [21/35], Step [500/1263], Loss: 0.9720\n",
      "Epoch [21/35], Step [600/1263], Loss: 0.9719\n",
      "Epoch [21/35], Step [700/1263], Loss: 0.9816\n",
      "Epoch [21/35], Step [800/1263], Loss: 0.9778\n",
      "Epoch [21/35], Step [900/1263], Loss: 0.9849\n",
      "Epoch [21/35], Step [1000/1263], Loss: 1.0391\n",
      "Epoch [21/35], Step [1100/1263], Loss: 1.0096\n",
      "Epoch [21/35], Step [1200/1263], Loss: 0.9839\n",
      "Epoch [22/35], Step [100/1263], Loss: 0.9523\n",
      "Epoch [22/35], Step [200/1263], Loss: 0.9709\n",
      "Epoch [22/35], Step [300/1263], Loss: 0.9751\n",
      "Epoch [22/35], Step [400/1263], Loss: 1.0127\n",
      "Epoch [22/35], Step [500/1263], Loss: 0.9631\n",
      "Epoch [22/35], Step [600/1263], Loss: 0.9853\n",
      "Epoch [22/35], Step [700/1263], Loss: 0.9712\n",
      "Epoch [22/35], Step [800/1263], Loss: 0.9772\n",
      "Epoch [22/35], Step [900/1263], Loss: 0.9655\n",
      "Epoch [22/35], Step [1000/1263], Loss: 0.9873\n",
      "Epoch [22/35], Step [1100/1263], Loss: 0.9847\n",
      "Epoch [22/35], Step [1200/1263], Loss: 0.9671\n",
      "Epoch [23/35], Step [100/1263], Loss: 0.9529\n",
      "Epoch [23/35], Step [200/1263], Loss: 0.9771\n",
      "Epoch [23/35], Step [300/1263], Loss: 0.9901\n",
      "Epoch [23/35], Step [400/1263], Loss: 0.9550\n",
      "Epoch [23/35], Step [500/1263], Loss: 0.9772\n",
      "Epoch [23/35], Step [600/1263], Loss: 0.9711\n",
      "Epoch [23/35], Step [700/1263], Loss: 0.9575\n",
      "Epoch [23/35], Step [800/1263], Loss: 0.9492\n",
      "Epoch [23/35], Step [900/1263], Loss: 0.9578\n",
      "Epoch [23/35], Step [1000/1263], Loss: 0.9563\n",
      "Epoch [23/35], Step [1100/1263], Loss: 0.9751\n",
      "Epoch [23/35], Step [1200/1263], Loss: 0.9661\n",
      "Epoch [24/35], Step [100/1263], Loss: 0.9376\n",
      "Epoch [24/35], Step [200/1263], Loss: 0.9540\n",
      "Epoch [24/35], Step [300/1263], Loss: 0.9563\n",
      "Epoch [24/35], Step [400/1263], Loss: 0.9420\n",
      "Epoch [24/35], Step [500/1263], Loss: 0.9445\n",
      "Epoch [24/35], Step [600/1263], Loss: 0.9491\n",
      "Epoch [24/35], Step [700/1263], Loss: 0.9648\n",
      "Epoch [24/35], Step [800/1263], Loss: 0.9250\n",
      "Epoch [24/35], Step [900/1263], Loss: 0.9270\n",
      "Epoch [24/35], Step [1000/1263], Loss: 0.9264\n",
      "Epoch [24/35], Step [1100/1263], Loss: 0.9643\n",
      "Epoch [24/35], Step [1200/1263], Loss: 0.9728\n",
      "Epoch [25/35], Step [100/1263], Loss: 0.9138\n",
      "Epoch [25/35], Step [200/1263], Loss: 0.9230\n",
      "Epoch [25/35], Step [300/1263], Loss: 0.9203\n",
      "Epoch [25/35], Step [400/1263], Loss: 0.9608\n",
      "Epoch [25/35], Step [500/1263], Loss: 0.9592\n",
      "Epoch [25/35], Step [600/1263], Loss: 0.9536\n",
      "Epoch [25/35], Step [700/1263], Loss: 0.9352\n",
      "Epoch [25/35], Step [800/1263], Loss: 0.9487\n",
      "Epoch [25/35], Step [900/1263], Loss: 0.9433\n",
      "Epoch [25/35], Step [1000/1263], Loss: 0.9661\n",
      "Epoch [25/35], Step [1100/1263], Loss: 0.9404\n",
      "Epoch [25/35], Step [1200/1263], Loss: 0.9519\n",
      "Epoch [26/35], Step [100/1263], Loss: 0.9469\n",
      "Epoch [26/35], Step [200/1263], Loss: 0.9185\n",
      "Epoch [26/35], Step [300/1263], Loss: 0.9374\n",
      "Epoch [26/35], Step [400/1263], Loss: 0.9140\n",
      "Epoch [26/35], Step [500/1263], Loss: 0.9445\n",
      "Epoch [26/35], Step [600/1263], Loss: 0.9405\n",
      "Epoch [26/35], Step [700/1263], Loss: 0.9484\n",
      "Epoch [26/35], Step [800/1263], Loss: 0.9126\n",
      "Epoch [26/35], Step [900/1263], Loss: 0.9309\n",
      "Epoch [26/35], Step [1000/1263], Loss: 0.9499\n",
      "Epoch [26/35], Step [1100/1263], Loss: 0.9117\n",
      "Epoch [26/35], Step [1200/1263], Loss: 0.9170\n",
      "Epoch [27/35], Step [100/1263], Loss: 0.8886\n",
      "Epoch [27/35], Step [200/1263], Loss: 0.9342\n",
      "Epoch [27/35], Step [300/1263], Loss: 0.9002\n",
      "Epoch [27/35], Step [400/1263], Loss: 0.8985\n",
      "Epoch [27/35], Step [500/1263], Loss: 0.9291\n",
      "Epoch [27/35], Step [600/1263], Loss: 0.9155\n",
      "Epoch [27/35], Step [700/1263], Loss: 0.9183\n",
      "Epoch [27/35], Step [800/1263], Loss: 0.9424\n",
      "Epoch [27/35], Step [900/1263], Loss: 0.9177\n",
      "Epoch [27/35], Step [1000/1263], Loss: 0.9239\n",
      "Epoch [27/35], Step [1100/1263], Loss: 0.9048\n",
      "Epoch [27/35], Step [1200/1263], Loss: 0.9319\n",
      "Epoch [28/35], Step [100/1263], Loss: 0.8766\n",
      "Epoch [28/35], Step [200/1263], Loss: 0.9034\n",
      "Epoch [28/35], Step [300/1263], Loss: 0.9261\n",
      "Epoch [28/35], Step [400/1263], Loss: 0.9213\n",
      "Epoch [28/35], Step [500/1263], Loss: 0.9038\n",
      "Epoch [28/35], Step [600/1263], Loss: 0.8847\n",
      "Epoch [28/35], Step [700/1263], Loss: 0.9119\n",
      "Epoch [28/35], Step [800/1263], Loss: 0.9003\n",
      "Epoch [28/35], Step [900/1263], Loss: 0.9018\n",
      "Epoch [28/35], Step [1000/1263], Loss: 0.9272\n",
      "Epoch [28/35], Step [1100/1263], Loss: 0.9131\n",
      "Epoch [28/35], Step [1200/1263], Loss: 0.9222\n",
      "Epoch [29/35], Step [100/1263], Loss: 0.8880\n",
      "Epoch [29/35], Step [200/1263], Loss: 0.8676\n",
      "Epoch [29/35], Step [300/1263], Loss: 0.9315\n",
      "Epoch [29/35], Step [400/1263], Loss: 0.8875\n",
      "Epoch [29/35], Step [500/1263], Loss: 0.8942\n",
      "Epoch [29/35], Step [600/1263], Loss: 0.8835\n",
      "Epoch [29/35], Step [700/1263], Loss: 0.9128\n",
      "Epoch [29/35], Step [800/1263], Loss: 0.8777\n",
      "Epoch [29/35], Step [900/1263], Loss: 0.8914\n",
      "Epoch [29/35], Step [1000/1263], Loss: 0.8937\n",
      "Epoch [29/35], Step [1100/1263], Loss: 0.8824\n",
      "Epoch [29/35], Step [1200/1263], Loss: 0.8884\n",
      "Epoch [30/35], Step [100/1263], Loss: 0.8797\n",
      "Epoch [30/35], Step [200/1263], Loss: 0.8449\n",
      "Epoch [30/35], Step [300/1263], Loss: 0.8900\n",
      "Epoch [30/35], Step [400/1263], Loss: 0.8660\n",
      "Epoch [30/35], Step [500/1263], Loss: 0.8956\n",
      "Epoch [30/35], Step [600/1263], Loss: 0.8894\n",
      "Epoch [30/35], Step [700/1263], Loss: 0.9045\n",
      "Epoch [30/35], Step [800/1263], Loss: 0.8884\n",
      "Epoch [30/35], Step [900/1263], Loss: 0.8795\n",
      "Epoch [30/35], Step [1000/1263], Loss: 0.8960\n",
      "Epoch [30/35], Step [1100/1263], Loss: 0.8991\n",
      "Epoch [30/35], Step [1200/1263], Loss: 0.9138\n",
      "Checkpoint saved at epoch 30\n",
      "Epoch [31/35], Step [100/1263], Loss: 0.8666\n",
      "Epoch [31/35], Step [200/1263], Loss: 0.8668\n",
      "Epoch [31/35], Step [300/1263], Loss: 0.9043\n",
      "Epoch [31/35], Step [400/1263], Loss: 0.8937\n",
      "Epoch [31/35], Step [500/1263], Loss: 0.8739\n",
      "Epoch [31/35], Step [600/1263], Loss: 0.8835\n",
      "Epoch [31/35], Step [700/1263], Loss: 0.8608\n",
      "Epoch [31/35], Step [800/1263], Loss: 0.8739\n",
      "Epoch [31/35], Step [900/1263], Loss: 0.8778\n",
      "Epoch [31/35], Step [1000/1263], Loss: 0.8878\n",
      "Epoch [31/35], Step [1100/1263], Loss: 0.8746\n",
      "Epoch [31/35], Step [1200/1263], Loss: 0.8587\n",
      "Epoch [32/35], Step [100/1263], Loss: 0.8637\n",
      "Epoch [32/35], Step [200/1263], Loss: 0.8524\n",
      "Epoch [32/35], Step [300/1263], Loss: 0.8805\n",
      "Epoch [32/35], Step [400/1263], Loss: 0.8656\n",
      "Epoch [32/35], Step [500/1263], Loss: 0.8528\n",
      "Epoch [32/35], Step [600/1263], Loss: 0.8766\n",
      "Epoch [32/35], Step [700/1263], Loss: 0.8575\n",
      "Epoch [32/35], Step [800/1263], Loss: 0.8655\n",
      "Epoch [32/35], Step [900/1263], Loss: 0.8765\n",
      "Epoch [32/35], Step [1000/1263], Loss: 0.8595\n",
      "Epoch [32/35], Step [1100/1263], Loss: 0.8748\n",
      "Epoch [32/35], Step [1200/1263], Loss: 0.9083\n",
      "Epoch [33/35], Step [100/1263], Loss: 0.8495\n",
      "Epoch [33/35], Step [200/1263], Loss: 0.8360\n",
      "Epoch [33/35], Step [300/1263], Loss: 0.8356\n",
      "Epoch [33/35], Step [400/1263], Loss: 0.8584\n",
      "Epoch [33/35], Step [500/1263], Loss: 0.8795\n",
      "Epoch [33/35], Step [600/1263], Loss: 0.8454\n",
      "Epoch [33/35], Step [700/1263], Loss: 0.8566\n",
      "Epoch [33/35], Step [800/1263], Loss: 0.8575\n",
      "Epoch [33/35], Step [900/1263], Loss: 0.8588\n",
      "Epoch [33/35], Step [1000/1263], Loss: 0.8415\n",
      "Epoch [33/35], Step [1100/1263], Loss: 0.8897\n",
      "Epoch [33/35], Step [1200/1263], Loss: 0.8462\n",
      "Epoch [34/35], Step [100/1263], Loss: 0.8411\n",
      "Epoch [34/35], Step [200/1263], Loss: 0.8416\n",
      "Epoch [34/35], Step [300/1263], Loss: 0.8609\n",
      "Epoch [34/35], Step [400/1263], Loss: 0.8506\n",
      "Epoch [34/35], Step [500/1263], Loss: 0.8386\n",
      "Epoch [34/35], Step [600/1263], Loss: 0.8756\n",
      "Epoch [34/35], Step [700/1263], Loss: 0.8368\n",
      "Epoch [34/35], Step [800/1263], Loss: 0.8528\n",
      "Epoch [34/35], Step [900/1263], Loss: 0.8290\n",
      "Epoch [34/35], Step [1000/1263], Loss: 0.8735\n",
      "Epoch [34/35], Step [1100/1263], Loss: 0.8483\n",
      "Epoch [34/35], Step [1200/1263], Loss: 0.8618\n",
      "Epoch [35/35], Step [100/1263], Loss: 0.8314\n",
      "Epoch [35/35], Step [200/1263], Loss: 0.8332\n",
      "Epoch [35/35], Step [300/1263], Loss: 0.8276\n",
      "Epoch [35/35], Step [400/1263], Loss: 0.8245\n",
      "Epoch [35/35], Step [500/1263], Loss: 0.8585\n",
      "Epoch [35/35], Step [600/1263], Loss: 0.8442\n",
      "Epoch [35/35], Step [700/1263], Loss: 0.8367\n",
      "Epoch [35/35], Step [800/1263], Loss: 0.8530\n",
      "Epoch [35/35], Step [900/1263], Loss: 0.8558\n",
      "Epoch [35/35], Step [1000/1263], Loss: 0.8658\n",
      "Epoch [35/35], Step [1100/1263], Loss: 0.8531\n",
      "Epoch [35/35], Step [1200/1263], Loss: 0.8473\n",
      "Final model saved at models\\2_Group17_DLProject.pth\n"
     ]
    }
   ],
   "source": [
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir='tensorboard-runs/2_Group17_DLProject')\n",
    "\n",
    "checkpoint_path = 'model-checkpoints/2_Group17_DLProject'\n",
    "os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "model_save_path = 'models'\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "num_epochs = 35\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 100 == 0:  # Print every 100 batches\n",
    "            avg_loss = running_loss / 100\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {avg_loss:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Log loss to TensorBoard\n",
    "            writer.add_scalar('Loss/train', avg_loss, epoch)\n",
    "\n",
    "    # Save checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': running_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, f\"{checkpoint_path}/checkpoint_epoch_{epoch + 1}.pth\")\n",
    "        print(f\"Checkpoint saved at epoch {epoch + 1}\")\n",
    "\n",
    "# Save the final model after training\n",
    "final_model_path = os.path.join(model_save_path, '2_Group17_DLProject.pth')\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final model saved at {final_model_path}\")\n",
    "\n",
    "# Close the writer after training\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHihKgCyO1IS"
   },
   "source": [
    "## 6. Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we evaluate the trained model using the validation dataset:\n",
    "- The model is set to evaluation mode, and gradient computation is disabled.\n",
    "- For each batch, we perform a forward pass and predict the class labels.\n",
    "- Ground truth labels and predictions are stored and used to generate a classification report using `sklearn`. This report provides precision, recall, and F1-scores for each emotion class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dboWdi-7O1IS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.44      0.60      0.51       799\n",
      "     Disgust       0.64      0.49      0.56        87\n",
      "        Fear       0.40      0.33      0.36       820\n",
      "       Happy       0.82      0.79      0.80      1443\n",
      "         Sad       0.46      0.47      0.46       966\n",
      "    Surprise       0.70      0.73      0.72       634\n",
      "     Neutral       0.56      0.48      0.52       993\n",
      "\n",
      "    accuracy                           0.58      5742\n",
      "   macro avg       0.57      0.56      0.56      5742\n",
      "weighted avg       0.59      0.58      0.58      5742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "model = model.to(device)\n",
    "model_path = 'models/2_Group17_DLProject.pth'\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "# Initialize lists to store ground truth and predictions\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "# Disable gradient computation\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        # Move inputs and labels to the same device as the model\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get the predicted class\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Append ground truth and predictions to respective lists\n",
    "        y_true.extend(labels.cpu().numpy())  # Convert tensors to numpy\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Generate the classification report\n",
    "emotion_labels = {0:'Angry', 1:'Disgust', 2:'Fear', 3:'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n",
    "print(classification_report(y_true, y_pred, target_names=list(emotion_labels.values())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Removing the last convolutional layer has an effect on the accuracy, with our model achieving only 0.58. This suggests that the additional layer in the baseline model might have played a critical role in capturing more complex patterns in the data."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
